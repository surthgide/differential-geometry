\chapter{Esercizi: Geometria differenziale negli spazi euclidei}

\setcounter{tocdepth}{2}

\section{}\label{es1-1}

\begin{tcolorbox}
	Per ogni numero naturale $ k \in \N $ costruire una funzione $ C^{k}(\R) $ ma non $ C^{k+1}(\R) $.
\end{tcolorbox}

Per la funzione

\map{f_{k}}
	{\R}{\R}
	{x}{\alpha x^{k(k+2)/(k+1)} + \beta}
	
per $ \forall \alpha,\beta \in \R $ e con $ k \in \N $, vale $ f_{k} \in C^{k}(\R) $ ma $ f_{k} \notin C^{k+1}(\R) $.

%

\newpage

%

\section{}\label{es1-2}

\begin{tcolorbox}
	Dimostrare che la funzione
	
	\begin{align}
		\begin{split}
			f : \R & \to \R\\
			x &\mapsto%
				\begin{cases}
					e^{\sfrac{-1}{x^{2}}} & x \neq 0\\
					0 & x = 0
				\end{cases}
		\end{split}
	\end{align}
	
	risulta essere liscia ma non reale analitica.
\end{tcolorbox}

La funzione $ f $ è liscia in quanto, perché lo sia, è necessario che

\begin{equation}
	\pdv[k]{f}{x} \, (0) = \pdv[k]{x} e^{\sfrac{-1}{x^{2}}} \, (0) = 0 \qcomma \forall k \in \N
\end{equation}

e questo è vero poiché

\begin{equation}
	\lim_{x \to 0} \left( \dfrac{e^{\sfrac{-1}{x^{2}}}}{x^{p}} \right) = 0 \qcomma \forall p \in \N %
	\implies%
	\lim_{x \to 0} \left( \pdv[k]{x} \left( e^{\sfrac{-1}{x^{2}}} \right) \right) = 0 \qcomma \forall k \in \N
\end{equation}

La funzione non è però reale analitica perché, in un intervallo aperto qualsiasi di 0 non coincide con il suo sviluppo di Taylor: lo sviluppo di Taylor per la parte dei reali positivi è diversa da 0 per qualsiasi valore di $ x $ non nullo mentre la parte per i reali negativi è identicamente nulla, i.e. preso $ U $ un qualsiasi intorno di 0

\begin{equation}
	\sum_{k=0}^{+\infty} \left( \pdv[k]{x} \left( e^{\sfrac{-1}{x^{2}}} \right) \right) \dfrac{x^{k}}{k!} \neq 0 \qcomma \forall x \in U \setminus \{0\}
\end{equation}

%

\newpage

%

\section{}\label{es1-3}

\begin{tcolorbox}
	Siano $ a,b,c,d \in \R $ tale che $ a<b $. Dimostrare che i seguenti intervalli sono tutti diffeomorfi tra loro e diffeomorfi a $ \R $:
	
	\begin{equation}
		\begin{cases}
			(a,b)\\
			(c,+\infty)\\
			(-\infty,d)
		\end{cases}
	\end{equation}
\end{tcolorbox}

Consideriamo le applicazioni:

\map{f}
	{(a,b)}{(0,1)}
	{x}{\dfrac{x-a}{b-a}}

\map{g}
	{(0,1)}{(c,+\infty)}
	{x}{\dfrac{c}{x}}
	
\map{h}
	{(0,1)}{(-\infty,d)}
	{x}{\ln(x)-d}
	
\map{i}
	{(c,+\infty)}{\R}
	{x}{\ln(x-c)}

Queste sono diffeomorfismi in quanto bigezioni lisce con inversa liscia, dunque le loro composizioni sono ancora diffeomorfismi. Le seguenti composizioni delle applicazioni sopraccitate inducono i seguenti diffeomorfismi:

\begin{equation}
	\begin{cases}
		g \circ f \implies (a,b) \simeq (c,+\infty)\\
		h \circ f \implies (a,b) \simeq (-\infty,d)\\
		i \circ g \circ f \implies (a,b) \simeq \R\\
		h \circ g^{-1} \implies (c,+\infty) \simeq (-\infty,d)\\
		i \implies (c,+\infty) \simeq \R\\
		i \circ g \circ h^{-1} \implies (-\infty,d) \simeq \R
	\end{cases}
\end{equation}

%

\newpage

%

\section{}\label{es1-4}

\begin{tcolorbox}
	Dimostrare che l'applicazione
	
	\map{h}
		{B_{1}(0)}{\R^{n}}
		{x}{\left( \dfrac{x^{1}}{\sqrt{1 - \norm{x}^{2}}}, \cdots, \dfrac{x^{n}}{\sqrt{1 - \norm{x}^{2}}} \right)}
	
	definisce un diffeomorfismo tra la palla aperta unitaria centrata nell'origine di $ \R^{n} $ e $ \R^{n} $. Dedurre che la palla aperta di centro $ c \in \R^{n} $ e raggio $ r > 0 $ in $ \R^{n} $ è diffeomorfa a $ \R^{n} $.
\end{tcolorbox}

L'applicazione $ h $ è una bigezione liscia in quanto ogni sua componente è liscia poiché

\begin{equation}
	\pdv[k]{(x^{i})} \left( \dfrac{x^{i}}{\sqrt{ 1-\norm{x}^{2} }} \right) < \infty \qcomma \forall k \in \N, \, \forall x \in B_{1}(0), \, \forall i=1,\dots,n
\end{equation}

La sua inversa

\map{h^{-1}}
	{\R^{n}}{B_{1}(0)}
	{x}{\left( \dfrac{x^{1}}{\sqrt{1 + \norm{x}^{2}}}, \cdots, \dfrac{x^{n}}{\sqrt{1 + \norm{x}^{2}}} \right)}
	
è ancora liscia per lo stesso motivo, dunque $ h $ induce il diffeomorfismo $ B_{1}(0) \simeq \R^{n} $.\\
Se consideriamo l'applicazione lineare (dunque liscia con inversa liscia e perciò diffeomorfismo)

\map{g}
	{B_{r}(c)}{B_{1}(0)}
	{x}{\dfrac{x-c}{r}}
	
con $ c = (c^{1},\dots,c^{n}) $, e la componiamo con $ h $, otteniamo

\map{f = h \circ g}
	{B_{r}(c)}{\R^{n}}
	{x}{\left( \dfrac{\dfrac{x^{1} - c^{1}}{r}}{\sqrt{1 + \norm{\dfrac{x-c}{r}}^{2}}}, \cdots, \dfrac{\dfrac{x^{n} - c^{n}}{r}}{\sqrt{1 + \norm{\dfrac{x-c}{r}}^{2}}} \right)}

L'applicazione $ f $ è un diffeomorfismo in quanto composizione di diffeomorfismi, dunque $ f $ induce il diffeomorfismo $ B_{r}(c) \simeq \R^{n} $.

%

\newpage

%

\section{}\label{es1-5}

\begin{tcolorbox}
	Sia $ f \in C^{\infty}(\R^{2}) $. Usando il teorema di Taylor con resto, dimostrare che esistono $ g_{11},g_{12},g_{22} \in C^{\infty}(\R^{2}) $ tali che
	
	\begin{equation}
		f(x,y) = f(0,0) + x \, \dfrac{\partial f}{\partial x} (0,0) + y \, \dfrac{\partial f}{\partial y} (0,0) + x^{2} \, g_{11}(x,y) + x y \, g_{12}(x,y) + y^{2} \, g_{22}(x,y)
	\end{equation}
\end{tcolorbox}

Dal teorema di Taylor con resto, se $ f \in C^{\infty} (\R^{2}) $ ($ \R^{2} $ è stellato rispetto all'origine), abbiamo che

\begin{equation}
	\E g_{i_{1} \cdots i_{k}} \in C^{\infty} (\R^{2})
\end{equation}

definite come

\begin{equation}
	g_{i_{1} \cdots i_{k}} (0,0) \doteq \dfrac{1}{k!} \dfrac{\partial^{k} f}{\partial x^{i_{1}} \cdots \partial x^{i_{k}}} (0,0)
\end{equation}

tali che

\begin{equation}
	f(x,y) = f(0,0) + \sum_{m=1}^{k} \sum_{\substack{i_{1},\dots,i_{k}=1 \\ {i_{k}} > \cdots > i_{1}}}^{m} g_{i_{1} \cdots i_{k}} (x,y) \prod_{j=1}^{k} x^{i_{j}} \qcomma \forall k \in \N
\end{equation}

Espandendo quest'ultima forma per $ k=1 $ otteniamo

\begin{align}
	\begin{split}
		f(x,y) &= f(0,0) + x \, g_{1} (x,y) + y \, g_{2} (x,y)\\
		&= f(0,0) + x \, g_{1} (0,0) + y \, g_{2} (0,0) + x^{2} \, g_{11}(x,y) + x y \, g_{12}(x,y) + y^{2} \, g_{22}(x,y)\\
		&= f(0,0) + x \, \dfrac{\partial f}{\partial x} (0,0) + y \, \dfrac{\partial f}{\partial y} (0,0) + x^{2} \, g_{11}(x,y) + x y \, g_{12}(x,y) + y^{2} \, g_{22}(x,y)
	\end{split}
\end{align}

dove gli ultimi 3 termini indicano il resto.

%

\newpage

%

\section{}\label{es1-6}

\begin{tcolorbox}
	Sia $ f \in C^{\infty} (\R^{2}) $ tale che
	
	\begin{equation}
		f(0,0) = \pdv{f}{x} \, (0,0) = \pdv{f}{y} \, (0,0) = 0
	\end{equation}

	Sia l'applicazione
	
	\map{g}
		{\R^{2}}{\R}
		{(t,u)}{%
			\begin{cases}
				\dfrac{f(t,tu)}{t} & t \neq 0\\\\
				0 & t = 0
			\end{cases}
			}
	
	Dimostrare che $ g \in C^{\infty}(\R^{2}) $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es1-7}

\begin{tcolorbox}
	Dimostrare che l'insieme $ C_{p}^{\infty}(\R^{n}) $ dei germi delle funzioni lisce intorno a $ p \in \R^{n} $ con le operazioni di somma e di prodotto definite negli appunti è un'algebra commutativa e unitaria.
\end{tcolorbox}

L'algebra $ A = (C_{p}^{\infty}(\R^{n}),+,\cdot) $ ha le operazioni definite come segue:

\map{+}
	{A \times A}{A}
	{([(f,U)],[(g,V)])}{[(f+g,U \cap V)]}
	
\map{\cdot}
	{A \times A}{A}
	{([(f,U)],[(g,V)])}{[(f g,U \cap V)]}

Perché sia effettivamente un'algebra, verifichiamo che sia distributiva e omogenea.\\
Per la distributività sinistra:

\begin{align}
	\begin{split}
		([(f,U)] + [(g,V)]) \cdot [(h,W)] &= [(f+g,U \cap V)] \cdot [(h,W)]\\
		&= [((f+g) h,U \cap V \cap W)]\\
		&= [(fh + gh,U \cap V \cap W)]\\
		&= [(fh,U \cap W)] + [(gh,V \cap W)]\\
		&= [(f,U)] \cdot [(h,W)] + [(g,V)] \cdot [(h,W)]
	\end{split}
\end{align}

per $ \forall [(f,U)], [(g,V)], [(h,W)] \in C_{p}^{\infty}(\R^{n}) $.\\
La distributività destra deriva immediatamente dalla distributività sinistra e dalla commutatività (condizione non necessaria per un'algebra): quest'ultima può essere verificata tramite i seguenti passaggi

\begin{align}
	\begin{split}
		[(f,U)] + [(g,V)] &= [(f+g,U \cap V)]\\
		&= [(g+f,V \cap U)]\\
		&= [(g,V)] + [(f,U)]\\\\
		%
		[(f,U)] \cdot [(g,V)] &= [(fg,U \cap V)]\\
		&= [(gf,V \cap U)]\\
		&= [(g,V)] \cdot [(f,U)]
	\end{split}
\end{align}

per $ \forall [(f,U)], [(g,V)] \in C_{p}^{\infty}(\R^{n}) $.\\
Per l'omogeneità:

\begin{align}
	\begin{split}
		\lambda ([(f,U)] \cdot [(g,V)]) &= \lambda [(fg,U \cap V)]\\
		&= [(\lambda fg,V \cap U)]\\
		&= [(\lambda f,U)] \cdot [(g,V)]\\
		&= [(f,U)] \cdot [(\lambda g,V)]
	\end{split}
\end{align}

per $ \forall [(f,U)], [(g,V)] \in C_{p}^{\infty}(\R^{n}) $ e $ \forall \lambda \in \R $.\\
Infine l'unitarietà, i.e.

\begin{equation}
	\E e = [(1,U)] \in C_{p}^{\infty}(\R^{n}) \mid [(f,U)] \cdot e = e \cdot [(f,U)] = [(f,U)] \qcomma \forall [(f,U)] \in C_{p}^{\infty}(\R^{n})
\end{equation}

può essere verificata tramite i seguenti passaggi:

\begin{align}
	\begin{split}
		[(f,U)] \cdot [(1,U)] &= [(f \cdot 1,U \cap U)]\\
		&= [(1 \cdot f,U \cap U)]\\
		&= [(1,U)] \cdot [(f,U)]\\
		&= [(f,U)]
	\end{split}
\end{align}

%

\newpage

%

\section{}\label{es1-8}

\begin{tcolorbox}
	Dimostrare che l'insieme $ \der_{p}(C_{p}^{\infty}(\R^{n})) $ delle derivazioni puntuali con le operazioni definite negli appunti è uno spazio vettoriale su $ \R $.
\end{tcolorbox}

Per dimostrare che $ \der_{p}(C_{p}^{\infty}(\R^{n})) $ sia uno spazio vettoriale su $ \R $ è necessario che le operazioni di somma tra derivazioni e moltiplicazione per scalari rispettino i seguenti 8 assiomi:

\begin{equation}
	\begin{cases}
		D_{v} + (D_{w} + D_{x}) = (D_{v} + D_{w}) + D_{x} & \text{ 1. associatività (somma) }\\
		%
		D_{v} + D_{w} = D_{w} + D_{v} & \text{ 2. commutatività (somma) }\\
		%
		\E 0 \in \der_{p}(C_{p}^{\infty}(\R^{n})) \, \mid \, D_{v} + 0 = D_{v} & \text{ 3. elemento neutro (somma) }\\
		%
		\E - D_{v} \in \der_{p}(C_{p}^{\infty}(\R^{n})) \, \mid \, D_{v} + (- D_{v}) = 0 & \text{ 4. inverso (somma) }\\
		%
		\alpha (\beta D_{v}) = (\alpha \beta) D_{v} & \text{ 5. compatibilità (moltiplicazione) }\\
		%
		\E 1 \in \R \, \mid \, 1 D_{v} = D_{v} & \text{ 6. elemento neutro (moltiplicazione) }\\
		%
		(\alpha + \beta) D_{v} = \alpha D_{v} + \beta D_{v} & \text{ 7. distributività (somma vettoriale) }\\
		%
		\alpha (D_{v} + D_{w}) = \alpha D_{v} + \alpha D_{w} & \text{ 8. distributività (somma scalare) }
	\end{cases}
\end{equation}

per $ \forall D_{v}, D_{w}, D_{x} \in \der_{p}(C_{p}^{\infty}(\R^{n})) $ e $ \forall \alpha, \beta \in \R $.\\
Per dimostrare queste proprietà consideriamo un qualunque $ [(f,U)] \in C_{p}^{\infty}(\R^{n}) $ e applichiamo a questo le derivazioni:

\begin{enumerate}
	\item Associatività (somma)
	
		\begin{align}
			\begin{split}
				( D_{v} + (D_{w} + D_{x}) ) ([(f,U)]) &= D_{v} ([(f,U)]) + (D_{w} + D_{x}) ([(f,U)])\\
				&= D_{v} ([(f,U)]) + D_{w} ([(f,U)]) + D_{x} ([(f,U)])\\
				&= (D_{v} + D_{w}) ([(f,U)]) + D_{x} ([(f,U)])\\
				&= ( (D_{v} + D_{w}) + D_{x} ) ([(f,U)])
			\end{split}
		\end{align}
	
	\item Commutatività (somma)
	
		\begin{align}
			\begin{split}
				(D_{v} + D_{w}) ([(f,U)]) &= D_{v} ([(f,U)]) + D_{w} ([(f,U)])\\
				&= D_{w} ([(f,U)]) + D_{v} ([(f,U)])\\
				&= (D_{w} + D_{v}) ([(f,U)])
			\end{split}
		\end{align}
	
		dove nel secondo passaggio abbiamo usato la commutatività della somma in $ \R $
	
	\item Elemento neutro (somma)
	
		\map{0}
			{C_{p}^{\infty}(\R^{n})}{\R}
			{[(f,U)]}{0}
			
		per $ \forall [(f,U)] \in C_{p}^{\infty}(\R^{n}) $, dunque
	
		\begin{align}
			\begin{split}
				(D_{v} + 0) ([(f,U)]) &= D_{v} ([(f,U)]) + 0 ([(f,U)])\\
				&= D_{v} ([(f,U)]) + 0\\
				&= D_{v} ([(f,U)])
			\end{split}
		\end{align}
	
	\item Inverso (somma)
	
		\map{- D_{v}}
			{C_{p}^{\infty}(\R^{n})}{\R}
			{[(f,U)]}{- \sum_{i=1}^{n} \pdv{f}{x^{i}} \, (p) \, v^{i}}
			
		dunque
	
		\begin{align}
			\begin{split}
				(D_{v} + (- D_{v})) ([(f,U)]) &= D_{v} ([(f,U)]) + (- D_{v}) ([(f,U)])\\
				&= \sum_{i=1}^{n} \pdv{f}{x^{i}} \, (p) \, v^{i} + \left( - \sum_{i=1}^{n} \pdv{f}{x^{i}} \, (p) \, v^{i} \right)\\
				&= 0
			\end{split}
		\end{align}
	
	\item Compatibilità (moltiplicazione)
	
		\begin{align}
			\begin{split}
				(\alpha (\beta D_{v})) ([(f,U)]) &= \alpha (\beta D_{v}) ([(f,U)])\\
				&= \alpha D_{v} ([(\beta f,U)])\\
				&= \alpha \beta D_{v} ([(f,U)])\\
				&= (\alpha \beta) D_{v} ([(f,U)])
			\end{split}
		\end{align}
	
	\item Elemento neutro (moltiplicazione)
	
		\begin{align}
			\begin{split}
				(1 D_{v}) ([(f,U)]) &= D_{v} ([(1 f,U)])\\
				&= D_{v} ([(f,U)])
			\end{split}
		\end{align}
	
	\item Distributività (somma vettoriale)
	
		\begin{align}
			\begin{split}
				(\alpha + \beta) D_{v} ([(f,U)]) &= D_{v} ([((\alpha + \beta) f,U)])\\
				&= D_{v} ([(\alpha f + \beta f,U)])\\
				&= \alpha D_{v} ([(f,U)]) + \beta D_{v} ([(f,U)])\\
			\end{split}
		\end{align}
	
	\item Distributività (somma scalare)
	
		\begin{align}
			\begin{split}
				\alpha (D_{v} + D_{w}) ([(f,U)]) &= (D_{v} + D_{w}) ([(\alpha f,U)])\\
				&= D_{v} ([(\alpha f,U)]) + D_{w} ([(\alpha f,U)])\\
				&= \alpha D_{v} ([(f,U)]) + \alpha D_{w} ([(f,U)])
			\end{split}
		\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per $ \forall D_{v}, D_{w}, D_{x} \in \der_{p}(C_{p}^{\infty}(\R^{n})) $ e $ \forall \alpha, \beta \in \R $.

%

\newpage

%

\section{}\label{es1-9}

\begin{tcolorbox}
	Dimostrare che l'insieme dei campi di vettori lisci $ \chi(U) $ su un aperto $ U \subset \R^{n} $ con le operazioni definite negli appunti è uno spazio vettoriale su $ \R $ e un $ C^{\infty} $-modulo.
\end{tcolorbox}

Per dimostrare che $ \chi(U) $ sia uno spazio vettoriale su $ \R $ è necessario che le operazioni di somma tra campi di vettori e moltiplicazione per scalari rispettino i seguenti 8 assiomi:

\begin{equation}
	\begin{cases}
		X + (Y + Z) = (X + Y) + Z & \text{ 1. associatività (somma) }\\
		%
		X + Y = Y + X & \text{ 2. commutatività (somma) }\\
		%
		\E 0 \in \chi(U) \, \mid \, X + 0 = X & \text{ 3. elemento neutro (somma) }\\
		%
		\E - X \in \chi(U) \, \mid \, X + (- X) = 0 & \text{ 4. inverso (somma) }\\
		%
		\alpha (\beta X) = (\alpha \beta) X & \text{ 5. compatibilità (moltiplicazione) }\\
		%
		\E 1 \in \R \, \mid \, 1 X = X & \text{ 6. elemento neutro (moltiplicazione) }\\
		%
		(\alpha + \beta) X = \alpha X + \beta X & \text{ 7. distributività (somma vettoriale) }\\
		%
		\alpha (X + Y) = \alpha X + \alpha Y & \text{ 8. distributività (somma scalare) }
	\end{cases}
\end{equation}

per $ \forall X, Y, Z \in \chi(U) $ e $ \forall \alpha, \beta \in \R $.\\
Ricordiamo che le operazioni sono definite come:

\begin{align}
	\begin{split}
		(X + Y)_{p} &\doteq X_{p} + Y_{p}\\
		(\alpha X)_{p} &\doteq \alpha X_{p}
	\end{split}
\end{align}

per $ \forall X, Y \in \chi(U) $, $ \forall \alpha \in \R $ e $ \forall p \in U \subset \R^{n} $, dove i campi di vettori saranno:

\begin{equation}
	X = \sum_{i=1}^{n} a^{i} \pdv{x^{i}} \qcomma Y = \sum_{i=1}^{n} b^{i} \pdv{x^{i}}
\end{equation}

con $ a_{i}, b_{i} \in C^{\infty}(U) $.\\
Per dimostrare dunque queste proprietà, valutiamo i campi di vettori in un qualunque $ p \in U \subset \R^{n} $:

\begin{enumerate}
	\item Associatività (somma)
	
		\begin{align}
			\begin{split}
				(X + (Y + Z))_{p} &= (X + Y)_{p} + Z_{p}\\
				&= X_{p} + Y_{p} + Z_{p}\\
				&= X_{p} + (Y + Z)_{p}\\
				&= (X + (Y + Z))_{p}
			\end{split}
		\end{align}
	
	\item Commutatività (somma)
	
		\begin{align}
			\begin{split}
				(X + Y)_{p} &= X_{p} + Y_{p}\\
				&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= Y_{p} + X_{p}\\
				&= (Y + X)_{p}
			\end{split}
		\end{align}
	
	\item Elemento neutro (somma)
	
		\map{0}
			{U}{\bigsqcup_{p \in U} T_{p} (\R^{n})}
			{p}{%
				0_{p} = \sum_{i=1}^{n} 0 \eval{ \pdv{x^{i}} }_{p}\\
				&\stackrel{\R^{n}}{\mapsto} (0, \dots, 0)%
				}
			
		dunque
		
		\begin{align}
			\begin{split}
				(X + 0)_{p} &= X_{p} + 0_{p}\\
				&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} 0 \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} (a^{i} (p) + 0) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= X_{p}
			\end{split}
		\end{align}
	
	\item Inverso (somma)
	
		\map{- X}
			{U}{\bigsqcup_{p \in U} T_{p} (\R^{n})}
			{p}{\sum_{i=1}^{n} (- a^{i} (p)) \eval{ \pdv{x^{i}} }_{p}}
		
		dunque
		
		\begin{align}
			\begin{split}
				(X + (- X))_{p} &= X_{p} + (- X)_{p}\\
				&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} (- a^{i} (p)) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} (a^{i} (p) - a^{i} (p)) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} 0 \eval{ \pdv{x^{i}} }_{p}\\
				&= 0_{p}
			\end{split}
		\end{align}
	
	\item Compatibilità (moltiplicazione)
	
		\begin{align}
			\begin{split}
				(\alpha (\beta X))_{p} &= \alpha (\beta X)_{p}\\
				&= \alpha \beta X_{p}\\
				&= (\alpha \beta) X_{p}\\
				&= ((\alpha \beta) X)_{p}
			\end{split}
		\end{align}
	
	\item Elemento neutro (moltiplicazione)
	
		\begin{align}
			\begin{split}
				(1 X_{p}) &= 1 X_{p}\\
				&= 1 \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} (1 a^{i} (p)) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= X_{p}
			\end{split}
		\end{align}
	
	\item Distributività (somma vettoriale)
	
		\begin{align}
			\begin{split}
				(\alpha (X + Y))_{p} &= \alpha (X + Y)_{p}\\
				&= \alpha (X_{p} + Y_{p})\\
				&= \alpha \left( \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p} \right)\\
				&= \alpha \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \alpha \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= \alpha X_{p} + \alpha Y_{p}\\
				&= (\alpha X)_{p} + (\alpha Y)_{p}\\
				&= (\alpha X + \alpha Y)_{p}
			\end{split}
		\end{align}
	
	\item Distributività (somma scalare)
	
		\begin{align}
			\begin{split}
				((\alpha + \beta) X)_{p} &= (\alpha + \beta) X_{p}\\
				&= (\alpha + \beta) \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= \alpha \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \beta \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= \alpha X_{p} + \beta X_{p}\\
				&= (\alpha X)_{p} + (\beta X)_{p}\\
				&= (\alpha X + \beta X)_{p} 
			\end{split}
		\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per $ \forall X, Y, Z \in \chi(U) $ e $ \forall \alpha, \beta \in \R $.\\
Sia l'applicazione

\map{\cdot}%
	{C^{\infty}(U) \times \chi(U)}{\chi(U)}%
	{(f,X)}{f X}

Per dimostrare che $ (\chi(U),+) $ sia un $ C^{\infty} $-modulo è necessario che siano verificate queste proprietà sia a sinistra che a destra:

\begin{equation}
	\begin{cases}
		1_{C^{\infty}(U)} X = X & \text{ 1. elemento neutro (somma) }\\
		f (g X) = (f g) \cdot X & \text{ 2. compatibilità (moltiplicazione) }\\
		f (X+Y) = f X + f Y & \text{ 3. distributività (somma vettoriale) }\\
		(f+g) X = f X + g X & \text{ 4. distributività (somma scalare) }
	\end{cases}
\end{equation}

per $ \forall f,g \in C^{\infty}(U) $ e $ \forall X,Y \in \chi(U) $. Siccome la moltiplicazione per funzione è commutativa è sufficiente dimostrare che $ (\chi(U),+) $ sia un $ C^{\infty}(U) $-modulo sinistro (o destro) per dimostrare che sia $ C^{\infty}(U) $-modulo.\\
Dimostriamo dunque le proprietà riportate sopra, ancora una volta valutando i campi di vettori in un qualunque $ p \in U \subset \R^{n} $:

\begin{enumerate}
	\item Elemento neutro (somma)
	
		\map{1_{C^{\infty}(U)}}
			{U}{\R}
			{p}{1}
			
		dunque
			
		\begin{align}
			\begin{split}
				(1_{C^{\infty}(U)} X)_{p} &= 1_{C^{\infty}(U)} (p) X_{p}\\
				&= 1 \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} (1 a^{i} (p)) \eval{ \pdv{x^{i}} }_{p}\\
				&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p}\\
				&= X_{p}
			\end{split}
		\end{align}
	
	\item Compatibilità (moltiplicazione)
	
		\begin{align}
			\begin{split}
				(f (g X))_{p} &= f (p) (g X)_{p}\\
				&= f (p) g (p) X_{p}\\
				&= (f g) (p) X_{p}\\
				&= ((f g) X)_{p}
			\end{split}
		\end{align}
	
	\item Distributività (somma vettoriale)
	
		\begin{align}
			\begin{split}
				(f (X + Y))_{p} &= f (p) (X + Y)_{p}\\
				&= f (p) (X_{p} + Y_{p})\\
				&= f (p) X_{p} + f (p) Y_{p}\\
				&= (f X)_{p} + (f Y)_{p}\\
				&= (f X + f Y)_{p}
			\end{split}
		\end{align}
	
	\item Distributività (somma scalare)
	
		\begin{align}
			\begin{split}
				((f + g) X)_{p} &= (f + g) (p) X_{p}\\
				&= (f (p) + g (p)) X_{p}\\
				&= f (p) X_{p} + g (p) X_{p}\\
				&= (f X)_{p} + (g X)_{p}\\
				&= (f X + g X)_{p}
			\end{split}
		\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per $ \forall f,g \in C^{\infty}(U) $ e $ \forall X,Y \in \chi(U) $.

%

\newpage

%

\section{}\label{es1-10}

\begin{tcolorbox}
	Sia $ A $ un'algebra su un campo $ \K $. Dimostrare che le operazioni
	
	\begin{equation}
		\begin{cases}
			(D_{1}+D_{2})(a) = D_{1}(a) + D_{2}(a)\\
			(\lambda D)(a) = \lambda D(a)
		\end{cases}
	\end{equation}
	
	per $ \forall \lambda \in \K $ e $ \forall D_{1},D_{2},D \in Der(A) $ dotano $ Der(A) $ della struttura di spazio vettoriale su $ \K $.
\end{tcolorbox}

Per dimostrare che $ \der(A) $ sia uno spazio vettoriale su un campo $ \K $ è necessario che le operazioni di somma tra derivazioni e moltiplicazione per scalari rispettino i seguenti 8 assiomi:

\begin{equation}
	\begin{cases}
		D_{1} + (D_{2} + D_{3}) = (D_{1} + D_{2}) + D_{3} & \text{ 1. associatività (somma) }\\
		%
		D_{1} + D_{2} = D_{2} + D_{1} & \text{ 2. commutatività (somma) }\\
		%
		\E 0 \in \der(A)) \, \mid \, D + 0 = D & \text{ 3. elemento neutro (somma) }\\
		%
		\E - D \in \der(A) \, \mid \, D + (- D) = 0 & \text{ 4. inverso (somma) }\\
		%
		\alpha (\beta D) = (\alpha \beta) D & \text{ 5. compatibilità (moltiplicazione) }\\
		%
		\E \eta \in \K \, \mid \, \eta D = D & \text{ 6. elemento neutro (moltiplicazione) }\\
		%
		(\alpha + \beta) D = \alpha D + \beta D & \text{ 7. distributività (somma vettoriale) }\\
		%
		\alpha (D_{1} + D_{2}) = \alpha D_{1} + \alpha D_{2} & \text{ 8. distributività (somma scalare) }
	\end{cases}
\end{equation}

per $ \forall D_{1}, D_{2}, D_{3}, D \in \der(A) $ e $ \forall \alpha, \beta \in \K $.\\
Per dimostrare queste proprietà consideriamo un qualunque $ a \in A $ e applichiamo a questo le derivazioni:

\begin{enumerate}
	\item Associatività (somma)
	
		\begin{align}
			\begin{split}
				( D_{1} + (D_{2} + D_{3}) ) (a) &= D_{1} (a) + (D_{2} + D_{3}) (a)\\
				&= D_{1} (a) + D_{2} (a) + D_{3} (a)\\
				&= (D_{1} + D_{2}) (a) + D_{3} (a)\\
				&= ( (D_{1} + D_{2}) + D_{3} ) (a)
			\end{split}
		\end{align}
	
	\item Commutatività (somma)
	
		\begin{align}
			\begin{split}
				(D_{1} + D_{2}) (a) &= D_{1} (a) + D_{2} (a)\\
				&= D_{2} (a) + D_{1} (a)\\
				&= (D_{2} + D_{1}) (a)
			\end{split}
		\end{align}
		
		dove nel secondo passaggio abbiamo usato la commutatività della somma dell'algebra ereditata dallo spazio vettoriale che la compone
	
	\item Elemento neutro (somma)
	
		\map{0}
			{A}{A}
			{a}{0}
		
		dove
		
		\begin{equation}
			a + 0 = a \qcomma \forall a \in A
		\end{equation}
		
		dunque
		
		\begin{align}
			\begin{split}
				(D + 0) (a) &= D (a) + 0 (a)\\
				&= D (a) + 0\\
				&= D (a)
			\end{split}
		\end{align}
	
	\item Inverso (somma)
	
		\map{- D}
			{A}{A}
			{a}{- D (a)}
		
		dunque
		
		\begin{align}
			\begin{split}
				(D + (- D)) (a) &= D (a) + (- D) (a)\\
				&= D (a) + - D (a)\\
				&= 0
			\end{split}
		\end{align}
	
	\item Compatibilità (moltiplicazione)
	
		\begin{align}
			\begin{split}
				(\alpha (\beta D)) (a) &= \alpha (\beta D) (a)\\
				&= \alpha \beta D (a)\\
				&= (\alpha \beta) D (a)\\
				&= ((\alpha \beta) D) (a)
			\end{split}
		\end{align}
	
	\item Elemento neutro (moltiplicazione)
	
		\begin{align}
			\begin{split}
				(\eta D) (a) &= \eta D (a)\\
				&= D (\eta a)\\
				&= D (a)
			\end{split}
		\end{align}
	
		dove abbiamo usato il fatto che lo spazio vettoriale che compone l'algebra è sullo stesso campo $ \K $ rispetto a quest'ultima
	
	\item Distributività (somma vettoriale)
	
		\begin{align}
			\begin{split}
				((\alpha + \beta) D) (a) &= (\alpha + \beta) D (a)\\
				&= \alpha D (a) + \beta D (a)\\
				&= (\alpha D) (a) + (\beta D) (a)\\
				&= (\alpha D + \beta D) (a)
			\end{split}
		\end{align}
	
	\item Distributività (somma scalare)
	
		\begin{align}
			\begin{split}
				(\alpha (D_{1} + D_{2})) (a) &= \alpha (D_{1} + D_{2}) (a)\\
				&= \alpha (D_{1} (a) + D_{2} (a))\\
				&= \alpha D_{1} (a) + \alpha D_{2} (a)\\
				&= (\alpha D_{1}) (a) + (\alpha D_{2}) (a)\\
				&= (\alpha D_{1} + \alpha D_{2}) (a)
			\end{split}
		\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per $ \forall D_{1}, D_{2}, D_{3}, D \in \der(A) $ e $ \forall \alpha, \beta \in \K $.

%

\newpage

%

\section{}\label{es1-11}

\begin{tcolorbox}
	Siano $ D_{1} $ e $ D_{2} $ due derivazioni di un'algebra $ A $ su un campo $ \K $, i.e. $ D_{1},D_{2} \in Der(A) $. Mostrare che $ D_{1} \circ D_{2} $ non è necessariamente una derivazione di $ A $ mentre
	
	\begin{equation}
		D_{1} \circ D_{2} - D_{2} \circ D_{1} \in Der(A)
	\end{equation}
\end{tcolorbox}

Perché $ D_{1} \circ D_{2} $ sia una derivazione deve, in particolare, soddisfare la regola di Leibniz, ma questo non è verificato:

\begin{align}
	\begin{split}
		(D_{1} \circ D_{2})(a \cdot b) &= D_{1}(D_{2}(a \cdot b))\\
		&= D_{1}( D_{2}(a) \cdot b + a \cdot D_{2}(b) )\\
		&= D_{1}(D_{2}(a)) \cdot b + D_{2}(a) \cdot D_{1}(b) + D_{1}(a) \cdot D_{2}(b) + a \cdot D_{1}(D_{2}(b))\\
		&= (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b) + D_{2}(a) \cdot D_{1}(b) + D_{1}(a) \cdot D_{2}(b)\\
		&\neq (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b)
	\end{split}
\end{align}

Mentre per la combinazione $ D_{1} \circ D_{2} - D_{2} \circ D_{1} $ questa prescrizione è verificata:

\begin{align}
	\begin{split}
		(D_{1} \circ D_{2} - D_{2} &\circ D_{1})(a \cdot b) = D_{1}(D_{2}(a \cdot b)) - D_{2}(D_{1}(a \cdot b))\\
		&= (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b) + D_{2}(a) \cdot D_{1}(b) + D_{1}(a) \cdot D_{2}(b) +\\
		& \hspace{13px} - ((D_{2} \circ D_{1})(a) \cdot b + a \cdot (D_{2} \circ D_{1})(b) + D_{1}(a) \cdot D_{2}(b) + D_{2}(a) \cdot D_{1}(b))\\
		%
		&= (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b) + \cancel{ D_{2}(a) \cdot D_{1}(b) } + \cancel{ D_{1}(a) \cdot D_{2}(b) } +\\
		& \hspace{13px} - (D_{2} \circ D_{1})(a) \cdot b - a \cdot (D_{2} \circ D_{1})(b) - \cancel{ D_{1}(a) \cdot D_{2}(b) } - \cancel{ D_{2}(a) \cdot D_{1}(b) }\\
		%
		&= (D_{1} \circ D_{2} - D_{2} \circ D_{1})(a) \cdot b + a \cdot (D_{1} \circ D_{2} - D_{2} \circ D_{1})(b)
	\end{split}
\end{align}

dove nell'ultimo passaggio abbiamo semplificato due coppie di fattori avvalendoci della commutatività dell'operazione $ \cdot : A \to A $ dell'algebra $ A $, dunque $ D_{1} \circ D_{2} - D_{2} \circ D_{1} \in Der(A) $.

\chapter{Esercizi: Varietà differenziabili}

\section{}\label{BONUS2-1}

\begin{tcolorbox}
	Verificare che l'unione disgiunta di spazi topologici
	
	\begin{equation}
		A = \bigsqcup_{j \in J} A_{j} \equiv \bigcup_{j \in J} A_{j} \times \{j\}
	\end{equation}
	
	è uno spazio topologico, sapendo che $ U $ è aperto in $ A $ se e solo se $ U \cap A_{j} $ è aperto in $ A_{j} $ per $ \forall j \in J $.
\end{tcolorbox}

Per dimostrare che l'unione disgiunta di spazi topologici $ A $ sia ancora uno spazio topologico dobbiamo dimostrare che:

\begin{itemize}
	\item L'intersezione di due aperti in $ A $ sia ancora aperto in $ A $
	
	\item L'unione di un numero qualsiasi di aperti in $ A $ sia ancora aperto in $ A $
\end{itemize}

Per la prima, prendiamo due aperti $ U $ e $ V $ in $ A $: questo significa che $ U \cap A_{j} $ e $ V \cap A_{j} $ saranno aperti in $ A_{j} $ per $ \forall j \in J $ per la condizione imposta. Siccome gli $ A_{j} $ sono spazi topologici, l'intersezione di due aperti è ancora un aperto, dunque

\begin{equation}
	U \cap V \cap A_{j} \text{ aperto in } A_{j} \qcomma \forall j \in J
\end{equation}

A questo punto, vale l'implicazione inversa, perciò $ U \cap V $ è aperto in $ A $.\\\\
Analogamente, per la seconda, prendiamo una famiglia infinita di aperti $ V_{i} $ (i.e. $ i \in N $) in $ A $: questo significa che $ V_{i} \cap A_{j} $ saranno aperti in $ A_{j} $ per $ \forall j \in J $ per la condizione imposta. Siccome gli $ A_{j} $ sono spazi topologici, l'unione di un numero qualsiasi di aperti è ancora un aperto. Definendo

\begin{equation}
	V \doteq \bigcup_{i=1}^{\infty} V_{i}
\end{equation}

possiamo dire che $ V \cap A_{j} $ è aperto in $ A_{j} $ per $ \forall j \in J $. A questo punto, vale l'implicazione inversa, perciò $ V $ è aperto in $ A $.

\section{}\label{es2-1}

\begin{tcolorbox}
	Sia $ \S^{n} $ la sfera unitaria in $ \R^{n+1} $. Trovare un atlante differenziabile di $ \S^{n} $ con $ 2(n+1) $ carte.
\end{tcolorbox}

Prendendo un parametro $ k = 1,\dots,n+1 $, definiamo\footnote{%
	La funzione $ \ceil*{x} $ \textit{ceiling} (soffitto) mappa un numero reale nel successivo numero naturale più vicino (arrotonda per eccesso), e.g. $ \ceil*{\frac{3}{2}} = 3 $.%
} i $ 2(n+1) $ aperti

\begin{equation}
	U_{i} = \left\{ (x^{1},\dots,x^{n+1}) \in \R^{n+1} \st (-1)^{i} x^{\ceil*{\frac{i}{2}}} > 0 \right\} \qcomma k = 1, \dots, 2(n+1)
\end{equation}

per i quali vale $ U_{i} \subset \R^{n+1} $ e che formano un ricoprimento per $ \S^{n} $, i.e.

\begin{equation}
	\S^{n} = \bigcup_{i=1}^{2(n+1)} U_{i}
\end{equation}

Definiamo anche gli aperti per $ j = 1,\dots,n+1 $

\begin{equation}
	D_{j} = \{ (x^{1},\dots,x^{j-1},x^{j+1},\dots,x^{n+1}) \in \R^{n} \times \{0\} = \R^{n} \, \mid \, \norm{x} < 1 \} = B_{1}(0) \simeq \R^{n}
\end{equation}

i quali, in sostanza, sono palle di raggio unitario centrate nell'origine nel piano $ \R^{n} \times \{0\} $, dove

\begin{equation}
	\norm{x} = \sqrt{ \sum_{i=1}^{n} (x^{i})^{2} }
\end{equation}

Infine definiamo le applicazioni lisce

\map{\phi_{i}}%
	{U_{i}}{D_{\ceil*{\frac{i}{2}}}}%
	{(x^{1},\dots,x^{i-1},x^{i},x^{i+1},\dots,x^{n+1})}{(x^{1},\dots,x^{i-1},x^{i+1},\dots,x^{n+1})}

con inverse

\map{\phi_{i}^{-1}}%
	{D_{\ceil*{\frac{i}{2}}}}{U_{i}}%
	{(x^{1},\dots,x^{i-1},x^{i},\dots,x^{n})}{\left( x^{1},\dots,x^{i-1},(-1)^{i} \sqrt{1-\norm{x}^{2}},x^{i},\dots,x^{n} \right)}

La famiglia di coppie $ \mathfrak{U} = \{ (U_{i},\phi_{i}) \}_{i=1,\dots,2(n+1)} $ definisce un atlante differenziabile per $ \S^{n} $ in quanto i cambi di carte sono lisci: la dimostrazione è analoga a quella fatta nell'Esempio \ref{unit-sph} per $ \S^{2} $.

%

\newpage

%

\section{}\label{es2-2}

\begin{tcolorbox}
	Dimostrare che la struttura differenziabile su $ \S^{n} $ definita nell’esercizio precedente e quella definita dalle proiezioni
	stereografiche coincidono.
\end{tcolorbox}

Per dimostrare che le due strutture differenziabili coincidano è sufficiente mostrare che le carte di ognuna siano $ C^{\infty} $-compatibili con quelle dell'altra.\\
Considerando le intersezioni:

\begin{align}
		U_{N} \cap U_{i} &= %\\
		\begin{cases}
			U_{i}, & i \neq 2(n+1)\\
			U_{i} \setminus \{ N \}, & i = 2(n+1)
		\end{cases}\\
		%
		U_{S} \cap U_{i} &= %\\
		\begin{cases}
			U_{i}, & i \neq 2(n+1) - 1 = 2 n + 1\\
			U_{i} \setminus \{ S \}, & i = 2n+1
		\end{cases}
\end{align}

possiamo scrivere i cambi di carta

\begin{gather}
	\pi_{N} \circ \phi_{i}^{-1} : \phi_{i}(U_{N} \cap U_{i}) \to \pi_{N}(U_{N} \cap U_{i})\\
	\pi_{S} \circ \phi_{i}^{-1} : \phi_{i}(U_{S} \cap U_{i}) \to \pi_{S}(U_{S} \cap U_{i})\\
	\nonumber\\
	\phi_{i} \circ \pi_{N}^{-1} : \pi_{N}(U_{N} \cap U_{i}) \to \phi_{i}(U_{N} \cap U_{i})\\
	\phi_{i} \circ \pi_{S}^{-1} : \pi_{N}(U_{S} \cap U_{i}) \to \phi_{i}(U_{S} \cap U_{i})
\end{gather}

esplicitamente abbiamo che

\begin{align}
	\begin{split}
		(\pi_{N} \circ \phi_{i}^{-1})(x^{1},\dots,x^{n}) &= \pi_{N} \left( x^{1}, \dots, x^{i-1}, (-1)^{i} \sqrt{1 - \norm{x}^{2}}, x^{i}, \dots, x^{n} \right)\\
		&= \left( \dfrac{2 x^{1}}{1-x^{n}}, \dots, \dfrac{2 x^{i-1}}{1-x^{n}}, \dfrac{2 (-1)^{i} \sqrt{1 - \norm{x}^{2}}}{1-x^{n}}, \dfrac{2 x^{i}}{1-x^{n}}, \dots, \dfrac{2 x^{n-1}}{1-x^{n}} \right)\\\\
		%
		(\phi_{i} \circ \pi_{N}^{-1})(x^{1},\dots,x^{n}) &= \phi_{i} \left( \dfrac{x^{1}}{1 + \norm{x}^{2}}, \dots, \dfrac{x^{n}}{1 + \norm{x}^{2}}, \dfrac{1 - \norm{x}^{2}}{1 + \norm{x}^{2}} \right)\\
		&= \left( \dfrac{x^{1}}{1 + \norm{x}^{2}}, \dots, \dfrac{x^{i-1}}{1 + \norm{x}^{2}}, \dfrac{x^{i+1}}{1 + \norm{x}^{2}}, \dots, \dfrac{x^{n}}{1 + \norm{x}^{2}}, \dfrac{1 - \norm{x}^{2}}{1 + \norm{x}^{2}} \right)
	\end{split}
\end{align}

e analogamente per $ \pi_{S} $: i cambi di carta sono lisci dunque le strutture differenziabili sono equivalenti.

%

\newpage

%

\section{}\label{es2-5}

\begin{tcolorbox}
	Dimostrare che la Grassmanniana $ G(k,n) $ è uno spazio topologico connesso e compatto.
\end{tcolorbox}

\paragraph{Connessione}

Siccome il quoziente preserva la proprietà di connessione, è sufficiente mostrare che lo spazio delle matrici di rango $ k $, i.e. $ F(k,n) $, sia connesso per dimostrare che lo sia $ G(k,n) $.\\
Uno spazio topologico connesso per archi è anche connesso, dunque dimostriamo che per qualunque coppia di matrici di $ F(k,n) $ sia possibile costruire una funzione continua che le colleghi, i.e. preso $ I = [0,1] \subset \R $

\begin{gather}
	F(k,n) \text{ connesso per archi} \nonumber\\
	\Updownarrow\\
	\forall A, B \in F(k,n), \E f : I \to F(k,n) \text{ continua} %
	\, \mid \, f(0) = A \, \wedge \, f(1) = B \nonumber
\end{gather}

Siano due matrici invertibili $ P \in GL_{n}(\R) $ e $ Q \in GL_{k}(\R) $, una qualsiasi matrice $ A \in F(k,n) $ può essere scritta come

\begin{equation}
	A = P J Q \qcomma J \doteq \pmqty{I_{k} \\ 0_{n-k,k}}
\end{equation}

Consideriamo ora le due applicazioni continue seguenti:

\map{P_{t}}
	{I}{GL_{n}(\R)}
	{t}{P_{t}}

dove

\begin{equation}
	\begin{cases}
		P_{t}(0) = P_{0} = P\\
		P_{t}(1) = P_{1} = \diag(1,\dots,1,\det(P))
	\end{cases}
\end{equation}

\map{Q_{t}}
	{I}{GL_{n}(\R)}
	{t}{Q_{t}}

dove

\begin{equation}
	\begin{cases}
		Q_{t}(0) = Q_{0} = Q\\
		Q_{t}(1) = Q_{1} = \diag(1,\dots,1,\det(Q))
	\end{cases}
\end{equation}

Queste sono continue perché ognuna risiede in una delle componenti connesse dell'insieme delle matrici invertibili, i.e. in una delle seguenti due parti

\begin{equation}
	GL_{m}(\R) = \{ A \in GL_{m}(\R) \mid \det(A) > 0 \} \sqcup \{ A \in GL_{m}(\R) \mid \det(A) < 0 \}
\end{equation}

A questo punto, consideriamo l'applicazione

\map{f}
	{I}{F(k,n)}
	{t}{P_{t} J Q_{t}}

dove

\begin{equation}
	\begin{cases}
		f(0) = P_{0} J Q_{0} = A\\
		f(1) = P_{1} J Q_{1} = J
	\end{cases}
\end{equation}

La continuità di questa funzione deriva dalla continuità di $ P_{t} $ e $ Q_{t} $.\\
Collegando ogni matrice di $ F(k,n) $ alla matrice $ J $ (anch'essa in $ F(k,n) $ in quanto di rango $ k $) è possibile collegare queste matrici tra loro, dimostrando che $ F(k,n) $ è connesso e dunque lo è anche la Grassmanniana.

\paragraph{Compattezza}

Possiamo pensare alla Grassmanniana anche come quoziente di una \textit{varietà differenziale di Stiefel}\footnote{%
	Una varietà differenziale di Stiefel $ V_{k}(\R^{n}) $ è l'insieme di tutte le basi ortonormali di dimensione $ k $, i.e. le $ k $-uple di vettori linearmente indipendenti e normalizzati; questa varietà è un sottoinsieme di $ \R^{n} $.%
}

\begin{equation}
	G(k,n) = \dfrac{V_{k}(\R^{n})}{\sim}
\end{equation}

dove due basi di $ V_{k}(\R^{n}) $ sono equivalenti se generano lo stesso $ k $-spazio.\\
Siccome il quoziente preserva anche la proprietà di compattezza, è sufficiente mostrare che la varietà di Stiefel sia connessa per dimostrare che lo sia la Grassmanniana.\\
Sappiamo che se un sottoinsieme di $ \R^{n} $ è chiuso e limitato allora è compatto: dato che $ V_{k}(\R^{n}) \subset \R^{n} $ è chiusa e limitata, allora è compatta e dunque lo è anche la Grassmanniana.

%

\newpage

%

\section{}\label{es2-6}

\begin{tcolorbox}
	Siano $ M $ e $ N $ due varietà differenziabili e $ q_{0} \in N $. Dimostrare che
	
	\map{i_{q_{0}}}%
		{M}{M \times N}%
		{p}{(p,q_{0})}
	
	è un'applicazione liscia.
\end{tcolorbox}

Prendiamo gli atlanti differenziabili $ \{(U,\phi)\} \in M $ e $ \{(U \times V,\phi \times \psi)\} \in M \times N $: perché $ i_{q_{0}} $ sia liscia questa deve essere continua (lo è in quanto inclusione) e la composizione

\map{(\phi \times \psi) \circ i_{q_{0}} \circ \phi^{-1}}%
	{\phi( i_{q_{0}}^{-1}(U \times V) \cap U )}{\R^{m+n}}%
	{\phi(p)}{(p^{1},\dots,p^{m},q_{0}^{1},\dots,q_{0}^{n})}

deve essere liscia. Per dimostrare che lo sia, dalla Proposizione \ref{map-comp}, basta mostrare che le sue componenti siano lisce: presa la proiezione sulla $ k $-esima componente

\map{r^{k}}%
	{\R^{n}}{\R}%
	{(x^{1},\dots,x^{n})}{x^{k}}
	
le componenti della composizione sono date dalla seguente applicazione

\map{r^{k} \circ (\phi \times \psi) \circ i_{q_{0}} \circ \phi^{-1}}%
	{\phi( i_{q_{0}}^{-1}(U \times V) \cap U )}{\R}%
	{\phi(p)}{%
		\begin{cases}
			p^{k} & k \leqslant m\\
			q_{0}^{k} & k > m
		\end{cases}%
	}

le quali sono tutte lisce.

%

\newpage

%

\section{}\label{es2-7}

\begin{tcolorbox}
	Sia $ \S^{1} $ il cerchio unitario di $ \R^{2} $. Dimostrare che una funzione liscia $ f : \R^{2} \to \R $ si restringe a una funzione liscia $ f_{|\S^{1}} : \S^{1} \to \R $.
\end{tcolorbox}

Presa una carta $ (U,\phi) $ della struttura differenziabile di $ \R^{2} $, perché $ f $ sia liscia è necessario che sia liscia la composizione

\begin{equation}
	f \circ \phi^{-1} : \phi(U) \to \R
\end{equation}

con $ U \subset \R^{2} $ aperto.\\
Siccome la definizione non dipende dalla carta scelta e dal fatto che un diffeomorfismo

\begin{equation}
	g : U \to g(U) \subset \R^{2}
\end{equation}

definisce una carta della struttura differenziale $ (U,g) $\footnote{%
	Vedi Proposizione \ref{diffeo-map}.%
}, possiamo prendere il diffeomorfismo

\map{g}%
	{\R^{2}}{\S^{1}}%
	{(x,y)}{\left( \dfrac{x}{\sqrt{x^{2} + y^{2}}},\dfrac{y}{\sqrt{x^{2} + y^{2}}} \right)}
	
e dunque la carta $ (\S^{1},g) $ della struttura differenziale di $ \R^{2} $ per riscrivere la definizione di funzione liscia, dunque

\begin{equation}
	f \circ g^{-1} : g(\R^{2}) = \S^{1} \to \R
\end{equation}

la quale può essere riscritta come

\begin{equation}
	f_{|\S^{1}} : \S^{1} \to \R
\end{equation}

%

\newpage

%

\section{}\label{es2-8}

\begin{tcolorbox}
	Sia un'applicazione
	
	\begin{align}
		\begin{split}
			F : \R^{2} &\to \R^{3}\\
			(x,y) &\mapsto (x,y,xy)
		\end{split}
	\end{align}
	
	e $ p = (x,y) \in \R^{2} $. Trovare $ a,b,c \in \R $ tali che:
	
	\begin{equation}
		F_{*p} \left( \left. \dfrac{\partial}{\partial x} \right|_{p} \right) = a \left. \dfrac{\partial}{\partial u} \right|_{F(p)} + b \left. \dfrac{\partial}{\partial v} \right|_{F(p)} + c \left. \dfrac{\partial}{\partial w} \right|_{F(p)}
	\end{equation}
\end{tcolorbox}

Per trovare i coefficienti dell'immagine del differenziale ricordiamo che, per una funzione tra varietà differenziabili $ F : N \to M $ con carte

\begin{equation}
	\begin{cases}
		(U,\phi) \in N, & \phi = (x^{1},\dots,x^{n})\\
		(V,\psi) \in N, & \psi = (y^{1},\dots,y^{m})
	\end{cases}
\end{equation}

vale

\begin{equation}
	F_{*p} \left( \eval{\pdv{x^{j}}}_{p} \right) = \sum_{k=1}^{m} \left( \pdv{F^{k}}{x^{j}}\ (p) \right) \left( \eval{\pdv{y^{k}}}_{F(p)} \right)
\end{equation}

dove $ F^{k} = y^{k} \circ F $.\\
Essendo  il differenziale un'applicazione lineare, possiamo anche scrivere

\begin{equation}
	F_{*p} (X_{p}) = A(p) X_{p} = Y_{F(p)} \in T_{F(p)}(M)
\end{equation}

dove

\begin{equation}
	A(p) = \left[ \pdv{F^{k}}{x^{j}}\ (p) \right]
\end{equation}

Per la funzione considerata, abbiamo che

\begin{equation}
	A(x,y) = \mqty( 1 & 0 \\ 0 & 1 \\ y & x )
\end{equation}

Possiamo identificare $ T_{p}(\R^{k}) = \R^{k} $ dunque

\begin{align}
	\begin{split}
		\left( \mu \eval{\pdv{x}}_{p}, \nu \eval{\pdv{y}}_{p} \right) &\to (\mu,\nu)\\
		\left( a \eval{\pdv{u}}_{F(p)}, b \eval{\pdv{v}}_{F(p)}, c \eval{\pdv{w}}_{F(p)} \right) &\to (a,b,c)
	\end{split}
\end{align}

dunque

\begin{equation}
	F_{*p} \left( \mqty( \mu \\ \nu ) \right) = \mqty( 1 & 0 \\ 0 & 1 \\ y & x ) \mqty( \mu \\ \nu ) = \mqty( \mu \\ \nu \\ y \mu + x \nu )
\end{equation}

perciò abbiamo che

\begin{equation}
	F_{*p} \left( \mqty( 1 \\ 0 ) \right) = \mqty( 1 & 0 \\ 0 & 1 \\ y & x ) \mqty( 1 \\ 0 ) = \mqty( 1 \\ 0 \\ y ) = \eval{\pdv{u}}_{F(p)} + y \eval{\pdv{w}}_{F(p)}
\end{equation}

i.e. $ (a,b,c) = (1,0,y) $.

%

\newpage

%

\section{}\label{es2-9}

\begin{tcolorbox}
	Siano $ x $ e $ y $ le coordinate standard su $ \R^{2} $ e $ U = \R^{2} \setminus \{(0,0)\} $. In $ U $ le coordinate polari $ (\rho, \theta) $ con $ \rho > 0 $ e $ \theta \in (0,2\pi) $ sono definite come
	
	\begin{equation}
		\begin{cases}
			x = \rho \cos(\theta)\\
			y = \rho \sin(\theta)
		\end{cases}
	\end{equation}
	
	Si scrivano $ \sfrac{\partial}{\partial \rho} $ e $ \sfrac{\partial}{\partial \theta} $ in funzione di $ \sfrac{\partial}{\partial x} $ e $ \sfrac{\partial}{\partial y} $.
\end{tcolorbox}

L'equazione che lega i vettori di una base dello spazio tangente

\begin{equation}
	T_{p}(\R^{2} \setminus \{(0,0)\}) = \R^{2} \setminus \{(0,0)\}
\end{equation}

a un'altra base è la seguente:

\begin{equation}
	\eval{\pdv{u^{j}}}_{p} = \sum_{k=1}^{m} \left( \pdv{v^{k}}{u^{j}}\ (p) \right) \left( \eval{\pdv{v^{k}}}_{p} \right)
\end{equation}

dove $ u = (\rho,\theta) $ e $ v = (x,y) $.\\
In questo caso, possiamo scrivere i vettori dello spazio tangente come:

\begin{align}
	\begin{split}
		X_{p} = \left( \eval{\pdv{x}}_{p}, \eval{\pdv{y}}_{p} \right) &\doteq (\mu,\nu)\\\\
		Y_{p} = \left( \eval{\pdv{\rho}}_{p}, \eval{\pdv{\theta}}_{p} \right) &\doteq (a,b)
	\end{split}
\end{align}

La matrice di trasformazione è

\begin{equation}
	T = \left[ \pdv{v^{k}}{u^{j}}\ (p) \right] = \mqty( \pdv{x}{\rho} & \pdv{x}{\theta} \\\\ \pdv{y}{\rho} & \pdv{y}{\theta} ) %
	= \mqty( \cos(\theta) & - \rho \sin(\theta) \\\\ \sin(\theta) & \rho \cos(\theta) )
\end{equation}

perciò

\begin{equation}
	Y_{p} = T X_{p} %
	\implies %
	\mqty( a \\ b ) = \mqty( \cos(\theta) & - \rho \sin(\theta) \\\\ \sin(\theta) & \rho \cos(\theta) ) \mqty( \mu \\ \nu ) = \mqty( \cos(\theta) \mu - \rho \sin(\theta) \nu \\\\ \sin(\theta) \mu + \rho \cos(\theta) \nu )
\end{equation}

dunque

\begin{align}
	\begin{split}
		\eval{\pdv{\rho}}_{p} &= \cos(\theta) \eval{\pdv{x}}_{p} - \rho \sin(\theta) \eval{\pdv{y}}_{p}\\\\
		\eval{\pdv{\theta}}_{p} &= \sin(\theta) \eval{\pdv{x}}_{p} + \rho \cos(\theta) \eval{\pdv{y}}_{p}
	\end{split}
\end{align}

%

\newpage

%

\section{}\label{es2-10}

\begin{tcolorbox}
	Sia $ p = (x,y) $ un punto di $ \R^{2} $. Allora
	
	\begin{equation}
		c_{p}(t) = \mqty( \cos(2t) & -\sin(2t) \\\\ \sin(2t) & \cos(2t) ) \mqty(x \\ y)
	\end{equation}
	
	è una curva liscia in $ \R^{2} $ che inizia in $ p $. Calcolare $ c'(0) $.
\end{tcolorbox}

Considerando $ N = \R^{2} $ con carta $ (U,\phi) $ dove $ \phi = (x^{1},\dots,x^{n}) $, il vettore velocità della curva

\map{c}%
	{\R}{\R^{2}}%
	{t}{A(t) \, p = \mqty( \cos(2t) & -\sin(2t) \\\\ \sin(2t) & \cos(2t) ) \mqty(x \\ y)}

è dato da

\begin{equation}
	c'(t_{0}) = \sum_{j=1}^{n} \dot{c}(t_{0}) \eval{\pdv{x^{j}}}_{t_{0}}%
	\implies%
	c'(0) = \dot{c}_{1}(0) \eval{\pdv{x}}_{0} + \dot{c}_{2}(0) \eval{\pdv{y}}_{0}
\end{equation}

Calcoliamo dunque le componenti del vettore tangente:

\begin{align}
	\begin{split}
		\dot{c}(0) &= \eval{\dv{t}}_{0}(A(t) \, p)\\
		&= \left( \eval{\dv{t}}_{0} A(t) \right) p\\
		&= \eval{ \dv{t} \left( \mqty( \cos(2t) & -\sin(2t) \\\\ \sin(2t) & \cos(2t) ) \right) }_{0} \mqty(x \\ y)\\
		&= 2 \eval{ \mqty( -\sin(2t) & -\cos(2t) \\\\ \cos(2t) & -\sin(2t) ) }_{0} \mqty(x \\ y)\\
		&= 2 \mqty( 0 & -1 \\ 1 & 0 ) \mqty(x \\ y)\\
		&= \mqty(-2y \\ 2x)
	\end{split}
\end{align}

da cui

\begin{equation}
	c'(0) = -2y \eval{\pdv{x}}_{0} + 2x \eval{\pdv{y}}_{0}
\end{equation}

%

\newpage

%

\section{}\label{es2-11}

\begin{tcolorbox}
	Siano $ N $ e $ M $ varietà differenziabili e $ \pi_{N} : N \times M \to N $ e $ \pi_{M} : N \times M \to M $ le proiezioni naturali. Dimostrare che per $ (p,q) \in N \times M $ l'applicazione
	
	\begin{equation}
		(\pi_{N_{*p}},\pi_{M_{*p}}) : T_{(p,q)}(N \times M) \to T_{p}(N) \times T_{q}(M)
	\end{equation}
	
	è un isomorfismo.
\end{tcolorbox}

qui

% -----------------------

% fine pdf 2.1-2.3

% -----------------------

%

\newpage

%

\section{}\label{es2-12}

\begin{tcolorbox}
	Siano $ S_{1} $ e $ S_{2} $ due sottovarietà di due varietà differenziabili $ M_{1} $ e $ M_{2} $ rispettivamente. Dimostrare che $ S_{1} \times S_{2} $ è una sottovarietà di $ M_{1} \times M_{2} $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-13}

\begin{tcolorbox}
	Sia l'applicazione
	
	\begin{align}
		\begin{split}
			F : \R^{2} &\to \R\\
			(x,y) &\mapsto x^{2}-6xy+y^{2}
		\end{split}
	\end{align}

	Trovare i $ c \in \R $ tali che $ F^{-1}(c) $ sia una sottovarietà di $ \R^{2} $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-14}

\begin{tcolorbox}
	Dire se le soluzione del sistema
	
	\begin{equation}
		\begin{cases}
			x^{3} + y^{3} + z^{3} = 1\\
			z = xy
		\end{cases}
	\end{equation}

	costituiscono una sottovarietà di $ \R^{3} $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{BONUS2-3}

\begin{tcolorbox}
	Sia la sottovarietà di $ \R^{3} $
	
	\begin{equation}
		S = \{ (x,y,z) \in \R^{3} \mid x^{3} + y^{3} + z^{3} = 1 \wedge x+y+z=0 \} \subset \R^{3}
	\end{equation}
	
	Calcolare lo spazio tangente $ T_{p}(S) $ con $ p \in S $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-15}

\begin{tcolorbox}
	Un polinomio $ F(x_{0},\dots,x_{n}) \in \R[x_{0},\dots,x_{n}] $ è omogeneo di grado $ k $ se è combinazione lineare di monomi $ x_{0}^{i_{1}} \cdots x_{n}^{i_{m}} $ di grado $ k $ tale che $ \sum_{j=1}^{m} i_{j} = k $. Dimostrare che
	
	\begin{equation}
		\sum_{i=0}^{n} x^{i} \, \dfrac{\partial F}{\partial x^{i}} = k F
	\end{equation}

	Dedurre che $ F^{-1}(c) $ con $ c \neq 0 $ è una sottovarietà di $ \R^{n} $ di dimensione $ n-1 $. Dimostrare inoltre che per $ c,d>0 $ si ha che $ F^{-1}(c) \simeq F^{-1}(d) $ diffeomorfe e lo stesso vale per $ c,d<0 $.\\\\
	Suggerimento per la prima parte: usare l'uguaglianza
	
	\begin{equation}
		F(\lambda x_{0},\dots,\lambda x_{n}) = \lambda^{k} F(x_{0},\dots,x_{n})
	\end{equation}
	
	valida per ogni $ \lambda \in \R $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-16}

\begin{tcolorbox}
	Dimostrare che
	
	\begin{equation}
		\mathbb{SL}_{n}(\C) = \{ A \in M_{n}(\C) \mid \det(A) \neq 0 \} \subset M_{n}(\C)
	\end{equation}

	è una sottovarietà di $ M_{n}(\C) $ con $ \dim(\mathbb{SL}_{n}(\C)) = 2n^{2}-2  $
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-17}

\begin{tcolorbox}
	Sia $ F : N \to M $ un'applicazione liscia tra varietà differenziabili. Dimostrare che l'insieme $ \mathcal{PR}_{F} $ dei punti regolari di $ F $ è un aperto di $ N $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-18}

\begin{tcolorbox}
		Sia $ F : N \to M $ un'applicazione liscia tra varietà differenziabili. Dimostrare che se $ F $ è chiusa allora  l'insieme $ \mathcal{VR}_{F} $ dei valori regolari di $ F $ è un aperto in $ M $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-19}

\begin{tcolorbox}
	Dimostrare che l'applicazione
	
	\begin{align}
		\begin{split}
			F : \R &\to \R^{3}\\
			t &\mapsto (t,t^{2},t^{3})
		\end{split}
	\end{align}

	è un embedding liscio e scrivere $ F(\R) $ come zero di funzioni.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-20}

\begin{tcolorbox}
	Dimostrare che l'applicazione
	
	\begin{align}
		\begin{split}
			F : \R &\to \R^{3}\\
			t &\mapsto (\cosh(t),\sinh(t))
		\end{split}
	\end{align}
	
	è un embedding liscio e che
	
	\begin{equation}
		F(\R) = \{ (x,y) \in \R^{2} \mid x^{2}-y^{2} = 1 \}
	\end{equation}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-21}

\begin{tcolorbox}
	Dimostrare che la composizione di immersioni è un’immersione e che il prodotto cartesiano di due immersioni è un’immersione.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-22}

\begin{tcolorbox}
	Dimostrare che se $ F : N \to M $ è un'immersione e $ Z \subset N $ è una sottovarietà di $ N $ allora $ F_{|Z} : Z \to M $ è un'immersione.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-23}

\begin{tcolorbox}
	Dimostrare che l'applicazione
	
	\begin{align}
		\begin{split}
			F : \S^{2} &\to \R^{4}\\
			(x,y,z) &\mapsto (x^{2}-y^{2},xy,xz,yz)
		\end{split}
	\end{align}
	
	induce un embedding liscio da $ \rp{2} $ a $ \R^{4} $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-24}

\begin{tcolorbox}
	Dimostrare che un'immersione iniettiva e propria è un embedding liscio. Mostrare che esistono embedding lisci che non sono applicazione proprie.\\\\
	Ricordare che un'applicazione continua $ f : X \to Y $ tra spazi topologici è propria se $ f^{-1}(K) $ è compatto in $ X $ per ogni compatto $ K $ di $ Y $.
\end{tcolorbox}

qui

% -----------------------

% fine pdf 2.4

% -----------------------

%

\newpage

%

\section{}\label{es2-25}

\begin{tcolorbox}
	Sia $ N $ una sottovarietà di una varietà differenziabile $ M $. Dimostrare che $ T(N) $ è una sottovarietà di $ T(M) $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{BONUS2-4}

\begin{tcolorbox}
	Verificare che la sfera $ \S^{1} $ sia parallelizzabile.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-26}

\begin{tcolorbox}
	Una varietà differenziabile $ N $ è detta \textit{orientabile} se esiste un atlante di $ N $ rispetto al quale il determinante jacobiano dei cambi di carte è positivo. Dimostrare che:
	
	\begin{enumerate}
		\item $ \rp{3} $ è una varietà orientabile;
		\item il fibrato tangente $ T(N) $ di una varietà differenziabile $ N $ è orientabile.
	\end{enumerate}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-27}

\begin{tcolorbox}
	Dimostrare che l'applicazione $ \mathcal{F}_{*} $ che associa a ogni varietà differenziabile il suo fibrato tangente e a ogni applicazione $ F : N \to M $ tra varietà differenziabili il suo differenziale $ F_{*} : T(N) \to T(M) $ definita come
	
	\begin{equation}
		\mathcal{F}_{*}((p,v)) = (F(p), F_{*p}(v))
	\end{equation}

	per $ \forall (p,v) \in T(N) $ definisce un funtore covariante dalla categoria delle varietà differenziabili in sé stessa.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-28}

\begin{tcolorbox}
	Una \textit{derivazione} di un'algebra di Lie $ (V,[\cdot,\cdot]) $ su un campo $ \K $ è un'applicazione lineare $ D : V \to V $ tale che
	
	\begin{equation}
		D([Y,Z]) = [DY,Z] + [Y,DZ]
	\end{equation}

	per $ \forall Y,Z \in V $. Dimostrare che, dato $ X \in V $, l'applicazione
	
	\begin{align}
		\begin{split}
			D_{X} : V &\to V\\
			Y &\mapsto [X,Y]
		\end{split}
	\end{align}

	è una derivazione.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-29}

\begin{tcolorbox}
	Siano $ M = \R \setminus 0 $ e $ X = \sfrac{\operatorname{d}}{\operatorname{dx}} \in \chi(M) $. Trovare la curva integrale di $ X $ massimale che inizia in un generico punto $ p \in \R $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-30}

\begin{tcolorbox}
	Trovare il flusso (locale) dei seguenti campi di vettori in $ \chi(\R^{2}) $:
	
	\begin{equation}
		\begin{cases}
			X = x \, \dfrac{\partial}{\partial x} - y \, \dfrac{\partial}{\partial y}\\\\
			Y = x \, \dfrac{\partial}{\partial x} + y \, \dfrac{\partial}{\partial y}\\\\
			Z = \dfrac{\partial}{\partial x} + y \, \dfrac{\partial}{\partial y}
		\end{cases}
	\end{equation}

	Nel caso siano completi, calcolare il loro gruppo di diffeomorfismi a un parametro.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-31}

\begin{tcolorbox}
	Dimostrare che il campo di vettori $ X = \sfrac{\partial}{\partial x} \in \chi(\R^{2} \setminus (0,0)) $ non è completo.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-32}

\begin{tcolorbox}
	Sia $ M $ una varietà differenziabile e $ X \in \chi(M) $ tale che $ X(p) = 0 $ in un punto $ p \in M $. Dimostrare che la curva integrale di $ X $ che inizia in $ p $ è la curva costante $ c(t)=p $.
\end{tcolorbox}

\textbf{l 26 m 36}

%

\newpage

%

\section{}\label{es2-33}

\begin{tcolorbox}
	Sia $ M $ una varietà differenziabile e $ X \in \chi(M) $ il campo di vettori nullo, i.e. $ X = 0 $. Descrivere il gruppo dei diffeomorfismi a un parametro associato a $ X $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es2-34}

\begin{tcolorbox}
	Siano $ F : N \to M $ un diffeomorfismo tra varietà differenziabili, $ X \in \chi(N) $ e $ f \in C^{\infty}(N) $. Dimostrare che
	
	\begin{equation}
		F_{*}(f X) = (f \circ F^{-1}) \, F_{*} X
	\end{equation}
\end{tcolorbox}

qui

\chapter{Esercizi: Gruppi e algebre di Lie}

\section{}\label{es3-1}

\begin{tcolorbox}
	Dimostrare che il prodotto diretto di due gruppi di Lie è un gruppo di Lie.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-2}

\begin{tcolorbox}
	Siano la proiezione
	
	\begin{align}
		\begin{split}
			\pi : \R^{2} &\to \S^{1} \times \S^{1}\\
			(t,s) &\mapsto (e^{2 \pi i t},e^{2 \pi i s})
		\end{split}
	\end{align}

	l'insieme
	
	\begin{equation}
		L = \{ (t,\alpha t) \in \R^{2} \mid \alpha \in \R \setminus \Q \}
	\end{equation}

	e la restrizione di $ \pi $ a $ L $, i.e. $ f = \pi_{|L} : L \to \S^{1} \times \S^{1} $. Siano
	
	\begin{itemize}
		\item $ \tau_{f} $ la topologia indotta da $ f $ su $ H = \pi(L) $;
		
		\item $ \tau_{s} $ la topologia indotta dall'inclusione $ H \subset \S^{1} \times \S^{1} $
	\end{itemize}

	Dimostrare che $ \tau_{s} \subset \tau_{f} $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-3}

\begin{tcolorbox}
	Sia la matrice
	
	\begin{equation}
		X = \begin{pmatrix} 0 & 1 \\\\ 1 & 0 \end{pmatrix}
	\end{equation}

	Dimostrare che
	
	\begin{equation}
		e^{X} = \begin{pmatrix} \cosh(1) & \sinh(1) \\\\ \sinh(1) & \cosh(1) \end{pmatrix}
	\end{equation}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-4}

\begin{tcolorbox}
	Trovare due matrici $ A $ e $ B $ tali che
	
	\begin{equation}
		e^{A+B} \neq e^{A} e^{B}
	\end{equation}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-5}

\begin{tcolorbox}
	Dimostrare che il gruppo unitario $ U(n) $ è compatto per ogni $ n \geqslant 1 $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-6}

\begin{tcolorbox}
	Siano $ G $ un gruppo di Lie e $ G_{0} $ la componente connessa di $ G $ che contiene $ e $ (elemento neutro di $ G $). Se $ \mu $ e $ i $ denotano rispettivamente la moltiplicazione e l'inversione in $ G $, provare che:
	
	\begin{enumerate}
		\item $ \mu(\{x\} \times G_{0}) \subset G_{0} $ per $ \forall x \in G $;
		
		\item $ i(G_{0}) \subset G_{0} $;
		
		\item $ G_{0} $ è un sottoinsieme aperto di $ G $;
		
		\item $ G_{0} $ è un sottogruppo di Lie di $ G $
	\end{enumerate}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{BONUS3-1}

\begin{tcolorbox}
	Verificare che $ SO(2) $ sia diffeomorfo a $ \S^{1} $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{BONUS3-2}

\begin{tcolorbox}
	Verificare che $ SU(2) $ sia diffeomorfo a $ \S^{3} $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-7}

\begin{tcolorbox}
	Sia $ G $ un gruppo di Lie con moltiplicazione $ \mu : G \times G \to G $. Dimostrare che
	
	\begin{equation}
		\mu_{*(a,b)}(X_{a},Y_{b}) = (R_{b})_{*a}(X_{a}) + (L_{a})_{*b}(Y_{b}) \qcomma \forall (a,b) \in G \times G, \, \forall X_{a} \in T_{a}(G), \, \forall Y_{b} \in T_{b}(G)
	\end{equation}

	dove $ L_{a} $ (risp. $ R_{a} $) denota la traslazione a sinistra (risp. a destra) associata ad $ a $ (risp. $ b $).
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-8}

\begin{tcolorbox}
		Sia $ G $ un gruppo di Lie con inversione $ i : G \to G $. Dimostrare che
	
	\begin{equation}
		i_{*a}(Y_{a}) = -(R_{a^{-1}})_{*e}(L_{a^{-1}})_{*e} (Y_{a}) \qcomma \forall a \in G, \, \forall Y_{a} \in T_{a}(G)
	\end{equation}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-9}

\begin{tcolorbox}
	Verificare che il commutatore tra matrici
	
	\begin{equation}
		[A,B] = AB - BA
	\end{equation}

	definisce un'algebra di Lie sullo spazio tangente all'identità dei gruppi:
	
	\begin{equation}
		\begin{cases}
			O(n)\\
			SO(n)\\
			U(n)\\
			SU(n)\\
			SL_{n}(\R)\\
			SL_{n}(\C)
		\end{cases}
	\end{equation}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-10}

\begin{tcolorbox}
	Verificare che l'esponenziale di una matrice definisce un'applicazione
	
	\begin{align}
		\begin{split}
			e : T_{I_{n}}(G) &\to G\\
			A &\mapsto e^{A}
		\end{split}
	\end{align}

	per i gruppi di Lie
	
	\begin{equation}
		\begin{cases}
			GL_{n}(\R)\\
			GL_{n}(\C)\\
			O(n)\\
			SO(n)\\
			U(n)\\
			SU(n)\\
			SL_{n}(\R)\\
			SL_{n}(\C)
		\end{cases}
	\end{equation}
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-11}

\begin{tcolorbox}
	Sia $ G = G_{1} \times \cdots \times G_{s} $ il prodotto diretto di gruppi di Lie. Dimostrare che l'algebra di Lie di $ G $ è isomorfa alla somma diretta delle algebre di Lie dei $ G_{i} $ con $ i=1,\dots,n $.
\end{tcolorbox}

qui

%

\newpage

%

\section{}\label{es3-12}

\begin{tcolorbox}
	Dimostrare che ogni gruppo di Lie è parallelizzabile.
\end{tcolorbox}

qui
