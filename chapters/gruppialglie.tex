\section{Gruppi di Lie}

Un gruppo $ G $ è un \textit{gruppo di Lie} se l'insieme del gruppo è una varietà differenziabile, è un gruppo algebrico e se le sue operazioni

\begin{align}
	\begin{split}
		\mu : G \times G &\to G\\
		(a,b) &\mapsto a b\\\\
		%
		i : G &\to G\\
		a &\mapsto a^{-1}
	\end{split}
\end{align}

sono lisce.\\\\
%
Le traslazioni possono essere a sinistra e a destra:

\begin{itemize}
	\item Dato $ a \in G $, la traslazione a sinistra
	
	\map{L_{a}}{G}{G}{b}{a b}
	
	è un'applicazione liscia, in quanto restrizione di un'applicazione liscia (i.e. $ L_{a} = \mu(a,\cdot) $), ed è inoltre un diffeomorfismo, in quanto $ L_{a}^{-1} = L_{a^{-1}} $ è ancora liscia;
	
	\item Dato $ a \in G $, la traslazione a destra
	
	\map{R_{a}}{G}{G}{b}{b a}
	
	è un'applicazione liscia, in quanto restrizione di un'applicazione liscia (i.e. $ R_{a} = \mu(\cdot,a) $), ed è inoltre un diffeomorfismo, in quanto $ R_{a}^{-1} = R_{a^{-1}} $ è ancora liscia.
\end{itemize}

\subsubsection{\textit{Esempi}}

\paragraph{1. Gruppo lineare $ GL_{n}(\R) $}

Il gruppo delle matrici invertibili

\begin{equation}
	GL_{n}(\R) = \{ A \in M_{n}(\R) \mid \det(A) \neq 0 \} \subset M_{n}(\R) = \R^{n^{2}}
\end{equation}

con la struttura differenziale ereditata da $ M_{n}(\mathbb{R}) = \mathbb{R}^{n^{2}} $ in quanto $ GL_{n}(\mathbb{R}) $ è aperto in questo spazio, è un gruppo di Lie rispetto alla moltiplicazione e l'inversione:

\begin{align}
	\begin{split}
		\mu : GL_{n}(\R) \times GL_{n}(\R) &\to GL_{n}(\R)\\
		(A,B) &\mapsto A B\\\\
		%
		i : GL_{n}(\R) &\to GL_{n}(\R)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

le quali sono lisce.

\paragraph{2. Gruppo lineare speciale $ SL_{n}(\R) $}

Il gruppo delle matrici con determinante unitario

\begin{equation}
	SL_{n}(\R) = \{ A \in M_{n}(\R) \mid \det(A) = 1 \} \subset M_{n}(\R) = \R^{n^{2}}
\end{equation}

è un gruppo di Lie rispetto alla moltiplicazione e l'inversione:

\begin{align}
	\begin{split}
		\mu : SL_{n}(\R) \times SL_{n}(\R) &\to SL_{n}(\R)\\
		(A,B) &\mapsto A B\\\\
		%
		i : SL_{n}(\R) &\to SL_{n}(\R)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

in quanto sottovarietà di $ GL_{n}(\R) $ (e quindi una varietà) di dimensione $ n^{2}-1 $ e gruppo algebrico rispetto alle sue operazioni, le quali sono lisce.\\
Dimostriamo ora che la moltiplicazione è liscia\footnote{%
	Questa dimostrazione è già stata fatta nell'Esempio \ref{ex-slnr}.%
}: prendiamo la moltiplicazione (liscia) in $ GL_{n}(\R) $

\map{F}{GL_{n}(\R) \times GL_{n}(\R)}{GL_{n}(\R)}{(A,B)}{A B}

e l'inclusione liscia

\begin{equation}
	i \times i : SL_{n}(\R) \times SL_{n}(\R) \to GL_{n}(\R) \times GL_{n}(\R)
\end{equation}

L'applicazione $ G = F \circ (i \times i) $ è dunque liscia perché composizione di applicazione lisce; siccome il prodotto di due matrici con determinante unitario (i.e. in $ SL_{n}(\R) $) è ancora una matrice con determinante unitario, l'immagine di $ G $

\begin{equation}
	G(SL_{n}(\R) \times SL_{n}(\R)) \subset SL_{n}(\R)
\end{equation}

A questo punto, dato che $ SL_{n}(\R) $ è una sottovarietà di $ GL_{n}(\R) $ (ipotesi del teorema che permette di mantenere l'applicazione liscia), possiamo considerare la restrizione del codominio della funzione $ G $

\map{\tilde{G}}{SL_{n}(\R) \times SL_{n}(\R)}{SL_{n}(\R)}{(A,B)}{A B}

la quale è identica a $ \mu $, dunque $ \mu \in C^{\infty}(SL_{n}(\R) \times SL_{n}(\R)) $.\\
Per dimostrare che l'inversione sia liscia, consideriamo l'inversione in $ GL_{n}(\R) $

\map{F}{GL_{n}(\R)}{GL_{n}(\R)}{A}{A^{-1}}

la quale è liscia e, analogamente per la moltiplicazione, definiamo $ G = F \circ i $

\map{G}{SL_{n}(\R)}{GL_{n}(\R)}{A}{A^{-1}}

e dunque la restrizione del suo codominio a $ SL_{n}(\R) $ (sottovarietà di $ GL_{n}(\R) $)

\map{\tilde{G}}{SL_{n}(\R)}{SL_{n}(\R)}{A}{A^{-1}}

funzione che coincide con l'inversione in $ SL_{n}(\R) $, rendendo dunque l'inversione  $ i $ liscia.

\paragraph{3. Gruppo ortogonale $ O(n) $}

Il gruppo delle matrici ortogonali

\begin{equation}
	O(n) = \{ A \in GL_{n}(\R) \mid A^{T} A = I \} \subset GL_{n}(\R)
\end{equation}

è una sottovarietà di $ GL_{n}(\R) $ (dal teorema della preimmagine di un'applicazione di rango costante).\\
Il ragionamento per cui $ O(n) $ sia un gruppo di Lie rispetto alla moltiplicazione e l'inversione

\begin{align}
	\begin{split}
		\mu : O(n) \times O(n) &\to O(n)\\
		(A,B) &\mapsto A B\\\\
		%
		i : O(n) &\to O(n)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

è analogo a quello fatto per $ SL_{n}(\R) $.\\
Calcoliamo ora la dimensione di $ O(n) $ come varietà e il suo spazio tangente nell'identità $ T_{I}(O(n)) $. Consideriamo l'insieme delle matrici simmetriche di ordine $ n $

\begin{equation}
	S(n) = \{ B \in M_{n}(\R) \mid B^{T} = B \}
\end{equation}

e l'applicazione liscia

\map{f}%
	{GL_{n}(\R)}{S(n)}%
	{A}{A^{T} A}

dove $ (A^{T} A)^{T} = A^{T} A $ dunque $ A^{T} A \in S(n) $. Dimostrando che $ I \in \mathcal{VR}_{f} $, per il teorema della preimmagine di un valore regolare, avremmo che $ O(n) $ è una sottovarietà di $ GL_{n}(\R) $ con dimensione

\begin{equation}
	\dim(O(n)) = \dim(GL_{n}(\R)) - \dim(S(n))
\end{equation}

dove $ S(n) $ è uno spazio vettoriale su $ \R $ e anche un sottospazio vettoriale di $ M_{n}(\R) $, la cui dimensione è data dalla quantità di condizioni necessarie al fine di determinare una matrice simmetrica, i.e.

\begin{equation}
	\dim(S(n)) = \dfrac{n (n+1)}{2}
\end{equation}

Siccome $ \dim(GL_{n}(\R)) = n^{2} $, otteniamo che

\begin{equation}
	\dim(O(n)) = n^{2} - \dfrac{n (n+1)}{2} = \dfrac{n (n-1)}{2}
\end{equation}

Verifichiamo ora che $ I \in \mathcal{VR}_{f} $: per fare ciò, dobbiamo calcolare il differenziale di $ f $ e controllare che tutti i punti che stanno nell'immagine di $ I $ attraverso $ f_{*} $ siano punti regolari

\begin{equation}
	f_{*A} : T_{A}(GL_{n}(\R)) \to T_{f(A)}(S(n))
\end{equation}

possiamo identificare $ T_{A}(GL_{n}(\R)) = M_{n}(\R) $ e $ T_{f(A)}(S(n)) = S(n) $ (in quanto $ S(n) $ è uno spazio vettoriale), perciò $ f_{*} $ porta matrici in matrici simmetriche. Calcoliamo dunque il differenziale $ f_{*A}(X) $ prendendo una curva che passi per $ A $ in 0 e il cui vettore tangente sia $ B $

\begin{equation}
	\begin{cases}
		c : (-\varepsilon,\varepsilon) \to GL_{n}(\R)\\
		c(0) = A\\
		c'(0) = \dot{c}(0) = X
	\end{cases}
\end{equation}

dove

\begin{equation}
	T_{A}(GL_{n}(\R)) = M_{n}(\R) \implies c'(0) = \dot{c}(0)
\end{equation}

perciò

\begin{align}
	\begin{split}
		f_{*A}(X) &= \left. \dfrac{\operatorname{d}}{\operatorname{dt}} f(c(t)) \right|_{t=0}\\
		&= \left. \dfrac{\operatorname{d}}{\operatorname{dt}} [c(t)^{T} c(t)] \right|_{t=0}\\
		&= \left. [ \dot{c}(t)^{T} c(t) + c(t)^{T} \dot{c}(t) ] \right|_{t=0}\\
		&= X^{T} A + X A^{T}
	\end{split}
\end{align}

i.e.

\map{f_{*A}}{M_{n}(\R)}{S(n)}{X}{X^{T} A + X A^{T}}

Vale la condizione

\begin{equation}
	I \in \mathcal{VR}_{f} %
	\iff%
	f_{*A} : M_{n}(\R) \to S(n) \text{ suriettiva} \qquad \forall A \in f^{-1}(I) = O_{n}
\end{equation}

Perché $ f_{*A} $ sia suriettiva

\begin{equation}
	\forall A \in O(n), \, \forall B \in S(n), \, \exists X \in M_{n}(\R) \mid X^{T} A + X A^{T} = B
\end{equation}

è sufficiente dunque prendere $ X = \sfrac{1}{2} AB $, i.e.

\begin{align}
	\begin{split}
		X^{T} A + X A^{T} &= \left( \dfrac{1}{2} A B \right)^{T} A + \dfrac{1}{2} A^{T} A B\\
		&= \dfrac{1}{2} B^{T} A^{T} A + \dfrac{1}{2} B\\
		&= \dfrac{1}{2} B + \dfrac{1}{2} B\\
		&= B
	\end{split}
\end{align}

dunque $ I \in \mathcal{VR}_{f} $ e $ \dim(O(n)) = n(n-1)/2 $. Da questo ragionamento, otteniamo anche lo spazio tangente a $ O(n) $ in $ I $ poiché, per il teorema della preimmagine di un valore regolare, vale la seguente uguaglianza

\begin{equation}
	T_{A}(f^{-1}(I)) = T_{A}(O(n)) = \ker(f_{*A}), \qquad A \in f^{-1}(I)
\end{equation}

perciò

\begin{equation}
	T_{I}(O(n)) = \ker(f_{*I})
\end{equation}

ma abbiamo che

\begin{equation}
	f_{*I}(X) = X^{T} + X
\end{equation}

perciò

\begin{equation}
	T_{I}(O(n)) = \ker(f_{*I}) = \{ X \in M_{n}(\R) \mid X^{T} = -X \}
\end{equation}

ovvero lo spazio tangente di $ O(n) $ è formato dalle matrici antisimmetriche (spazio vettoriale), il quale ha dimensione esattamente $ \dim(T_{I}(O(n))) = n(n-1)/2 $.

\subsection{Omomorfismi e isomorfismi}

Un \textit{omomorfismo} di gruppi\footnote{%
	Omomorfismo e omeomorfismo sono due concetti differenti legati a due parti differenti della matematica.%
} è un'applicazione (non necessariamente liscia) che preserva le moltiplicazioni di un gruppo nell'altro.

\begin{remark}
Siano $ F $ un omomorfismo, $ e_{H} \in H $ e $ e_{G} \in G $ gli elementi neutri dei rispettivi gruppi, allora $ F(e_{H}) = e_{G} $.
\end{remark}

Un omomorfismo di due gruppi di Lie $ H $ e $ G $ è un'applicazione liscia $ F : H \to G $ tale che sia un omomorfismo di gruppi, i.e.

\begin{equation}
	F \circ L_{h} = L_{F(h)} \circ F, \qquad \forall h \in H
\end{equation}

in quanto, se $ F $ è un omomorfismo di gruppi

\begin{align}
	\begin{split}
		F(h k) &= F(h) \, F(k)\\
		(F \circ L_{h})(k) &= (L_{F(h)} \circ F)(k)\\
		F \circ L_{h} &= L_{F(h)} \circ F
	\end{split}
\end{align}

Un \textit{isomorfismo di Lie} è un omomorfismo di gruppi che sia anche un diffeomorfismo.

\subsection{Sottogruppi di Lie}

Siano $ G $ un gruppo di Lie e $ H \neq \{\} $ un suo sottoinsieme, diremo che $ H $ è un \textit{sottogruppo di Lie} (immerso) di $ G $ se:

\begin{enumerate}
	\item $ H $ è un sottogruppo algebrico di $ G $, in notazione $ H < G $;
	
	\item $ H $ è una sottovarietà immersa di $ G $, i.e. è l'immagine di $ G $ tramite un'immersione iniettiva (la topologia di $ H $ non è necessariamente la topologia indotta da $ G $);
	
	\item Le operazioni di moltiplicazione e inversione, indotte da $ G $, sono lisce.
\end{enumerate}

\begin{remark}
	Se $ H \subset G $ sottogruppo algebrico ed $ H $ è una sottovarietà di $ G $, allora la terza condizione è superflua. Questo perché: consideriamo la moltiplicazione $ f : G \times G \to G $ in $ G $ e l'inclusione $ i : H \to G $, la loro composizione $ g = f \circ (i \times i) : H \times H \to G $ ha come immagine $ g(H \times H) \subset H $ perciò possiamo restringere il codominio di questa al solo insieme $ H $, ottenendo dunque la moltiplicazione $ \tilde{g} = \mu : H \times H \to H $ per $ H $, la quale è liscia per i teoremi sulle sottovarietà; il ragionamento è analogo per l'inversione.
\end{remark}

Se $ H \subset G $ sottogruppo algebrico ed $ H $ è una sottovarietà di $ G $, diremo che $ H $ un \textit{sottogruppo di Lie embedded}.\\
Ad esempio, $ SL_{n}(\R) $ e $ O(n) $ sono sottogruppi di Lie embedded di $ GL_{n}(\R) $, perché sottovarietà di quest'ultimo.

\begin{theorem}\label{th-liesub-var}
	Siano $ G $ e $ H $ varietà differenziabili, se $ H < G $ sottogruppo algebrico e $ H $ è chiuso in $ G $ (come sottospazio topologico) allora $ H $ è un sottogruppo embedded di $ G $.
\end{theorem}

Da questo teorema, possiamo ancora derivare che $ SL_{n}(\R) $ e $ O(n) $ siano sottogruppi di Lie embedded di $ GL_{n}(\R) $ (e quindi sottovarietà) in quanto varietà e chiusi:

\begin{itemize}
	\item $ SL_{n}(\R) $ è un chiuso in $ GL_{n}(\R) $ in quanto controimmagine di 1 (chiuso in $ \R $) tramite l'applicazione continua (porta chiusi in chiusi)
	
	\map{f}{GL_{n}(\R)}{\R}{A}{\det(A)}
	
	\item $ O(n) $ è un chiuso in $ GL_{n}(\R) $ in quanto controimmagine di $ I $ (chiuso in $ S(n) $) tramite l'applicazione
	
	\map{f}{GL_{n}(\R)}{S(n)}{A}{A^{T} A}
\end{itemize}

\subsubsection{\textit{Esempio}}

Questo esempio è riferito ad un sottogruppo di Lie immerso importante, il resto dei sottogruppi considerati, saranno sottogruppi di Lie embedded.\\
Consideriamo come gruppo di Lie il toro $ \T^{2} = \S^{1} \times \S^{1} $ ($ \S^{1} $ è un gruppo di Lie, quindi il prodotto diretto di due $ \S^{1} $ è ancora un gruppo di Lie) con le operazioni naturali e l'applicazione

\map{F}{\R}{\T^{2}}{t}{(e^{2 \pi i t},e^{2 \pi i \alpha t})}

dove $ \alpha \in \R \setminus \Q $, i.e. $ \alpha $ è irrazionale; questa applicazione è un'immersione iniettiva e un omomorfismo di gruppi, in quanto

\begin{align}
	\begin{split}
		F(t+s) &= (e^{2 \pi i (t+s)},e^{2 \pi i \alpha (t+s)})\\
		&= (e^{2 \pi i t} e^{2 \pi i s},e^{2 \pi i \alpha t} e^{2 \pi i \alpha s})\\
		&= (e^{2 \pi i t},e^{2 \pi i \alpha t}) (e^{2 \pi i s},e^{2 \pi i \alpha s})\\
		&= F(t) \, F(s)
	\end{split}
\end{align}

L'immagine di $ \R $ tramite $ F $ è un sottogruppo di $ \T^{2} $, i.e. $ F(\R) < \T^{2} $, è una sottovarietà immersa ($ F $ non è un embedding) di $ \T^{2} $ e le operazioni su $ F(\R) $ sono lisce, dunque $ H \doteq F(\R) $ è un sottogruppo di Lie (immerso) di $ \T^{2} $.

\section{Esponenziale di una matrice}

Sia una matrice quadrata $ X \in M_{n}(\R) $, definiamo l'\textit{esponenziale di matrice} come\footnote{%
In particolare $ X^{0} = I $.}

\begin{equation}
	e^{X} = \sum_{i=0}^{+\infty} \dfrac{X^{i}}{i!}, \qquad X \in M_{n}(\R)
\end{equation}

sulla falsa riga delle serie di potenze

\begin{equation}
	e^{x} = \sum_{i=0}^{+\infty} \dfrac{x^{i}}{i!}, \qquad x \in \R
\end{equation}

Non è però chiaro se la serie di matrici considerata sia convergente e dunque che $ e^{X} \in M_{n}(\R) $: questo è facilmente dimostrabile se la matrice all'esponente è

\begin{itemize}
	\item la matrice nulla $ X = 0_{n} $ da cui $ e^{0_{n}} = I_{n} = I $;
	
	\item la matrice è un multiplo della matrice identità $ X = x I_{n} $, i.e. $ X^{i} = x^{i} I_{n} $, da cui $ e^{X} = x I_{n} $.
\end{itemize}

Per dimostrarlo in generale, facciamo una digressione su concetti di analisi utili alla dimostrazione.

\subsection{Spazi vettoriali e algebre normati}

Uno spazio vettoriale $ V $ sui reali $ \R $ è detto \textit{normato} se esiste un'applicazione

\map{\norm{\cdot}}{V}{\R}{v}{\norm{v}}

chiamata \textit{norma} tale che siano soddisfatte le condizioni:

\begin{equation}
	\begin{cases}
		\begin{cases}
			\norm{v} \geqslant 0\\
			\norm{v} = 0 \iff v = 0 \in V
		\end{cases}%
		 & \text{definita positiva}\\
		\norm{\lambda v} = \abs{\lambda} \norm{v} & \text{omogeneità}\\
		\norm{v + w} \leqslant \norm{v} + \norm{w} & \text{subaddittività}
	\end{cases}
\end{equation}

per $ \forall \lambda \in \R $ e $ \forall v,w \in V $.\\
Consideriamo in particolare lo spazio vettoriale delle matrici quadrate sui reali $ M_{n}(\R) $ di dimensione $ n^{2} $ e come norma

\map{\norm{}}%
	{M_{n}(\R)}%
	{\R}%
	{X = [x_{ij}]_{i,j=1,\dots,n}}%
	{\left( \sum_{i,j=1}^{n} x_{ij}^{2} \right)^{\sfrac{1}{2}}}
	
la quale corrisponde alla norma usuale in $ \R^{n^{2}} $: da ciò deriva che questa norma soddisfa le condizioni poste sopra\footnote{%
	In particolare, la subadditività deriva dalla diseguaglianza di Cauchy-Schwarz (\textit{C-S}):
	
	\begin{equation}
		\norm{v \cdot w} \leqslant \norm{v} \norm{w}, \qquad v,w \in \R^{n},
	\end{equation}%
}, rendendo quindi $ M_{n}(\R) $ uno spazio vettoriale normato.\\\\
%
Una tripletta $ (V,\cdot,\norm{}) $ è un'\textit{algebra normata} se $ V $ è uno spazio vettoriale su $ \R $, $ (V,\cdot) $ è un'algebra su $ \R $ e $ (V,\norm{\cdot}) $ è uno spazio vettoriale normato tali che valga la proprietà di submoltiplicatività

\begin{equation}
	\norm{v \cdot w} \leqslant \norm{v} \norm{w}, \qquad v,w \in V,
\end{equation}

che lega l'operazione dell'algebra $ \cdot $ e la norma $ \norm{} $.\\
La tripletta $ (M_{n}(\R),\cdot,\norm{}) $ con $ \cdot $ la moltiplicazione tra matrici e $ \norm{} $ la norma sopra definita per $ M_{n}(\R) $ è un'algebra normata: per verificarlo dobbiamo dimostrare che il prodotto tra matrici e la norma soddisfino la proprietà di submoltiplicatività (in quanto le altre condizioni sono già soddisfatte).\\
Siano le matrici quadrate $ X = [x_{ij}] $ e $ y = [y_{ij}] $, allora utilizzando la diseguaglianza di Cauchy-Schwarz

\begin{equation}
	(X Y)_{ij}^{2} = \left( \sum_{k=1}^{n} x_{ik} y_{kj} \right)^{2} \leqslant \left( \sum_{k=1}^{n} x_{ik}^{2} \right) \left( \sum_{k=1}^{n} y_{kj}^{2} \right), \qquad i,j=1,\dots,n
\end{equation}

se consideriamo dunque la somma al quadrato

\begin{align}
	\begin{split}
		\norm{X Y}^{2} &= \sum_{i,j=1}^{n} (X Y)_{ij}^{2}\\
		&= \sum_{i,j=1}^{n} \left( \sum_{k=1}^{n} x_{ik} y_{kj} \right)^{2}\\
		&\leqslant \sum_{i,j=1}^{n} \left( \sum_{k=1}^{n} x_{ik}^{2} \right) \left( \sum_{k=1}^{n} y_{kj}^{2} \right)\\
		&= \left( \sum_{i,k=1}^{n} x_{ik}^{2} \right) \left( \sum_{j,k=1}^{n} y_{kj}^{2} \right)\\
		&= \norm{X}^{2} \norm{Y}^{2}
	\end{split}
\end{align}

da cui $ \norm{X Y} \leqslant \norm{X} \norm{Y} $.

\begin{definition}
	Sia $ (V,\cdot,\norm{}) $ un'algebra normata, allora
	
	\begin{itemize}
		\item Se $ s_{n} \in V $ è una successione convergente, i.e $ s_{n} \to s \in V $, allora
		
		\begin{equation}
			s_{n} \to s \implies a s_{n} \to a s, \qquad \forall a \in V
		\end{equation}
	
		\item Se consideriamo la serie
		
		\begin{equation}
			\sum_{n=0}^{+\infty} s_{n} = s %
			\implies%
			\begin{cases}
				\displaystyle \sum_{n=0}^{+\infty} a s_{n} = a s\\\\
				\displaystyle \sum_{n=0}^{+\infty} s_{n} a = s a
			\end{cases}
			\qquad \forall a \in V
		\end{equation}
	
		dove $ a s $ e $ s a $ implicano la moltiplicazione dell'algebra.
	\end{itemize}
\end{definition}

Per definizione, la notazione $ s_{n} \to s $ implica l'equazione

\begin{equation}
	\lim_{n \to \infty} \norm{s_{n} - s} = 0
\end{equation}

\begin{proof}
	Per la prima proprietà
	
	\begin{equation}
		\begin{cases}
			\norm{a s_{n} - a s} = \norm{a (s_{n} - s)}\\
			\lim\limits_{n \to \infty} \norm{s_{n} - s} = 0
		\end{cases}
		\implies%
		\begin{cases}
			\norm{a s_{n} - a s} = \abs{a} \norm{s_{n} - s}\\
			s_{n} \to s
		\end{cases}
		\implies%
		a s_{n} \to a s
	\end{equation}

	Per la seconda: per definizione, una serie converge se la successione delle somme parziali converge allo stesso valore, i.e.
	
	\begin{equation}
		\sum_{n=0}^{+\infty} s_{n} = s %
		\iff%
		\begin{cases}
			\displaystyle \tilde{s}_{k} = \sum_{n=0}^{k} s_{n}\\
			\tilde{s}_{k} \to s
		\end{cases}
	\end{equation}

	dunque per la proprietà
	
	\begin{equation}
		\sum_{n=0}^{+\infty} a s_{n} = a s %
		\iff%
		\begin{cases}
			\displaystyle a \tilde{s}_{k} = \sum_{n=0}^{k} a s_{n}\\
			a \tilde{s}_{k} \to a s
		\end{cases}
	\end{equation}

	dove per la prima proprietà
	
	\begin{equation}
		\sum_{n=0}^{+\infty} s_{n} = s %
		\iff%
		\tilde{s}_{k} \to s%
		\implies%
		a \tilde{s}_{n} \to a \tilde{s} %
		\iff%
		\sum_{n=0}^{+\infty} a s_{n} = a s
	\end{equation}

	Il ragionamento è analogo per la moltiplicazione per $ a $ a destra.
\end{proof}

Una serie è \textit{assolutamente convergente} se è convergente la stessa serie considerando le norme degli addendi, i.e.

\begin{equation}
	\sum_{n=0}^{+\infty} s_{n} \text{ assolutamente convergente} \iff \sum_{n=0}^{+\infty} \norm{s_{n}} \text{ convergente}
\end{equation}

Una successione $ s_{n} $ è una \textit{successione di Cauchy} se

\begin{equation}
	s_{n} \text{ di Cauchy} \iff \forall \varepsilon > 0, \, \exists p,q,N_{\varepsilon} \in \N \mid \norm{s_{p} - s_{q}} \leqslant \epsilon, \quad \forall p,q \geqslant N_{\varepsilon}
\end{equation}

\begin{remark}
	Una successione convergente è di Cauchy\footnote{%
		Per dimostrarlo è sufficiente prendere una differenza tra gli addendi arbitrariamente piccola e sfruttare la subadditività.%
	}, ma non è necessariamente vero che una successione di Cauchy sia convergente.
\end{remark}

\subsection{Spazi vettoriali e algebre completi}

Uno spazio normato $ (V,\norm{}) $ è detto \textit{completo} se ogni sua successione di Cauchy è convergente. Uno spazio normato e completo si chiama \textit{spazio di Banach}.\\
Analogamente, un'algebra normata $ (V,\cdot,\norm{}) $ è \textit{completa} se $ (V,\norm{}) $ è completo. Un'algebra normata e completa si chiama \textit{algebra di Banach}.\\\\
%
Possiamo prendere come esempio di algebra di Banach $ (M_{n}(\R),\cdot,\norm{}) $, in quanto $ (M_{n}(\R),\norm{}) $ è uno spazio completo perché lo è $ \R^{n^{2}} $ e questo si identifica con lo spazio delle matrici quadrate $ M_{n}(\R) = \R^{n^{2}} $.

\begin{definition}
	Sia $ (V,\norm{}) $ uno spazio completo, se una serie è assolutamente convergente, allora è anche convergente (strettamente), i.e.
	
	\begin{equation}
		\sum_{n=0}^{+\infty} \norm{s_{n}} \text{ convergente} \implies \sum_{n=0}^{+\infty} s_{n} \text{ convergente}
	\end{equation}
\end{definition}

Come controesempio dell'implicazione inversa, la serie in $ \R $

\begin{equation}
	\sum_{n=0}^{+\infty} \dfrac{(-1)^{n}}{n}
\end{equation}

è convergente ma non assolutamente convergente, se si prende la norma in $ \R $, i.e. il valore assoluto.

\begin{proof}
	Consideriamo la successione di serie parziali $ \tilde{s}_{k} \in V $ e $ p,q \in \N $ con $ p > q $, per ipotesi la serie è assolutamente convergente perciò
	
	\begin{equation}
		\norm{\tilde{s}_{p} - \tilde{s}_{q}} = \norm{ \sum_{n=q+1}^{p} \tilde{s}_{n} } \leqslant \sum_{n=q+1}^{p} \norm{\tilde{s}_{n}} < \varepsilon, \qquad \forall p,q > N_{\varepsilon} \in \N
	\end{equation}

	dove nel secondo passaggio abbiamo utilizzato la subadditività, dunque $ \tilde{s}_{k} $ è di Cauchy. Essendo $ V $ completo, la successione $ \tilde{s}_{k} $ è convergente strettamente dunque anche la serie è convergente.
\end{proof}

\subsection{Definizione di esponenziale di matrice}

Mostriamo ora che la serie

\begin{equation}
	e^{X} = \sum_{i=0}^{+\infty} \dfrac{X^{i}}{i!}, \qquad X \in M_{n}(\R)
\end{equation}

sia convergente in $ M_{n}(\R) $.\\
Essendo lo spazio $ M_{n}(\R) $ completo, è sufficiente verificare la serie sia assolutamente convergente. Utilizzando la submoltiplicatività

\begin{equation}
	\sum_{i=0}^{+\infty} \norm{ \dfrac{X^{i}}{i!} } = %
	\sum_{i=0}^{+\infty} \dfrac{1}{i!} \norm{X^{i}} \leqslant %
	\sum_{i=0}^{+\infty} \dfrac{1}{i!} \norm{X}^{i} = %
	e^{\norm{X}}
\end{equation}

dove $ \norm{X} \in \R $, dunque la serie converge.

\subsection{Proprietà dell'esponenziale di matrice}

Valgono le seguenti proprietà:

\begin{itemize}
	\item Se $ A B = B A $ i.e. $ [A,B] = 0 $, allora $ e^{A + B} = e^{A} e^{B} = e^{B} e^{A} $ per $ \forall A,B \in M_{n}(\R) $;
	
	\item L'esponenziale di matrice è una matrice invertibile, i.e. $ e^{A} \in GL_{n}(\R) $ per $ \forall A \in M_{n}(\R) $;
	
	\item %
	\begin{equation}
		\dfrac{\operatorname{d}}{\operatorname{dt}} e^{t X} = X e^{t X} = e^{t X} X, \qquad \forall X\in M_{n}(\R), \, \forall t \in \R
	\end{equation}
\end{itemize}

\begin{proof}
	\begin{itemize}
		\item Considerando che $ [A,B] = 0 $ possiamo utilizzare la formula binomiale, dunque
		
		\begin{align}
			\begin{split}
				e^{A + B} &= \sum_{k=0}^{+\infty} \dfrac{1}{k!} (A+B)^{k}\\
				&= \sum_{k=0}^{+\infty} \dfrac{1}{k!} \left( \sum_{j=0}^{k} \binom{k}{j} A^{k-j} B^{j} \right)\\
				&= \sum_{k=0}^{+\infty} \dfrac{1}{k!} \left( \sum_{j=0}^{k} \dfrac{k!}{j! (k-j)!} A^{k-j} B^{j} \right)\\
				&= \sum_{k=0}^{+\infty} \sum_{j=0}^{k} \left( \dfrac{A^{k-j}}{(k-j)!} \right) \left( \dfrac{B^{j}}{j!} \right)\\
				&= \left( \sum_{k=0}^{+\infty} \dfrac{A^{k}}{k!} \right) \left( \sum_{j=0}^{+\infty} \dfrac{B^{j}}{j!} \right)\\
				&= e^{A} e^{B}
			\end{split}
		\end{align}
		
		\item La matrice $ e^{A} $ è invertibile con inversa, dalla prima proprietà, $ e^{-A} $ in quanto
		
		\begin{equation}
			e^{A} e^{-A} = e^{-A} e^{A} = e^{A-A} = e^{0_{n}} = I
		\end{equation}
	
		\item %
		\begin{align}
			\begin{split}
				\dfrac{\operatorname{d}}{\operatorname{dt}} e^{t X} &= \dfrac{\operatorname{d}}{\operatorname{dt}} \left( \sum_{j=0}^{+\infty} \dfrac{(t X)^{j}}{j!} \right)\\
				&= \sum_{j=0}^{+\infty} \dfrac{\operatorname{d}}{\operatorname{dt}} \left( \dfrac{(t X)^{j}}{j!} \right)\\
				&= \sum_{j=0}^{+\infty} \left( \dfrac{j X (t X)^{j-1}}{j!} \right)\\
				&= X \sum_{j=0}^{+\infty} \left( \dfrac{j (t X)^{j-1}}{j (j-1)!} \right)\\
				&= X \sum_{j=0}^{+\infty} \dfrac{(t X)^{j-1}}{(j-1)!}\\
				&= X \sum_{k=0}^{+\infty} \dfrac{(t X)^{k}}{k!}\\
				&= X e^{t X} = e^{t X} X
			\end{split}
		\end{align}
	
		dove nel quarto passaggio abbiamo usato il fatto che
		
		\begin{equation}
			s_{n} \to s \implies a s_{n} \to a s
		\end{equation}
	\end{itemize}
\end{proof}

\begin{remark}
	Se consideriamo il campo dei numeri complessi $ \C $ al posto di $ \R $ per gli spazi vettoriali e dunque una matrice con entrate complesse ha come norma
	
	\map{\norm{}}%
		{M_{n}(\C)}%
		{\R}%
		{X = [x_{ij}]_{i,j=1,\dots,n}}%
		{\left( \sum_{i,j=1}^{n} \abs{x_{ij}}^{2} \right)^{\sfrac{1}{2}}}
		
	tutti i ragionamenti fatti in questa sezione sono validi, e.g. $ e^{X} $ è ancora una matrice invertibile anche se $ X \in M_{n}(\C) $, i.e. $ e^{X} \in GL_{n}(\C) $.
\end{remark}

\section{Richiami di algebra lineare}

\subsection{Prodotti scalari ed hermitiani}

Nell'algebra $ (\R,\cdot) $, presi due vettori $ v,w \in \R^{n} $ il loro prodotto scalare $ v \cdot w \in \R $ è definito come

\begin{equation}
	v \cdot w = (v^{1},\dots,v^{n}) \cdot (w^{1},\dots,w^{n}) \doteq \sum_{j=1}^{n} v^{j} w^{j}
\end{equation}

Considerando tutti i vettori come matrici con $ n $ righe e una colonna, possiamo pensare al prodotto scalare come il prodotto di un vettore riga per un vettore colonna

\begin{equation}
	v \cdot w = v^{T} w = %
	\begin{pmatrix}
		v^{1} & \cdots & v^{n}
	\end{pmatrix}%
	\begin{pmatrix}
		w^{1} \\ \vdots \\ w^{n}
	\end{pmatrix}
\end{equation}

Per vettori in campo complesso, i.e. nell'algebra $ (\C,\cdot) $, definiamo il \textit{prodotto hermitiano} $ v \cdot w \in \C $ come

\begin{equation}
	v \cdot w \doteq \sum_{j=1}^{n} v^{j} \bar{w}^{j} = v^{T} \bar{w}
\end{equation}

con $ v,w \in \C^{n} $ e $ \bar{w} $ indica il coniugato di $ w $ (sia per la componente che per tutte le componenti dell'intero vettore colonna). Il prodotto hermitiano è definito positivo in quanto

\begin{equation}
	v \cdot v = \sum_{j=1}^{n} v^{j} \bar{v}^{j} = \sum_{j=1}^{n} \abs{v}^{j}
\end{equation}

Essendo $ v \cdot v \in \R $ ha senso dire che $ v \cdot v \geqslant 0 $ e che

\begin{equation}
	v \cdot v = 0 \iff v = 0 \in \C^{n}
\end{equation}

cioè il prodotto hermitiano è definito positivo.\\
Il prodotto hermitiano non è bilineare come il prodotto scalare, ma lineare per la prima entrata e sesquilineare per la seconda, i.e.

\begin{equation}
	\begin{cases}
		(\lambda v_{1} + \mu v_{2}) \cdot w = \lambda (v_{1} \cdot w) + \mu (v_{2} \cdot w)\\
		v \cdot (\lambda w_{1} + \mu w_{2}) = \bar{\lambda} (v \cdot w_{1}) + \bar{\mu} (v \cdot w_{2})
	\end{cases}%
	\qquad \forall \lambda,\mu \in \C, \, \forall v,w \in \C^{n}
\end{equation}

dunque non è nemmeno simmetrico ma vale l'uguaglianza

\begin{equation}
	v \cdot w = \overline{w \cdot v}
\end{equation}

\subsection{Matrici ortogonali e unitarie}

Utilizzando come entrate i numeri reali, le matrici ortogonali $ O(n) $ sono definite come

\begin{equation}
	O(n) = \{ A \in M_{n}(\R) \mid A^{T} A = I \}
\end{equation}

In campo complesso, si parla di \textit{matrici unitarie}, definite come

\begin{equation}
	U(n) = \{ A \in M_{n}(\C) \mid  = I \}
\end{equation}

dove $ A ^{-1} = A^{*} = \bar{A}^{T} $.\\
Analogamente come le matrici ortogonali hanno colonne ortogonali tra loro e di norma unitaria (entrambi rispetto al prodotto scalare), anche le matrici unitarie hanno colonne ortogonali tra loro e di norma unitaria (entrambi rispetto al prodotto hermitiano): svolgendo il prodotto matriciale nella definizione di $ U(n) $

\begin{equation}
	A^{*} A = %
	\begin{pmatrix}
		\bar{a}_{11} & \bar{a}_{21} & \cdots & \bar{a}_{n1} \\ %
		\bar{a}_{12} & \bar{a}_{22} & & \bar{a}_{n2}\\ %
		\vdots & & \ddots & \vdots \\ %
		\bar{a}_{1n} & \bar{a}_{2n} & \cdots & \bar{a}_{nn}
	\end{pmatrix}%
	\begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\ %
		a_{21} & a_{22} & & a_{2n}\\ %
		\vdots & & \ddots & \vdots \\ %
		a_{n1} & a_{n2} & \cdots & a_{nn}
	\end{pmatrix}
\end{equation}

se consideriamo solo la prima entrata della matrice prodotto, otteniamo

\begin{equation}
	\abs{a_{11}}^{2} + \abs{a_{21}}^{2} + \cdots + \abs{a_{n1}}^{2} = 1
\end{equation}

e così per il resto degli elementi nella diagonale, mentre tutti gli altri prodotti hermitiani tra il resto delle colonne è nullo, i.e. le colonne sono tra loro ortogonali.

\begin{definition}
	Le colonne di una matrice unitaria sono una base ortonormale per l'algebra $ (\C^{n},\cdot) $, i.e. prese due colonne $ v_{i} $ e $ v_{j} $ di una matrice unitaria, vale la seguente uguaglianza $ v_{i} \cdot v_{j} = \delta_{ij} $.
\end{definition}


\begin{definition}
	La dimensione di un sottospazio $ W \subset \C^{n} $ in $ \C^{n} $ è doppia rispetto alla dimensione che questo avrebbe sui numeri reali $ \R $, in quanto esiste l'identificazione $ \C^{n} = \R^{2n} $, i.e.

	\begin{equation}
		\dim_{\C^{n}}(W) = 2 \dim_{\R^{n}}(W)
	\end{equation}
\end{definition}

\subsection{Matrici simili}

Siano due matrici $ A,B \in M_{n}(\C) $, diremo che $ A $ è \textit{simile} a $ B $ se esiste una matrice invertibile $ S \in GL_{n}(\C) $ tale che valga la relazione

\begin{equation}
	B = S^{-1} A S
\end{equation}

Le stesse due matrici sono \textit{unitariamente simili} se la matrice $ S $ della relazione è unitaria.

\subsection{Altre proprietà}

\begin{definition}
	Se $ A $ è simile a $ B $ se e solo se esiste un'applicazione lineare $ T : \C^{n} \to \C^{n} $ tali che $ A $ e $ B $ rappresentano $ T $ rispetto a due basi di $ \C^{n} $.
\end{definition}

\begin{definition}
	$ A $ è unitariamente simile a $ B $ se e solo se esiste un'applicazione lineare $ T : \C^{n} \to \C^{n} $ tali che $ A $ e $ B $ rappresentano $ T $ rispetto a due basi ortonormali di $ \C^{n} $.
\end{definition}

Questi due risultati derivano dal fatto che, prese due basi $ \mathcal{A} $ e $ \mathcal{B}' $ per $ \C^{n} $ con la matrice $ S $ che rappresenta il cambio di base, i.e.

\begin{equation}
	\begin{cases}
		x' = S x\\
		[T(x)]_{\mathcal{B}} = S [T(x)]_{\mathcal{A}}
	\end{cases}
\end{equation}

abbiamo che l'applicazione lineare $ T $ applicata ad un vettore $ x $ ha due immagini a seconda della base in cui sono scritte

\begin{align}
	\begin{split}
		T : \C^{n} &\to \C^{n}\\
		x &\stackrel{\mathcal{A}}{\mapsto} [T(x)]_{\mathcal{A}} = A x\\
		x' &\stackrel{\mathcal{B}}{\mapsto} [T(x)]_{\mathcal{B}} = B x'
	\end{split}
\end{align}

le quali sono legate da

\begin{align}
	\begin{split}
		[T(x)]_{\mathcal{B}} &= B x'\\
		S [T(x)]_{\mathcal{A}} &= B S x\\
		[T(x)]_{\mathcal{A}} &= S^{-1} B S x\\
		[T(x)]_{\mathcal{A}} &= A x
	\end{split}
\end{align}

da cui

\begin{equation}
	A = S^{-1} B S
\end{equation}

il quale rende le due matrici simili.

\begin{definition}
	"$ A $ è (unitariamente) simile a $ B $" è una relazione di equivalenza in $ M_{n}(\C) $, i.e. $ A \sim B $.
\end{definition}

\begin{definition}
	Se $ A \sim B $ allora $ A $ e $ B $ hanno stessi traccia, rango, determinante e polinomio caratteristico.
\end{definition}

\begin{proof}
	\begin{itemize}
		\item Per quanto riguarda la traccia:

		\begin{equation}
			\tr(A) = \tr(S^{-1} B S) = \tr(B S^{-1} S) = \tr(B)
		\end{equation}

		\item Il rango di una matrice non cambia se la si moltiplica per una matrice invertibile

		\item Per quanto riguarda il determinante, per Binet:

		\begin{equation}
			\det(A) = \det(S^{-1} B S) = \det(S^{-1}) \det(B) \det(S) = \det(B) \det(S^{-1} S) = \det(B)
		\end{equation}

		\item Per quanto riguarda il polinomio caratteristico, per Binet:
		
		\begin{align}
			\begin{split}
				P_{\lambda}(A) &= \det(A - \lambda I)\\
				&= \det(S^{-1} B S - \lambda I)\\
				&= \det(S^{-1} B S - S^{-1} \lambda I S)\\
				&= \det(S^{-1}) \det(B - \lambda I) \det(S)\\
				&= \det(B - \lambda I)\\
				&= P_{\lambda}(B)
			\end{split}
		\end{align}
		
	\end{itemize}
\end{proof}

\subsection{Teorema di Schur e teorema spettrale}

\begin{theorem}(Teorema di Schur)
	Data una matrice $ A \in M_{n}(\C) $ esiste una matrice $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} A U = T = %
		\smqty(\dmat{\lambda_{1}, & & K \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}

	con $ T $ matrice triangolare (superiore), i.e. una matrice che ha tutte entrate nulle sotto la diagonale principale.\\
	In altre parole, $ A $ è unitariamente simile ad una matrice triangolare (superiore), il che rende $ \lambda_{1},\dots,\lambda_{n} $ gli autovalori di $ A $.
\end{theorem}

\begin{proof}
	La dimostrazione di questo teorema è fatta per induzione su $ n $.\\
	Per $ n=1 $, le matrici diventano numeri e il teorema è dimostrato in quanto sono già "diagonali".\\
	Supponiamo dunque che sia vero per $ n-1 $ e dimostriamo per $ n $: dimostrare che $ A $ è simile a una matrice triangolare superiore è equivalente a trovare una base ortonormale $ \{v_{1},\dots,v_{n}\} $ (rispetto al prodotto hermitiano, i.e. $ v_{i}^{T} \bar{v}_{j} = \delta_{ij} $ con $ i,j=1,\dots,n $) di $ \C^{n} $ tale che $ A v_{k} $ sia combinazione lineare dei vettori di base.\\
	Siano $ \lambda_{1} \in \C $ un autovalore per $ A $ e $ v_{1} \neq 0 $ il corrispondente autovettore, i.e. $ A v_{1} = \lambda_{1} v_{1} $. Sia lo spazio dei vettori generati da $ v_{1} $
	
	\begin{equation}
		W = \expval{v_{1}}_{\C} = \{ \lambda v_{1} \mid \lambda \in \C \}
	\end{equation}
	
	il quale avrà $ \dim_{\C}(W) = 1 $, consideriamo ora il suo \textit{complemento ortogonale}
	
	\begin{equation}
		W^{\perp} = \{ v\in \C^{n} \mid v \cdot w = 0, \, \forall w \in W \}
	\end{equation}

	con $ \dim_{\C}(W^{\perp}) = n-1 $ e la proiezione $ \pi_{W^{\perp}} : \C^{n} \to W^{\perp} $. Consideriamo l'applicazione lineare
	
	\map{\pi_{W^{\perp}} \circ A}{W^{\perp}}{W^{\perp}}{w}{\pi_{W^{\perp}}(A w)}
	
	Per ipotesi induttiva, esiste una base ortonormale $ \{v_{2},\dots,v_{n}\} $ di $ W^{\perp} $ tale che $ (\pi_{W^{\perp}} \circ A)(v_{k}) $ è combinazione lineare di $ v_{2},\dots,v_{k} $ per $ \forall k=2,\dots,n $.\\
	A questo punto $ \{v_{1},\dots,v_{n}\} $ è una base ortonormale di $ \C^{n} $ tale che $ A v_{k} $ sia combinazione lineare dei vettori di base.\\
	Essendo la base ortonormale, la matrice $ A $ è dunque unitariamente simile a una matrice triangolare (superiore).
\end{proof}

La base $ \{v_{1},\dots,v_{n}\} $ che appare nel teorema di Schur viene chiamata \textit{base di Schur}.\\\\
%
Una matrice $ A \in M_{n}(\C) $ è detta \textit{normale} se $ A A^{*} = A^{*} A $, i.e. commuta con la sua trasposta coniugata: esempi di matrici normali sono

\begin{itemize}
	\item Le matrici unitarie, in quanto $ A^{*} A = I $;
	
	\item Le matrici ortogonali (le quali hanno entrate reali), perciò $ A^{*} A = A^{T} A = I $;
	
	\item Le matrici hermitiane, che coincidono con la loro trasposta coniugata, i.e. $ A = A^{*} $;
	
	\item Le matrici simmetriche $ A \in S(n) $ (con entrate reali), in quanto $ A = A^{T} $.
\end{itemize}

\begin{theorem}(Teorema spettrale)
	Ogni matrice $ A \in M_{n}(\C) $ normale è unitariamente simile a una matrice diagonale, i.e.
	
	\begin{equation}
		A \in M_{n}(\C) \qq{normale} \implies %
		\exists U \in U(n) \mid U^{*} A U = %
		\smqty(\dmat{\lambda_{1}, & & 0 \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}

	con $ \lambda_{j} $ gli autovalori di $ A $.\\
	In altre parole, una matrice normale è sempre diagonalizzabile e i suoi autovettori sono una base ortonormale di $ \C^{n} $.\\
	Ancora, una matrice $ A \in M_{n}(\C) $ è normale se e solo se esiste una base ortonormale di $ \C^{n} $ costituita dagli autovettori di $ A $.
\end{theorem}

\begin{proof}
	Se $ A \in M_{n}(\C) $, per il teorema di Schur esiste una matrice unitaria $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} A U = T = %
		\smqty(\dmat{\lambda_{1}, & & K \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}

	cioè $ A $ sia simile ad una matrice triangolare superiore.\\
	Essendo $ A $ normale, questo implica che anche $ U^{*} A U $ lo sia perché
	
	\begin{align}
		\begin{split}
			(U^{*} A U) (U^{*} A U)^{*} &= U^{*} A U U^{*} A^{*} U\\
			&= U^{*} A A^{*} U\\
			&= U^{*} A^{*} A U\\
			&= U^{*} A^{*} U U^{*} A U\\
			&= (U^{*} A U)^{*} (U^{*} A U)
		\end{split}
	\end{align}

	A questo punto, $ T $ è normale ma se una matrice è triangolare superiore e normale allora è diagonale: dimostriamo questo per induzione su $ n $.\\
	Per $ n=1 $, la matrice è diagonale in quanto numero.\\
	Supponiamo che sia vero per matrici di ordine $ n-1 $ e mostriamolo per matricidi ordine $ n $: riscriviamo la matrice normale triangolare superiore come
	
	\begin {equation}
		T = \mqty(\lambda_{1} & B \\ 0 & C)
	\end{equation}

	dove $ \lambda_{1} \in \C $, $ B \in M_{1,n-1}(\C) $, $ 0 \in M_{n-1,1} $ e $ C \in M_{n-1}(\C) $. Imponendo la condizione di matrice normale
	
	\begin{align}
		\begin{split}
			T T^{*} &= T^{*} T\\
			\mqty(\lambda_{1} & B \\ 0 & C) \mqty(\bar{\lambda}_{1} & 0 \\ B^{*} & C^{*}) &= \mqty(\bar{\lambda}_{1} & 0 \\ B^{*} & C^{*}) \mqty(\lambda_{1} & B \\ 0 & C)\\
			\mqty(\abs{\lambda}_{1} + B B^{*} & B C^{*} \\ C B^{*} & C C^{*}) &= \mqty(\abs{\lambda}_{1} & \bar{\lambda}_{1} B \\ \lambda_{1} B^{*} & B^{*} B + C^{*} C)
		\end{split}
	\end{align}

	otteniamo che
	
	\begin{equation}
		\begin{cases}
			B B^{*} = 0 \implies B = 0\\
			C^{*} C = C C^{*}
		\end{cases}
	\end{equation}

	rendendo $ C $ normale oltre che triangolare superiore: per ipotesi induttiva, $ C $ è diagonale quindi lo è anche $ T $.
\end{proof}

\begin{corollary}[1. Teorema spettrale per matrici simmetriche]
	Sia $ A \in S(n) $ con entrate reali ($ A \in M_{n}(\R) $), allora esiste una matrice $ P \in O(n) $ tale che
	
	\begin{equation}
		P^{T} A P = D = %
		\smqty(\dmat{\lambda_{1}, & & 0 \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}
\end{corollary}

\begin{proof}
	Per dimostrare questo corollario, è sufficiente dimostrare che gli autovalori di $ A $ siano reali e dunque anche gli autovettori sono reali: considerando l'uguaglianza
	
	\begin{equation}
		U(n) \cup M_{n}(\R) = O(n)
	\end{equation}

	il resto della dimostrazione deriva dal teorema spettrale.\\
	Per dimostrare che gli autovalori di una matrice $ A \in S(n) \subset M_{n}(\C) $  ad entrate reali siano reali, consideriamo la relazione
	
	\begin{equation}
		A v = \lambda v \qcomma \lambda \in \C, \, v \in \C^{n}
	\end{equation}

	da cui

	\begin{align}
		\begin{split}
			\lambda v &= A v\\
			\bar{v}^{T} \lambda v &= \bar{v}^{T} A v\\
			\lambda \abs{v}^{2} &= \bar{v}^{T} \bar{A}^{T} v\\
			&= \overline{A v}^{T} v\\
			&= \overline{\lambda v}^{T} v\\
			&= \bar{\lambda} v^{T} v\\
			&= \bar{\lambda} \abs{v}^{2}
		\end{split}
	\end{align}

	perciò $ \bar{\lambda} = \lambda \implies \lambda \in \R $.
\end{proof}

\begin{corollary}[2]
	Siano una matrice normale $ A \in M_{n}(\C) $ e un suo autovalore $ \lambda $, allora la molteplicità algebrica di $ \lambda $ coincide con quella geometrica, dove la prima indica il grado della soluzione $ \lambda $ all'interno del polinomio caratteristico mentre la seconda indica la dimensione del autospazio associato a $ \lambda $.
\end{corollary}

\begin{proof}
	Per dimostrare il corollario, ricordiamo che una matrice normale è sempre diagonalizzabile e una matrice è diagonalizzabile se e solo se la molteplicità algebrica dei suoi autovalori coincide con quella geometrica.
\end{proof}

\begin{corollary}[3]
	Sia $ A \in U(n) $, i.e. $ A^{*} A = I $, allora esiste una matrice unitaria $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} A U = %
		\smqty(\dmat{ e^{i \theta_{n}} & & 0 \\ & \ddots & \\ 0 & & e^{i \theta_{n}} }) \qcomma \theta_{j} \in \R, \, j=1,\dots,n
	\end{equation}

	In altre parole, una matrice unitaria è simile a una matrice diagonale con entrate di norma unitaria.
\end{corollary}

\begin{proof}
	Se $ A \in U(n) $ allora è normale, dunque esiste $ U \in U(n) $ tale che $ U^{*} A U $ sia diagonale, i.e.
	
	\begin{equation}
		U^{T} A U = %
		\smqty(\dmat{\lambda_{1} & & 0 \\ & \ddots & \\ 0 & & \lambda_{n}})
	\end{equation}

	quindi è sufficiente dimostrare che $ \abs{\lambda_{j}} = 1 $ per $ \forall j=1,\dots,n $:
	
	\begin{align}
		\begin{split}
			\lambda v &= A v\\
			\overline{(A v)}^{T} \lambda v &= \overline{(A v)}^{T} A v\\
			\overline{(\lambda v)}^{T} \lambda v &= \bar{v}^{T} A^{*} A v\\
			\bar{\lambda} \lambda \bar{v}^{T} v &= \bar{v}^{T} v\\
			\abs{\lambda}^{2} \abs{v}^{2} &= \abs{v}^{2}\\
			\abs{\lambda} &= 1
		\end{split}
	\end{align}
\end{proof}

Consideriamo l'insieme delle \textit{matrici unitarie speciali}:

\begin{equation}
	SU(n) = \{ X \in U(n) \mid \det(X) = 1 \} = \{ X \in M_{n}(\C) \mid X^{*} X = I \wedge \det(X) = 1 \}
\end{equation}

\begin{corollary}[4]
	Sia $ A \in SU(n) $, allora esiste una matrice unitaria $ U $ tale che
	
	\begin{equation}
		U^{*} A U = %
		\smqty(\dmat{ e^{i \theta_{n}} & & 0 \\ & \ddots & \\ 0 & & e^{i \theta_{n}} }) \qcomma \theta_{j} \in \R, \, j=1,\dots,n
	\end{equation}

	con
	
	\begin{equation}
		\sum_{j=1}^{n} \theta_{j} = 2 k \pi \qcomma k \in \Z
	\end{equation}
\end{corollary}

\begin{proof}
	\begin{align}
		\begin{split}
			1 &= \det(A)\\
			&= \prod_{j=1}^{n} e^{i \theta_{j}}\\
			&= e^{i \sum_{j=1}^{n} \theta_{j}}
		\end{split}
	\end{align}

	\begin{equation}
		1 = \det(A) = \det(U^{*} A U) = \prod_{j=1}^{n} e^{i \theta_{j}} = e^{i \sum_{j=1}^{n} \theta_{j}} %
		\implies%
		\sum_{j=1}^{n} \theta_{j} = 2 k \pi \qcomma k \in \Z
	\end{equation}
\end{proof}

\subsection{Forma canonica}

\begin{theorem}[Forma canonica ortogonale]
	Sia una matrice $ A \in O(n) $, i.e. $ A^{T} A = I $, allora esistono una matrice ortogonale $ P $, $ p,q \in \N $ e $ \theta_{j} \in (0,\pi) $ con $ j=1,\dots,k $ dove $ k = (n-p-q)/2 $ tali che
	
	\begin{equation}
		P^{T} A P = %
		\smqty(\dmat{ I_{p} & & & & & \\ & - I_{q} & & & 0 & \\ & & R_{1} & & & \\ & 0 & & \ddots & \\ & & & & R_{n-p-q} })
	\end{equation}

	sia una matrice a blocchi dove
	
	\begin{equation}
		R_{j} = \mqty(\dmat{ \cos(\theta_{j}) & \sin(\theta_{j}) \\\\ - \sin(\theta_{j}) & \cos(\theta_{j}) })
	\end{equation}

	in cui $ \det(R_{j}) = 1 $ e
	
	\begin{equation}
		\det(P^{T} A P) = %
		\begin{cases}
			1 & q = 2 k\\
			- 1 & q = 2 k + 1
		\end{cases}
	\end{equation}

	con $ k \in \N $.\\
	Se $ A \in SO(n) $ allora $ q $ è pari.
\end{theorem}

\subsection{Matrici elementari e generatori del gruppo lineare speciale}

Siano $ a \in \R $, $ i,j=1,\dots,n $ con $ i \neq j $, una \textit{matrice elementare} $ M_{a}(i,j) \in M_{n}(\R) $ è definita nel seguente modo:

\begin{equation}
	M_{a}(i,j) \doteq I + a E_{ij}
\end{equation}

dove

\begin{equation}
	[E_{ij}]_{kl} = %
	\begin{cases}
		1 & k = i \wedge l = j\\
		0 & \text{altrimenti}
	\end{cases}
\end{equation}

in altre parole, una matrice elementare è una matrice che ha 1 nella diagonale principale, $ a $ nell'entrata identificata dalla riga $ i $ e dalla colonna $ j $ e 0 nelle altre entrate.\\
Il determinante di una matrice elementare è sempre unitario, i.e.

\begin{equation}
	\det(M_{a}(i,j)) = 1
\end{equation}

in quanto è sempre una matrice triangolare superiore o inferiore, dunque il determinante è dato dal prodotto degli elementi sulla diagonale. L'inversa di una matrice elementare è data da

\begin{equation}
	(M_{a}(i,j))^{-1} = M_{-a}(i,j)
\end{equation}

ed è ancora una matrice elementare, in quanto

\begin{align}
	\begin{split}
		M_{a}(i,j) M_{-a}(i,j) &= (I + a E_{ij}) (I - a E_{ij})\\
		&= I + a E_{ij} - a E_{ij} - a^{2} \cancel{E_{ij} E_{ij}}\\
		&= I
	\end{split}
\end{align}

\begin{theorem}
	Siano $ \K $ un campo e il gruppo lineare speciale
	
	\begin{equation}
		SL_{n}(\K) = \{ A \in M_{n}(\K) \mid \det(A) = 1 \} \qcomma n \geqslant 1
	\end{equation}

	allora $ SL_{n}(\K) $ è generato da matrici elementari, i.e.
	
	\begin{equation}
		\forall A \in SL_{n}(\K) \qcomma \exists M_{a_{1}}(i_{1},j_{1}), \dots, M_{a_{t}}(i_{t},j_{t}) \mid A = \prod_{p=1}^{t} M_{a_{p}}(i_{p},j_{p})
	\end{equation}

	cioè ogni matrice del gruppo lineare speciale può essere scritta come prodotto finito di matrici elementari.
\end{theorem}

\begin{proof}
	Se $ A \in M_{n}(\K) $ e $ M_{b}(i,j) $ è una matrice elementare, allora il prodotto $ M_{b}(i,j) A $ corrisponde alla seguente operazione elementare sulle righe $ R_{i} $ di $ A $:
	
	\begin{equation}
		R_{i} \to R_{i} + b R_{j}
	\end{equation}

	con
	
	\begin{equation}
		R_{i} = \mqty(\dmat{ a_{i1} & \cdots & a_{in} })
	\end{equation}

	in quanto
	
	\begin{align}
		\begin{split}
			M_{b}(i,j) A &= (I + b E_{ij}) A\\
			&= A + b E_{ij} A\\
			&= \smqty(\dmat{ a_{11} & \cdots & a_{1n} \\ %
							\vdots & \ddots & \vdots \\ %
							a_{n1} & \cdots & a_{nn} }) + %
				b \smqty(\dmat{ 0 & \cdots & \cdots & & \cdots & & \cdots & 0 \\ %
								\vdots & \ddots & & & & & & \vdots \\ %
								& & \ddots & & & & & \\ %
								0 & \cdots & \cdots & 0 & \cdots & 1 & \cdots & 0 \\ %
								\vdots & & & & \ddots & & & \vdots \\ %
								& & & & & \ddots & & \\ %
								\vdots & & & & & & \ddots & \vdots \\ %
								0 & \cdots & \cdots & & \cdots & & \cdots & 0 }) %
				\smqty(\dmat{ a_{11} & \cdots & a_{1n} \\ %
							\vdots & \ddots & \vdots \\ %
							a_{n1} & \cdots & a_{nn} })\\
			%
			&= \smqty(\dmat{ R_{1} \\ %
							\vdots \\ %
							R_{i-1} \\ %
							R_{i} \\ %
							R_{i+1} \\ %
							\vdots \\ %
							\vdots \\ %
							R_{n} }) + %
				\smqty(\dmat{ 0 & \cdots & \cdots & & \cdots & & \cdots & 0 \\ %
							\vdots & \ddots & & & & & & \vdots \\ %
							& & \ddots & & & & & \\ %
							0 & \cdots & \cdots & 0 & \cdots & b & \cdots & 0 \\ %
							\vdots & & & & \ddots & & & \vdots \\ %
							& & & & & \ddots & & \\ %
							\vdots & & & & & & \ddots & \vdots \\ %
							0 & \cdots & \cdots & & \cdots & & \cdots & 0 }) %
				\smqty(\dmat{ R_{1} \\ %
							\vdots \\ %
							R_{i-1} \\ %
							R_{i} \\ %
							R_{i+1} \\ %
							\vdots \\ %
							\vdots \\ %
							R_{n} })\\
			%
			&= \smqty(\dmat{ R_{1} \\ %
							\vdots \\ %
							R_{i-1} \\ %
							R_{i} \\ %
							R_{i+1} \\ %
							\vdots \\ %
							\vdots \\ %
							R_{n} }) + %
				\smqty(\dmat{ 0 \\ %
							\vdots \\ %
							0 \\ %
							b R_{j} \\ %
							0 \\ %
							\vdots \\ %
							\vdots \\ %
							0 })\\
			%
			&= \smqty(\dmat{ R_{1} \\ %
							\vdots \\ %
							R_{i-1} \\ %
							R_{i} + b R_{j} \\ %
							R_{i+1} \\ %
							\vdots \\ %
							\vdots \\ %
							R_{n} })
		\end{split}
	\end{align}

	Analogamente, il prodotto $ A M_{b}(i,j) $ corrisponde alla seguente operazione elementare sulle colonne $ C_{i} $ di $ A $:
	
	\begin{equation}
		C_{j} \to C_{j} + b C_{i}
	\end{equation}

	con
	
	\begin{equation}
		C_{i} = \mqty(\dmat{ a_{1i} \\ \vdots \\ a_{ni} })
	\end{equation}
	
	in quanto
	
	\begin{align}
		\begin{split}
			A M_{b}(i,j) &= A (I + b E_{ij})\\
			&= A + b A E_{ij}\\
			&= \smqty(\dmat{ a_{11} & \cdots & a_{1n} \\ %
							\vdots & \ddots & \vdots \\ %
							a_{n1} & \cdots & a_{nn} }) + %
			b \smqty(\dmat{ a_{11} & \cdots & a_{1n} \\ %
							\vdots & \ddots & \vdots \\ %
							a_{n1} & \cdots & a_{nn} }) %
			\smqty(\dmat{ 0 & \cdots & \cdots & & \cdots & & \cdots & 0 \\ %
							\vdots & \ddots & & & & & & \vdots \\ %
							& & \ddots & & & & & \\ %
							0 & \cdots & \cdots & 0 & \cdots & 1 & \cdots & 0 \\ %
							\vdots & & & & \ddots & & & \vdots \\ %
							& & & & & \ddots & & \\ %
							\vdots & & & & & & \ddots & \vdots \\ %
							0 & \cdots & \cdots & & \cdots & & \cdots & 0 })\\
			%
			&= \smqty(\dmat{ C_{1} & \cdots & \cdots & C_{j-1} & C_{j} & C_{j+1} & \cdots & C_{n} }) + %
				\smqty(\dmat{ 0 & \cdots & \cdots & & \cdots & & \cdots & 0 \\ %
								\vdots & \ddots & & & & & & \vdots \\ %
								& & \ddots & & & & & \\ %
								0 & \cdots & \cdots & 0 & \cdots & b & \cdots & 0 \\ %
								\vdots & & & & \ddots & & & \vdots \\ %
								& & & & & \ddots & & \\ %
								\vdots & & & & & & \ddots & \vdots \\ %
								0 & \cdots & \cdots & & \cdots & & \cdots & 0 }) %
				\smqty(\dmat{ C_{1} & \cdots & \cdots & C_{j-1} & C_{j} & C_{j+1} & \cdots & C_{n} })\\
			%
			&= \smqty(\dmat{ C_{1} & \cdots & \cdots & C_{j-1} & C_{j} & C_{j+1} & \cdots & C_{n} }) + %
				\smqty(\dmat{ 0 & \cdots & \cdots & 0 & b C_{i} & 0 & \cdots & 0 })\\
			%
			&= \smqty(\dmat{ C_{1} & \cdots & \cdots & C_{j-1} & C_{j} + b C_{i} & C_{j+1} & \cdots & C_{n} })
		\end{split}
	\end{align}

	Per dimostrare il teorema è dunque sufficiente mostrare che data $ A \in SL_{n}(\K) $ esistono un numero finito di operazioni elementari sulle righe e sulle colonne di $ A $ tali che trasformino $ A $ nella matrice identità: per fare ciò, usiamo l'algoritmo di Gauss-Jordan:
	
	\begin{enumerate}
		\item Sia $ A \in SL_{n}(\K) $ e consideriamo il suo elemento $ a_{12} $: se questo è diverso da zero, saltiamo al passo successivo; nel caso in cui non lo sia, esisterà un elemento della stessa riga non nullo (in quanto la matrice ha determinante diverso da zero), i.e. $ a_{1j} \neq 0 $, e applichiamo la seguente operazione elementare
		
		\begin{equation}
			C_{2} \to C_{2} + C_{j}
		\end{equation}
	
		che rende $ a_{12} \neq 0 $
		
		\item Con l'operazione elementare
		
		\begin{equation}
			C_{1} \to C_{1} + \left( \dfrac{1-a_{11}}{a_{12}} \right) C_{2}
		\end{equation}
	
		otteniamo che $ a_{11} = 1 $, in quanto
		
		\begin{equation}
			a_{11} \to a_{11} + \left( \dfrac{1-a_{11}}{a_{12}} \right) a_{12} = a_{11} + 1 - a_{11} = 1
		\end{equation}
		
		\item Per $ \forall j \geqslant 2 $, applichiamo l'operazione elementare
		
		\begin{equation}
			C_{j} \to C_{j} - a_{1j} C_{1}
		\end{equation}
	
		rendendo la prima riga $ R_{1} $ della matrice $ A $ pari a 
		
		\begin{equation}
			R_{1} = \mqty(\dmat{ 1 & 0 & \cdots & 0 })
		\end{equation}
		
		\item Per $ \forall j \geqslant 2 $, applichiamo l'operazione elementare
		
		\begin{equation}
			R_{j} \to R_{j} - a_{j1} R_{1}
		\end{equation}
		
		rendendo la prima colonna $ C_{1} $ della matrice $ A $ pari a 
		
		\begin{equation}
			C_{1} = \mqty(\dmat{ 1 \\ 0 \\ \vdots \\ 0 })
		\end{equation}
	
		e dunque la matrice $ A $
		
		\begin{equation}
			A = %
			\mqty(\dmat{ 1 & 0 & \cdots & 0 \\ %
							0 & & & \\ %
							\vdots & & B &\\ %
							0 & & & })
		\end{equation}
	
		con $ B \in SL_{n-1}(\K) $, in quanto le operazioni elementari fatte non modificano il determinante della matrice $ A $ (e dunque della matrice $ B $)
		
		\item Per induzione, rifacendo gli stessi passaggi per $ B $, si ricava la matrice identità $ I $ mediante un numero finito di operazioni elementari sulle righe e sulle colonne; le operazioni elementari su $ B $ non intaccano le operazioni precedenti perché gli elementi fuori da $ B $ e non nella diagonale sono tutti nulli
	\end{enumerate}

	Una volta svolte tutte le operazioni elementari, scritte come moltiplicazioni a destra e a sinistra per matrici elementari, possiamo scrivere infine
	
	\begin{align}
		\begin{split}
			(M_{1} \cdots M_{q}) A (M_{q+1} \cdots M_{t}) &= I\\
			A &= (M_{1} \cdots M_{q})^{-1} (M_{q+1} \cdots M_{t})^{-1}\\
			&= (M_{q}^{-1} \cdots M_{1}^{-1}) (M_{t}^{-1} \cdots M_{q+1}^{-1})
		\end{split}
	\end{align}
\end{proof}

\subsection{Traccia, determinante ed esponenziale di una matrice}

Le seguenti considerazioni e dimostrazioni valgono indifferentemente se si considera come campo su cui sono definiti gli spazi quello dei numeri reali o quello dei numeri complessi, i.e. $ \K = \R,\C $. Per sintesi, useremo nella trattazione il campo dei reali $ \R $.

\begin{definition}
	Sia $ X \in M_{n}(\R) $, allora
	
	\begin{equation}
		\tr(X) = \sum_{j=1}^{n} \lambda_{j}
	\end{equation}
	
	dove i $ \lambda_{j} $ sono gli autovalori di $ X $.
\end{definition}

La traccia di una matrice ad entrare reali è un numero reale, i.e. $ \tr(X) \in \R $, nonostante ci possano essere autovalori complessi, perché quest'ultimi saranno complessi coniugati tra loro e, sommandoli, si elidono a vicenda.

\begin{proof}
	Per il teorema di Schur, $ X $ è simile a una matrice triangolare superiore, i.e. esiste $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} X U = T = %
		\smqty(\dmat{ \lambda_{1}, %
						& & A \\ %
						& \ddots & \\ %
						0 & & , %
						\lambda_{n} })
	\end{equation}
	
	dove i $ \lambda_{j} $ sono gli autovalori di $ X $. Possiamo ora scrivere
	
	\begin{align}
		\begin{split}
			\tr(X) &= \tr(U^{*} X U)\\
			&= \tr(T)\\
			&= \sum_{j=1}^{n} \lambda_{j}
		\end{split}
	\end{align}
\end{proof}

\begin{definition}
	Sia $ X \in M_{n}(\R) $, allora
	
	\begin{equation}
		\det(e^{X}) = e^{\tr(X)}
	\end{equation}
\end{definition}

\begin{proof}
	Supponiamo che $ X $ sia triangolare superiore, i.e.
	
	\begin{equation}
		X = %
		\smqty(\dmat{ \lambda_{1}, %
						& & A \\ %
						& \ddots & \\ %
						0 & & , %
						\lambda_{n} })
	\end{equation}

	la potenza $ k $-esima di questa matrice avrà la forma
	
	\begin{equation}
		X^{k} = %
		\smqty(\dmat{ \lambda_{1}^{k}, %
						& & A' \\ %
						& \ddots & \\ %
						0 & & , %
						\lambda_{n}^{k} })
	\end{equation}

	perciò l'esponenziale sarà
	
	\begin{align}
		\begin{split}
			e^{X} &= \sum_{k=0}^{+\infty} \dfrac{X^{k}}{k!}\\
			&= \sum_{k=0}^{+\infty} %
				\smqty(\dmat{ \lambda_{1}^{k}, %
								& & A' \\ %
								& \ddots & \\ %
								0 & & , %
								\lambda_{n}^{k} })\\
			 %
			&= \smqty(\dmat{ \displaystyle\sum_{k=0}^{+\infty} \dfrac{\lambda_{1}^{k}}{k!}, %
								& & A'' \\ %
								& \ddots & \\ %
								0 & & , %
								\displaystyle\sum_{k=0}^{+\infty} \dfrac{\lambda_{n}^{k}}{k!} })\\
			&= \smqty(\dmat{ e^{\lambda_{1}}, %
								& & A'' \\ %
								& \ddots & \\ %
								0 & & , %
								e^{\lambda_{n}} })
		\end{split}
	\end{align}

	A questo punto
	
	\begin{equation}
		\det(e^{X}) = \prod_{k=1}^{n} e^{\lambda_{k}} = e^{\sum_{k=1}^{n} \lambda_{k}} = e^{\tr(X)}
	\end{equation}

	Nel caso in cui la matrice $ X $ non fosse triangolare, per il teorema di Schur, $ X $ è simile a una matrice triangolare superiore, i.e. esiste $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} X U = T = %
		\smqty(\dmat{ \lambda_{1}, %
			& & A \\ %
			& \ddots & \\ %
			0 & & , %
			\lambda_{n} })
	\end{equation}
	
	Per calcolare il determinante di $ e^{X} $, utilizziamo Binet e la dimostrazione fatta sopra ottenendo
	
	\begin{align}
		\begin{split}
			\det(e^{X}) &= \det(U^{*} e^{X} U)\\
			&= \det(e^{U^{*} X U})\\
			&= e^{\tr(U^{*} X U)}\\
			&= e^{\tr(X)}
		\end{split}
	\end{align}
	
	nel secondo passaggio abbiamo utilizzato la proprietà $ U^{*} e^{X} U = e^{U^{*} X U} $, che si dimostra come
	
	\begin{align}
		\begin{split}
			e^{U^{*} X U} &= \sum_{k=0}^{+\infty} \dfrac{(U^{*} X U)^{k}}{k!}\\
			&= \sum_{k=0}^{+\infty} \dfrac{U^{*} X^{k} U}{k!}\\
			&= U^{*} \left( \sum_{k=0}^{+\infty} \dfrac{X^{k}}{k!} \right) U\\
			&= U^{*} e^{X} U
		\end{split}
	\end{align}

	dove $ (U^{*} X U)^{k} = U^{*} X^{k} U $ in quanto $ U^{*} U = I $ e, nel terzo passaggio, abbiamo usato le proprietà delle serie convergenti, i.e.
	
	\begin{equation}
		\sum_{n=0}^{+\infty} a s_{n} = a \sum_{n=0}^{+\infty} s_{n}
	\end{equation}
\end{proof}

\begin{corollary}
	Siccome la proposizione dà un metodo per calcolare il determinante di $ e^{X} $, ricaviamo dunque che l'esponenziale di una matrice è sempre invertibile\footnote{%
		Questo fatto è già stato dimostrato, questa è solo un'ulteriore conferma.%
	}, in quanto $ e^{\tr(X)} \neq 0 $ per $ \forall X \in M_{n}(\K) $.
\end{corollary}

\begin{corollary}
	L'esponenziale di una matrice sia sempre invertibile.\\
	Sia $ X \in M_{n}(\R) $, allora la curva
	
	\map{c}%
		{\R \times M_{n}(\R)}{GL_{n}(\R)}%
		{(t,X)}{e^{t X}}
	
	ha vettore tangente nell'origine esattamente $ X $, i.e. $ c'(0) = X $ dove valgono le seguenti identificazioni
	
	\begin{equation}
		\begin{cases}
			c(0) = I\\
			T_{I}(GL_{n}(\R)) = M_{n}(\R)\\
			c'(0) = \left. \dv{t} c \right|_{t=0}
		\end{cases}
	\end{equation}

	Più in generale, se $ A \in GL_{n}(\R) $ e $ B \in M_{n}(\R) $, allora per la curva
	
	\map{c}%
		{\R \times GL_{n}(\R) \times M_{n}(\R)}{GL_{n}(\R)}%
		{(t,A,B)}{A e^{t A^{-1} B}}
	
	valgono
	
	\begin{equation}
		\begin{cases}
			c(0) = A\\
			c'(0) = B
		\end{cases}
	\end{equation}
\end{corollary}

\begin{proof}
	Per il primo caso
	
	\begin{equation}
		c(0) = e^{0 X} = e^{0_{n}} = I
	\end{equation}

	e il vettore tangente
	
	\begin{equation}
		c'(0) = \left. \dv{t} e^{t X} \right|_{t=0} = \left. X e^{t X} \right|_{t=0} = X I = X
	\end{equation}

	più in generale
	
	\begin{equation}
		c(0) = A e^{0 A B} = A e^{0_{n}} = A
	\end{equation}

	e il vettore tangente
	
	\begin{equation}
		c'(0) = \left. \dv{t} A e^{t A^{-1} B} \right|_{t=0} = A \left. \dv{t} e^{t A^{-1} B} \right|_{t=0} = A \left. A^{-1} B e^{t A^{-1} B} \right|_{t=0} = B I = B
	\end{equation}
\end{proof}

\begin{corollary}
	Siano l'applicazione determinante
	
	\map{\det}%
		{GL_{n}(\R)}{\R}%
		{A}{\det(A)}
		
	e $ B \in T_{A}(GL_{n}(\R)) = M_{n}(\R) $\footnote{%
		Si ha anche che $ T_{A}(GL_{n}(\R)) = A \, T_{I}(GL_{n}(\R)) $, derivato dal differenziale della traslazione a sinistra nell'Esempio \ref{trasl-diff}.%
	}. Il differenziale del determinante $ \det_{*A} : T_{A}(GL_{n}(\R)) \to T_{\det(A)}(\R) $, tramite le identificazioni $  T_{A}(GL_{n}(\R)) = M_{n}(\R) $ e $ T_{\det(A)}(\R) = \R $, può essere scritto come

	\map{\operatorname{det}_{*A}}%
		{M_{n}(\R)}{\R}%
		{B}{\det(A) \tr(A^{-1} B)}
\end{corollary}

\begin{proof}
	Sia la curva
	
	\map{c}%
		{\R \times GL_{n}(\R) \times M_{n}(\R)}{GL_{n}(\R)}%
		{(t,A,B)}{A e^{t A^{-1} B}}
	
	con $ c(0) = A $ e $ c'(0) = B $, allora, usando le proprietà dell'esponenziale di matrice
	
	\begin{align}
		\begin{split}
			\operatorname{det}_{*A}(B) &= \left. \dv{t} \det(A e^{t A^{-1} B}) \right|_{t=0}\\
			&= \left. \dv{t} \det(A) \det(e^{t A^{-1} B}) \right|_{t=0}\\
			&= \det(A) \left. \dv{t} \det(e^{t A^{-1} B}) \right|_{t=0}\\
			&= \det(A) \left. \dv{t} e^{\tr(t A^{-1} B)} \right|_{t=0}\\
			&= \det(A) \left. \dv{t} e^{t \tr(A^{-1} B)} \right|_{t=0}\\
			&= \det(A) \tr(A^{-1} B) \left. e^{t \tr(A^{-1} B)} \right|_{t=0}\\
			&= \det(A) \tr(A^{-1} B) I\\
			&= \det(A) \tr(A^{-1} B)
		\end{split}
	\end{align}
\end{proof}

Come asserito all'inizio della sezione, queste proprietà valgono anche se si considera il campo dei numeri complessi $ \C. $

\section{Esempi di sottogruppi di Lie e loro topologia}

Alcuni gruppi di Lie sono $ GL_{n}(\R) $, $ GL_{n}(\C) $, $ SL_{n}(\R) $, $ SL_{n}(\C) $, $ O(n) $, $ U(n) $ e $ SU(n) $.

\subsection{$ SL_{n}(\R) $ come sottogruppo di Lie di $ GL_{n}(\R) $}\label{SL-sublie}

Il gruppo di Lie $ SL_{n}(\R) \subset GL_{n}(\R) $ è un sottogruppo di Lie di $ GL_{n}(\R) $ di dimensione $ \dim(SL_{n}(\R)) = n^{2} - 1 $ con spazio tangente

\begin{equation}
	T_{I}(SL_{n}(\R)) = \{ X \in M_{n}(\R) \mid \tr(X) = 0 \}
\end{equation}

Inoltre, $ SL_{n}(\R) $ è connesso (e quindi connesso per archi in quanto varietà), chiuso in $ GL_{n}(\R) $ ma non compatto.\\\\
%
Per dimostrare che sia un sottogruppo di Lie senza usare la proprietà di sottovarietà, dovremmo dimostrare che $ SL_{n}(\R) $ sia un sottogruppo algebrico di $ GL_{n}(\R) $, che sia una sottovarietà immersa e che le sue operazioni siano lisce; usando invece il teorema della preimmagine, possiamo dimostrare che $ SL_{n}(\R) $ è una sottovarietà di $ GL_{n}(\R) $\footnote{%
	Vedi Esempio \ref{SL-subvar}.%
} e dunque, per il Teorema \ref{th-liesub-var}, che è un sottogruppo di Lie embedded di $ GL_{n}(\R) $.\\
Per (ri)dimostrare che $ SL_{n}(\R) $ sia una sottovarietà di $ GL_{n}(\R) $, consideriamo sempre la funzione determinante

\map{f = \det}%
	{GL_{n}(\R)}{\R}%
	{A}{\det(A)}
	
da cui $ SL_{n}(\R) = f^{-1}(1) $ (il quale rende $ SL_{n}(\R) $ chiuso in quanto immagine di un chiuso tramite una funzione continua). Perché $ SL_{n}(\R) $ sia effettivamente una sottovarietà di $ GL_{n}(\R) $ è necessario che $ 1 \in \mathcal{VR}_{f} $, i.e. il differenziale $ f_{*A} $ è suriettivo per $ \forall A \in SL_{n}(\R) = f^{-1}(1) $: preso il differenziale

\map{f_{*A} = \operatorname{det}_{*A}}%
	{T_{A}(GL_{n}(\R))}{T_{\det(A)}(\R) = \R}%
	{B}{\det(A) \tr(A^{-1} B) = \tr(A^{-1} B)}

dove $ \det(A) = 1 $ in quanto $ A \in SL_{n}(\R) $, deve esiste $ B \in T_{A}(GL_{n}(\R)) = M_{n}(\R) $ tale che $ \tr(A^{-1} B) = c $ e questo si verifica per $ B = c A / n $ con $ n = \dim(M_{n}(\R)) $

\begin{equation}
	\tr(A^{-1} B) = \tr(\dfrac{c A^{-1} A}{n}) = \dfrac{c \tr(I_{n})}{n} = \dfrac{c \, n}{n} = c
\end{equation}

dunque $ SL_{n}(\R) $ è una sottovarietà di $ GL_{n}(\R) $ con dimensione (usando la funzione determinante)

\begin{equation}
	\dim(SL_{n}(\R)) = \dim(GL_{n}(\R)) - \dim(\R) = n^{2} - 1
\end{equation}

Il teorema della preimmagine asserisce inoltre che

\begin{equation}
	T_{I}(F^{-1}(c)) = \ker(F_{*p})
\end{equation}

da cui lo spazio tangente ad $ SL_{n}(\R) $

\begin{equation}
	T_{I}(SL_{n}(\R)) = \ker(\operatorname{det}_{*I}) = \{ X \in M_{n}(\R) = T_{A}(GL_{n}(\R)) \mid \tr(X) = 0 \}
\end{equation}

il quale ha dimensione $ n^{2} - 1 $ (esattamente come $ SL_{n}(\R) $) in quanto le sue entrate sono vincolate dalla condizione di avere traccia nulla. Nel caso in cui lo spazio tangente fosse calcolato in un altro punto

\begin{equation}
	T_{A}(SL_{n}(\R)) = A \, T_{I}(SL_{n}(\R))
\end{equation}

Per quanto riguarda la topologia di $ SL_{n}(\R) $, considerando il teorema di Heine-Borel\footnote{%
	Il teorema di Heine-Borel asserisce che un insieme è compatto se e solo se è chiuso e limitato nel suo spazio ambiente.%
} con spazio ambiente lo spazio euclideo $ \R^{n^{2}} $, in quanto $ SL_{n}(\R) \subset GL_{n}(\R) \subset M_{n}(\R) = \R^{n^{2}} $, $ SL_{n}(\R) $ non è compatto (per $ n \geqslant 2 $) in $ GL_{n}(\R) $ in quanto chiuso ma non limitato: prendendo l'insieme di matrici

\begin{equation}
	\left\{ %
		\smqty(\dmat{ k & 0 \\ 0 & \sfrac{1}{k}, I_{n-2} }) \qcomma k \in \N \setminus \{0\}
	\right\} %
	\subset SL_{n}(\R)
\end{equation}

questo è illimitato al variare di $ k $ in quanto non esiste una palla in $ GL_{n}(\R) $ che lo contenga.\\
Per uno spazio localmente euclideo ($ SL_{n}(\R) $ è una varietà quindi lo è), vale che questo è connesso se e solo se è connesso per archi\footnote{%
	Per uno spazio non localmente euclideo, la connessione per archi implica la connessione ma non viceversa.%
}. Per dimostrare dunque che $ SL_{n}(\R) $ sia connesso, facciamo vedere che una matrice qualunque $ A \in SL_{n}(\R) $ può essere unita tramite un arco, i.e. un'applicazione continua, all'identità: usando questo procedimento due volte, ogni matrice sarà unità ad ogni altra matrice di $ SL_{n}(\R) $ tramite archi. Le matrici appartenenti a $ SL_{n}(\R) $ possono essere scritte come prodotto finito di matrici elementari

\begin{equation}
	A = \prod_{k=1}^{s} M_{a_{k}}(i_{k},j_{k}) = \prod_{k=1}^{s} (I + a_{k} E_{i_{k} j_{k}})
\end{equation}

con $ E_{ij} $ la matrice che ha tutte entrate nulle tranne per quella nella riga $ i $-esima e colonna $ j $-esima dove è presente 1.\\
Sia la curva

\map{c}%
	{[0,1]}{SL_{n}(\R)}%
	{t}{\prod_{k=1}^{s} M_{t a_{k}}(i_{k},j_{k}) = \prod_{k=1}^{s} (I + t a_{k} E_{i_{k} j_{k}})}
	
un'applicazione continua con $ c(0) = I $ e $ c(1) = A $: questa curva funge da arco che collega la matrice identità ad $ A $, dimostrando dunque che $ SL_{n}(\R) $ è connesso.

\subsection{$ SL_{n}(\C) $ come sottogruppo di Lie di $ GL_{n}(\C) $}

Il gruppo di Lie $ SL_{n}(\C) \subset GL_{n}(\C) $ è un sottogruppo di Lie di $ GL_{n}(\C) $ di dimensione $ \dim_{\R}(SL_{n}(\C)) = 2 n^{2} - 2 $ con spazio tangente

\begin{equation}
	T_{I}(SL_{n}(\C)) = \{ X \in M_{n}(\C) \mid \tr(X) = 0 \}
\end{equation}

Inoltre, $ SL_{n}(\C) $ è connesso (e quindi connesso per archi in quanto varietà), chiuso in $ GL_{n}(\C) $ ma non compatto.\\\\
Analogamente ad $ SL_{n}(\R) $, l'insieme $ SL_{n}(\C) $ è una sottovarietà di $ GL_{n}(\C) $ in quanto preimmagine del valore regolare $ 1 \in \mathcal{VR}_{f} $ tramite da funzione

\map{f = \det}%
	{GL_{n}(\C)}{\C}%
	{A}{\det(A)}

da cui la dimensione

\begin{equation}
	\dim_{\R}(SL_{n}(\C)) = \dim_{\R}(GL_{n}(\C)) - \dim_{\R}(\C) = 2 n^{2} - 2
\end{equation}

Per quanto riguarda lo spazio tangente, prendendo il nucleo del differenziale

\map{f_{*A} = \operatorname{det}_{*A}}%
	{T_{A}(GL_{n}(\C))}{\C}%
	{B}{\tr(A^{-1} B)}

si ottiene

\begin{equation}
	T_{I}(SL_{n}(\C)) = \ker(\operatorname{det}_{*I}) = \{ X \in M_{n}(\C) \mid \tr(X) = 0 \}
\end{equation}

il quale ha dimensione $ 2 n^{2} - 2 $ (esattamente come $ SL_{n}(\C) $) in quanto le sue entrate sono vincolate dalla condizione di avere traccia nulla (che vale come due condizioni in quanto le entrate sono complesse).

\subsection{$ O(n) $ come sottogruppo di Lie di $ GL_{n}(\R) $}

Il gruppo delle matrici ortogonali

\begin{equation}
	O(n) = \{ A \in GL_{n}(\R) \mid A^{T} A = I \}
\end{equation}

è un sottogruppo di Lie embedded di $ GL_{n}(\R) $, compatto\footnote{%
	L'insieme $ O(n) $ è compatto in quanto chiuso e limitato in $ GL_{n}(\R) $: chiuso perché preimmagine di $ I \in \mathcal{VR}_{f} $ tramite l'applicazione continua
	
	\map{f}%
		{GL_{n}(\R)}{S(n)}%
		{A}{A^{T} A}
	
	e limitato in quanto le colonne delle matrici di $ O(n) $ hanno norma unitaria e quindi la somma delle colonne è limitata dalla dimensione $ n $ delle matrici.%
} e non connesso (per $ n \geqslant 1 $\footnote{%
	Per $ n=1 $ si ha l'insieme
	
	\begin{equation*}
		O(n) = \{ x \in \R \mid x^{2} = 1 \} = \{ \pm 1 \}
	\end{equation*}
	
	il quale non è connesso.%
}) ma costituito da due componenti connesse

\begin{equation}
	O(n) = SO(n) \sqcup S^{-}(n)
\end{equation}

dove

\begin{equation}
	\begin{cases}
		SO(n) = \{ A \in O(n) \mid \det(A) = 1 \}\\
		S^{-}(n) = \{ A \in O(n) \mid \det(A) = -1 \}
	\end{cases}
\end{equation}

La dimensione di $ O(n) $ è $ n(n-1)/2 $ e il suo spazio tangente coincide con l'insieme delle matrici simmetriche

\begin{equation}
	T_{I}(O(n)) = \{ X \in M_{n}(\R) \mid X^{T} = -X \}
\end{equation}

Sappiamo anche che $ SO(n) $ è un sottogruppo di Lie di $ O(n) $ della stessa dimensione (in quanto componente connessa) e con lo stesso spazio tangente di $ O(n) $.\\\\
%
Per dimostrare che $ O(n) $ non sia connesso (e quindi non connesso per archi in quanto varietà), usiamo il fatto che non esiste un arco che connette due matrici che abbiano rispettivamente determinante 1 e -1. Siano la matrice identità $ I $ e la matrice

\begin{equation}
	A = \smqty(\dmat{-1,1,\ddots,1})
\end{equation}

perciò con $ \det(I) = 1 $ e $ \det(A) = -1 $ e supponiamo per assurdo che esista una curva continua (arco) $ c : [0,1] \to O(n) $ tale che $ c(0) = I $ e $ c(1) = A $. Per la curva vale

\begin{equation}
	c(t) \in O(n) \implies \det(c(t)) \neq 0 \qcomma \forall t \in [0,1]
\end{equation}

e abbiamo che

\begin{equation}
	\begin{cases}
		\det(c(0)) = \det(I) = 1\\
		\det(c(1)) = \det(A) = -1
	\end{cases}
\end{equation}

Per il teorema del valor medio, esiste un $ t_{0} \in [0,1] $ tale che $ \det(c(t_{0})) = 0 $, ma questo è assurdo in quanto $ c(t) \in O(n) $ e dunque $ O(n) $ non è connesso.\\
Verifichiamo ora che il sottoinsieme $ SO(n) $ sia connessa (per archi) tramite la forma canonica ortogonale:

\begin{equation}
	A \in SO(n) \implies %
	\exists P \in O(n) \mid P^{T} A P = %
	\smqty(\dmat{ I_{p}, - I_{q}, R_{1}(\theta_{1}), \ddots, R_{k}(\theta_{k}) })
\end{equation}

con $ p,q,k \in \N $ e $ \theta_{j} \in (0,\pi) $ dove $ k = (n-p-q)/2 $ e $ j=1,\dots,k $, definendo $ R_{j}(\theta_{j}) $ come

\begin{equation}
	R_{j}(\theta_{j}) = \mqty(\dmat{ \cos(\theta_{j}) & \sin(\theta_{j}) \\\\ - \sin(\theta_{j}) & \cos(\theta_{j}) })
\end{equation}

Siccome il teorema vale per $ O(n) $, la condizione $ A \in SO(n) $ implica anche che $ \det(P^{T} A P) = 1 $ e quindi che $ \det(-I_{q}) = 1 $ in quanto $ \det(I_{p}) = \det(R_{j}) = 1 $, i.e. $ q $ deve essere pari. Consideriamo la curva continua (arco)

\map{c}%
	{[0,1]}{SO(n)}%
	{t}{%
		\mqty(\dmat{ %
					I_{p}, R_{1}(\pi t), \ddots, R_{q}(\pi t), R_{1}(t \theta_{1}), \ddots, R_{k}(t \theta_{k}) })
		}

con un numero pari $ q $ di $ R(\pi t) $, per cui vale

\begin{equation}
	\begin{cases}
		c(0) = I\\
		c(1) = P^{T} A P
	\end{cases}
\end{equation}

Considerando ora la curva

\map{g = P \, c \, P^{T}}%
	{[0,1]}{SO(n)}%
	{t}{%
		P \mqty(\dmat{ %
					I_{p}, R_{1}(\pi t), \ddots, R_{q}(\pi t), R_{1}(t \theta_{1}), \ddots, R_{k}(t \theta_{k}) }) P^{T}
	}

per cui vale

\begin{equation}
	\begin{cases}
		g(0) = I\\
		g(1) = A
	\end{cases}
\end{equation}

Abbiamo dunque costruito un arco che collega qualsiasi matrice di $ SO(n) $ alla matrice identità, dunque lo spazio è connesso (per archi).\\
L'insieme $ SO(n) $ è anche una componente connessa di $ O(n) $ perché se esistesse un insieme connesso $ M $ tale che

\begin{equation}
	SO(n) \subsetneqq M \subset O(n)
\end{equation}

un elemento di $ SO(n) $ potrebbe essere unito ad un elemento $ A \in M $ con $ \det(A) = -1 $ e, siccome $ M $ è connesso (per archi), esisterebbe un arco $ c : [0,1] \to M $ tale che

\begin{equation}
	\begin{cases}
		c(0) = I\\
		c(1) = A
	\end{cases}
\end{equation}

Questo è assurdo, per il teorema del valor medio, perché il determinante di $ \det(c(t)) \neq 0 $ ma $ c $ è un'applicazione continua che passerebbe da $ \det(c(0)) = 1 $ a $ \det(c(1)) = -1 $.\\
Analogamente, l'insieme delle matrici con determinante pari a $ -1 $, i.e. $ S^{-}(n) $, è l'altra componente connessa di $ O(n) $, per cui vale

\begin{equation}
	O(n) = SO(n) \sqcup S^{-}(n)
\end{equation}

Consideriamo le due seguenti proposizioni:

\begin{definition}
	Presi uno spazio topologico $ X $ e una sua componente connessa $ C \subset X $, allora $ C $ è chiuso.
\end{definition}

\begin{proof}
	La chiusura $ \bar{C} $ di una componente connessa $ C $ è ancora connessa e anche chiusa, dunque $ \bar{C} \equiv C $ perciò $ C $ è chiuso.
\end{proof}

\begin{definition}
	Siano $ X $ uno spazio topologico localmente euclideo e $ C \subset X $ una sua componente connessa (per archi, in quanto $ X $ è localmente euclideo), allora $ C $ è aperto in $ X $.
\end{definition}

\begin{proof}
	Sia $ p \in C \subset X $: essendo $ X $ localmente euclideo, esiste un intorno aperto $ U \ni p $ che sia connesso (per archi), in quanto la connessione (per archi) si preserva per omeomorfismi.\\
	Se, per assurdo, $ U \not\subset C $ allora $ U \cup C $ è un connesso tale che $ U \cup C \supsetneqq C $ ma questo non è possibile perché avremmo trovato un connesso che contiene strettamente $ C $, in contrasto con il fatto che $ C $ sia una componente connessa.\\
	Questo rende il punto (arbitrario) $ p $ interno a $ C $ e dunque $ C $ è aperto.
\end{proof}

Queste proposizioni insieme asseriscono che uno spazio topologico localmente euclideo, e.g. una varietà differenziabile, ha le sue componenti connesse sia aperte che chiuse: a questo punto, possiamo dire che l'insieme $ SO(n) $ è sia chiuso che aperto in $ O(n) $.\\
Qualunque aperto di una varietà differenziabile è una sottovarietà\footnote{%
	Vedi Osservazione \ref{subvar-open}.%
}, dunque $ SO(n) $ è una sottovarietà di $ O(n) $ e, per questo, è anche un sottogruppo di Lie embedded. La dimensione e lo spazio tangente di un aperto di una varietà sono gli stessi di quelli della varietà\footnote{%
	Vedi Esercizio \ref{es3-6}.%
}, perciò $ \dim(SO(n)) = n(n-1)/2 $ e

\begin{equation}
	T_{I}(SO(n)) = T_{I}(O(n)) = \{ X \in M_{n}(\R) \mid X^{T} = -X \}
\end{equation}

Essendo chiuso e limitato (o, equivalentemente, chiuso in un compatto, i.e. $ O(n) $), $ SO(n) $ è compatto.\\
Per piccoli valori di $ n $, abbiamo che:

\begin{itemize}
	\item $ SO(1) = \{ 1 \} $;
	
	\item $ SO(2) $ è diffeomorfo a $ \S^{1} $\footnote{%
		Vedi Esercizio \ref{BONUS3-1}.%
	};
	
	\item $ SO(3) $ è diffeomorfo a $ \mathcal{RP}^{3} $.
\end{itemize}

\subsection{$ U(n) $ come sottogruppo di Lie di $ GL_{n}(\C) $}

L'insieme delle matrici unitarie

\begin{equation}
	U(n) = \{ A \in GL_{n}(\C) \mid A^{*} A = I \}
\end{equation}

dove $ A^{*} = \bar{A}^{T} $, è un sottogruppo di Lie embedded di $ GL_{n}(\C) $ di dimensione $ \dim_{\R}(U(n)) = n^{2} $ con spazio tangente l'insieme delle matrici antihermitiane

\begin{equation}
	T_{I}(U(n)) = \{ X \in M_{n}(\C) \mid X^{*} = -X \}
\end{equation}

L'insieme $ U(n) $ è anche compatto e connesso.\\
Per dimostrare che $ U(n) $ sia un sottogruppo di Lie embedded di $ GL_{n}(\C) $, è sufficiente mostrare che $ U(n) $ sia una sottovarietà di $ GL_{n}(\C) $. Consideriamo l'applicazione continua

\map{f}%
	{GL_{n}(\C)}{H(n)}%
	{A}{A^{*} A}

dove il codominio è l'insieme delle matrici hermitiane

\begin{equation}
	H(n) = \{ B \in M_{n}(\C) \mid B^{*} = B \}
\end{equation}

Abbiamo che $ U(n) = f^{-1}(I) $, dunque dobbiamo verificare che $ I \in \mathcal{VR}_{f} $ per poter usare il teorema della preimmagine di un valore regolare. Prendiamo quindi differenziale $ f_{*A}(B) $ con $ A \in GL_{n}(\C) $ e $ B \in T_{A}(GL_{n}(\C)) = M_{n}(\C) $ e verifichiamo che sia suriettiva nella restrizione a matrici unitarie: presa una curva $ c : (-\varepsilon,\varepsilon) $ tale che $ c(0) = A $ e $ c'(0) = B $, possiamo scrivere

\begin{align}
	\begin{split}
		f_{*A}(B) &= \eval{ \dv{t} (f \circ c(t)) }_{t=0}\\
		&= \eval{ \dv{t} f(c(t)) }_{t=0}\\
		&= \eval{ \dv{t} (c(t)^{*} c(t)) }_{t=0}\\
		&= \eval{ \dot{c}(t)^{*} c(t) + c(t)^{*} \dot{c}(t) }_{t=0}\\
		&= B^{*} A + A^{*} B
	\end{split}
\end{align}

da cui, tramite le identificazioni $ T_{A}(GL_{n}(\C)) = M_{n}(\C) $ e $ T_{f(A)}(H(n)) = H(n) $ ($ H(n) $ è uno spazio vettoriale)

\map{f_{*A}}%
	{M_{n}(\C)}{H(n)}%
	{B}{B^{*} A + A^{*} B}

A questo punto abbiamo che

\begin{equation}
	I \in \mathcal{VR}_{f} \iff \exists B \in M_{n}(\C) \mid f_{*A}(B) = C \qcomma \forall C \in H(n), \, \forall A \in U(n) = f^{-1}(I)
\end{equation}

cioè $ f_{*A} $ è suriettiva per $ A \in U(n) $: questo si verifica per $ B = AC/2 $

\begin{align}
	\begin{split}
		f_{*A} \left( \dfrac{A C}{2} \right) &= \left( \dfrac{A C}{2} \right)^{*} A + A^{*} \left( \dfrac{A C}{2} \right)\\
		&= \dfrac{1}{2} (C^{*} A^{*} A + A^{*} A C)\\
		&= \dfrac{1}{2} (C^{*} + C)\\
		&= \dfrac{1}{2} (C + C)\\
		&= C
	\end{split}
\end{align}

in quanto $ C \in H(n) $, perciò $ I \in \mathcal{VR}_{f} $ e dunque $ U(n) $ è una sottovarietà di $ GL_{n}(\C) $.\\
La dimensione di $ U(n) $ deriva dal teorema della preimmagine

\begin{equation}
	\dim_{\R}(U(n)) = \dim_{\R}(GL_{n}(\C)) - \dim_{\R}(H(n)) = 2 n^{2} - \left( n +\dfrac{n (n-1)}{2} \, 2 \right) = 2 n^{2} - n^{2} = n^{2}
\end{equation}

dove la dimensione di $ H(n) $ deriva dal fatto che gli $ n $ elementi della diagonale sono reali mentre quelli sopra dalla diagonale determinano anche quelli sotto e sono complessi (da cui $ 2 (n(n-1)/2) $).\\
Sempre dal teorema della preimmagine, lo spazio tangente di $ U(n) $ è

\begin{equation}
	T_{I}(U(n)) = \ker(f_{*I}) = \{ X \in M_{n}(\C) \mid X^{*} = -X \}
\end{equation}

cioè $ T_{I}(U(n)) = H^{-}(n) $ ovvero lo spazio delle matrici antihermitiane.\\
La dimostrazione che $ U(n) $ sia compatto è analoga a quella per $ O(n) $. Per dimostrare che sia connesso, sia $ A \in U(n) $, per il corollario del teorema spettrale esiste una matrice $ U \in U(n) $ tale che

\begin{equation}
	U^{-1} A U = %
	\smqty(\dmat{ e^{i \theta_{1}}, \ddots, e^{i \theta_{n}} })
\end{equation}

con $ \theta_{j} \in \R $ per $ j=1,\dots,n $. Possiamo definire una curva continua

\map{c}%
	{[0,1]}{U(n)}%
	{t}{U \smqty(\dmat{ e^{i t \theta_{1}}, \ddots, e^{i t \theta_{n}} }) U^{-1}}

dove

\begin{equation}
	\begin{cases}
		c(0) = I\\
		c(1) = A
	\end{cases}
\end{equation}

Esiste dunque un arco che connette ogni matrice unitaria alla matrice identità perciò $ U(n) $ è connesso.

\subsection{$ SU(n) $ come sottogruppo di Lie di $ U(n) $}

L'insieme delle matrici

\begin{equation}
	U(n) = \{ A \in GL_{n}(\C) \mid A^{*} A = I \}
\end{equation}

è un sottogruppo di Lie embedded di $ U(n) $ di dimensione $ \dim_{\R}(SU(n)) = n^{2} - 1 $ con spazio tangente l'insieme delle matrici antihermitiane a traccia nulla

\begin{equation}
	T_{I}(U(n)) = \{ X \in M_{n}(\C) \mid X^{*} = -X \wedge \tr(X) = 0 \}
\end{equation}

L'insieme $ SU(n) $ è anche compatto e connesso.\\
Per dimostrare che sia un sottogruppo di Lie embedded di $ U(n) $, utilizziamo il teorema della preimmagine per dimostrare che ne sia una sottovarietà tramite l'applicazione

\map{f = \det}%
	{U(n)}{\S^{1}}%
	{A}{\det(A)}

Questa applicazione ha come codominio $ S(1) = \{ \pm 1 \} $ perché per $ A \in U(n) $ vale

\begin{equation}
	A^{*} A = I \implies %
	\begin{aligned}
		\det(I) &= \det(A^{*} A)\\
		1 &= \det(A^{*}) \det(A)\\
		&= \overline{\det(A)} \det(A)\\
		&= \abs{\det(A)}^{2}
	\end{aligned}%
	\implies %
	\det(A) = e^{i \theta} \qcomma \theta \in \R
\end{equation}

Prendendo $ 1 \in \C $, $ SU(n) $ ne è la controimmagine tramite il determinante, i.e. $ SU(n) = f^{-1}(1) $: perché sia una sottovarietà di $ U(n) $ è sufficiente che $ 1 \in \mathcal{VR}_{f} $, quindi prendiamo il differenziale dell'applicazione 

\map{\operatorname{det}_{*A}}%
	{T_{A}(U(n))}{T_{\det(A)}(\S^{1})}%
	{B}{\det(A) \tr(A^{-1} B)}
	
Siccome siamo interessati agli elementi di $ SU(n) $, abbiamo che $ \det(A) = 1 $, perciò dobbiamo studiare $ T_{1}(\S^{1}) $ con $ \S^{1} \subset \C $: possiamo trovare gli elementi di questo spazio tangente considerando la curva

\map{c}%
	{(-\varepsilon,\varepsilon)}{\S^{1}}%
	{t}{e^{i a t}}
	
con $ a \in \R $, $ c(0) = 1 $ e $ \dot{c} = i a $. Dato che $ \dot(c) \in T_{1}(\S^{1}) $, possiamo asserire che

\begin{equation}
	T_{1}(\S^{1}) = \expval{i}_{\R}
\end{equation}

cioè tutti i vettori paralleli all'asse immaginario che partano in $ 1 $.\\
A questo punto, perché $ 1 \in \mathcal{VR}_{f} $, è necessario che il differenziale sua suriettivo per $ \forall A \in SU(n) $ o equivalentemente

\begin{equation}
	1 \in \mathcal{VR}_{f} \iff \exists B \in T_{A}(U(n)) \mid \tr(A^{-1} B) = i a \qcomma A \in SU(n), \, a \in \R
\end{equation}

Abbiamo dimostrato che $ T_{A}(U(n)) = A \, T_{I}(U(n)) $ e $ T_{I}(U(n)) = H^{-}(n) $ cioè le matrici antihermitiane, perciò

\begin{equation}
	T_{A}(U(n)) = \{ AX \mid X^{*} = -X \}
\end{equation}

Per soddisfare la condizione, prendiamo $ B = AX $ con $ X = (i a/n) I $, da cui

\begin{equation}
	\tr(A^{-1} B) = \tr(A^{-1} A \, \dfrac{ia}{n} \, I) = \dfrac{ia \tr(I)}{n} = i a
\end{equation}

Questo dimostra che $ 1 \in \mathcal{VR}_{f} $ perciò $ SU(n) $ è una sottovarietà e quindi un sottogruppo di Lie embedded di $ U(n) $ di dimensione

\begin{equation}
	\dim_{\R}(SU(n)) = \dim_{\R}(U(n)) - \dim_{\R}(\S^{1}) = n^{2} - 1
\end{equation}

Lo spazio tangente di $ SU(n) $ deriva ancora dal teorema della preimmagine

\begin{equation}
	T_{I}(SU(n)) = \ker(\operatorname{det}_{*A}) = \{ X \in T_{I}(U(n)) \mid \tr(X) = 0 \}
\end{equation}

ma sappiamo che

\begin{equation}
	T_{I}(U(n)) = \{ X \in M_{n}(\C) \mid X^{*} = - X \}
\end{equation}

perciò

\begin{equation}
	T_{I}(U(n)) = \{ X \in M_{n}(\C) \mid X^{*} = -X \wedge \tr(X) = 0 \}
\end{equation}

Essendo $ SU(n) $ chiuso nel compatto $ U(n) $ (o anche chiuso e limitato), $ SU(n) $ è compatto.\\
L'insieme $ SU(n) $ è connesso perché, presa $ A \in SU(n) $, per il corollario del teorema spettrale, esiste una matrice $ U \in U(n) $ tale che

\begin{equation}
	U^{-1} A U = %
	\smqty(\dmat{ e^{i \theta_{1}}, \ddots, e^{i \theta_{n}} })
\end{equation}

con $ \theta_{j} \in \R $ per $ j=1,\dots,n $ e siccome il determinante deve essere unitario

\begin{equation}
	\sum_{j=1}^{n} \theta_{j} = 2 k \pi \qcomma k \in \Z
\end{equation}

Possiamo definire una curva continua

\map{c}%
	{[0,1]}{SU(n)}%
	{t}{U \smqty(\dmat{ e^{i t \theta_{1}}, \ddots, e^{i t \theta_{n}} }) U^{-1}}

dove

\begin{equation}
	\begin{cases}
		c(0) = I\\
		c(1) = A
	\end{cases}
\end{equation}

e la curva appartiene a $ SU(n) $ per Binet.\\
Esiste dunque un arco che connette ogni matrice unitaria speciale alla matrice identità perciò $ SU(n) $ è connesso\footnote{%
	Vedi Esercizio \ref{BONUS3-2}.%
}.\\
Notiamo anche che

\begin{equation}
	SU(1) = U(1) = SO(2) = \S^{1}
\end{equation}

\subsection{Isomorfismo tra $ GL_{n}(\R) $ e $ SL_{n}(\R) \times \R \setminus \{0\} $}

Un gruppo di Lie è un gruppo algebrico con operazioni lisce che sia anche una varietà differenziabile. Due gruppi di Lie possono essere legati da un diffeomorfismo, in quanto varietà, ma non essere isomorfe: questo succede se il diffeomorfismo non è anche un omomorfismo di gruppi, i.e. non preserva la moltiplicazione tra i gruppi.\\
Sia l'applicazione

\map{f}%
	{GL_{n}(\R)}{SL_{n}(\R) \times \R \setminus \{0\}}%
	{A}{(A M_{\sfrac{1}{\det(A)}},\det(A))}

dove il codominio è un prodotto diretto tra i gruppi di Lie (il gruppo delle matrici invertibili con determinante unitario $ SL_{n}(\R) $ e $ (\R \setminus \{0\},\cdot) $ con $ \cdot $ il prodotto tra numeri reali) e

\begin{equation}
	M_{r} \doteq %
	\mqty( r & 0_{1,n} \\\\ 0_{n,1} & I_{n-1} ) = %
	\smqty(\dmat{ r, 1, \ddots, 1 })
\end{equation}

con $ \det(M_{r}) = r $.\\
L'applicazione $ f $ è un diffeomorfismo ma non un isomorfismo.\\
L'immagine è ben definita, i.e. il primo termine appartiene a $ SL_{n}(\R) $

\begin{equation}
	\det(A M_{\sfrac{1}{\det(A)}}) = \det(A) \det(M_{\sfrac{1}{\det(A)}}) = \det(A) \dfrac{1}{\det(A)} = 1
\end{equation}

e il secondo appartiene a $ \R \setminus \{0\} $, il che è verificato in quanto $ \det(A) \neq 0 $ per $ \forall A \in GL_{n}(\R) $.\\
Questa applicazione è liscia perché tutte le entrate sono lisce e perché, considerando l'applicazione liscia

\begin{equation}
	g : GL_{n}(\R) \to GL_{n}(\R) \times \R \setminus \{0\}
\end{equation}
 
il codominio di $ f $ è una restrizione del codominio di $ g $ ad una sua sottovarietà.\\
L'inversa di $ f $ (ancora liscia anche perché restrizione di un'applicazione liscia) è

\map{f^{-1}}%
	{SL_{n}(\R) \times \R \setminus \{0\}}{GL_{n}(\R)}%
	{(S,r)}{S M_{r}}

in quanto

\begin{align}
	\begin{split}
		(f^{-1} \circ f)(A) &= f^{-1} \left( A M_{\sfrac{1}{\det(A)}},\det(A) \right)\\
		&= A M_{\sfrac{1}{\det(A)}} M_{\det(A)}\\
		&= A\\\\
		%
		(f \circ f^{-1})(S,r) &= f(S M_{r})\\
		&= \left( S M_{r} M_{\sfrac{1}{\det(S M_{r})}},\det(S M_{r}) \right)\\
		&= \left( S M_{r} M_{\sfrac{1}{\det(S) \det(M_{r})}},\det(S) \det(M_{r}) \right)\\
		&= \left( S M_{r} M_{\sfrac{1}{r}},r \right)\\
		&= (S,r)
	\end{split}
\end{align}

dove $ \det(S) = 1 $ poiché $ S \in SL_{n} (\R) $. Questo dimostra che $ f $ è un diffeomorfismo.\\
Perché sia un isomorfismo, oltre ad essere un diffeomorfismo, dovrebbe essere un omomorfismo e quindi preservare la moltiplicazione, ma

\begin{align}
	\begin{split}
		f(A B) &\neq f(A) \, f(B)\\
		\left( A B M_{\sfrac{1}{\det(A B)}}, \det(A B) \right) &\neq \left( A M_{\sfrac{1}{\det(A)}},\det(A) \right) \, \left( B M_{\sfrac{1}{\det(B)}},\det(B) \right)\\
		&\neq \left( A M_{\sfrac{1}{\det(A)}} B M_{\sfrac{1}{\det(B)}},\det(A) \det(B) \right)\\
		&\neq \left( A M_{\sfrac{1}{\det(A)}} B M_{\sfrac{1}{\det(B)}},\det(A B) \right)
	\end{split}
\end{align}

in quanto $ [B, M_{\sfrac{1}{\det(A)}}] \neq 0 $.\\
Da questa analisi otteniamo che

\begin{equation}
	GL_{n}(\R) \stackrel{diff.}{\simeq} SL_{n}(\R) \times \R \setminus \{0\} \quad \wedge \quad GL_{n}(\R) \stackrel{iso.}{\not\simeq} SL_{n}(\R) \times \R \setminus \{0\}
\end{equation}

Inoltre, dal diffeomorfismo, ricaviamo che $ GL_{n}(\R) $ è costituito da due componenti connesse (in quanto $ SL_{n}(\R) $ è connesso), i.e. le matrici con determinante positivo e quelle con determinante negativo: siccome $ \R \setminus \{0\} $ è costituito da due componenti connesse

\begin{equation}
	\R \setminus \{0\} = \R^{+} \sqcup \R^{-}
\end{equation}

abbiamo che

\begin{equation}
	GL_{n}(\R) = (SL_{n}(\R) \times \R^{+}) \sqcup (SL_{n}(\R) \times \R^{-}) \doteq GL_{n}^{+}(\R) \sqcup GL_{n}^{-}(\R)
\end{equation}

In generale, $ GL_{n}(\R) $ e $ SL_{n}(\R) \times \R \setminus \{0\} $ non sono quindi isomorfi: questo rimane vero per $ n $ pari, ma per $ n $ dispari è possibile trovare un isomorfismo tra questi due gruppi.
Preso un gruppo algebrico $ G $, il suo \textit{centro} $ Z(G) $ è l'insieme  degli elementi di $ G $ che commutano con tutti gli altri elementi, i.e.

\begin{equation}
	Z(G) = \{ z \in G \mid zg = gz, \, \forall g \in G \}
\end{equation}

Il centro del gruppo lineare è l'insieme di tutte le matrici scalari, i.e.

\begin{equation}
	Z(GL_{n}(\R)) = \{ A \in M_{n}(\R) \mid A = c I, \, c \in \R \setminus \{0\} \}
\end{equation}

Questo significa che

\begin{equation}
	Z(GL_{n}(\R)) = \R \setminus \{0\} \times I = \R \setminus \{0\}
\end{equation}

Siccome il centro del prodotto di due gruppi è il prodotto del centro dei gruppi, possiamo scrivere

\begin{equation}
	Z(SL_{n}(\R) \times \R \setminus \{0\}) = Z(SL_{n}(\R)) \times Z(\R \setminus \{0\})
\end{equation}

L'insieme dei reali è commutativo, i.e. $ Z(G) = G $ per $ G $ abeliano, dunque coincide con il suo centro mentre il gruppo lineare speciale ha centro diverso a seconda della dimensione delle matrici:

\begin{equation}
	Z(SL_{n}(\R)) = %
	\begin{cases}
		\{ \pm I \} & n = 2 k\\
		\{ I \} & n = 2 k + 1
	\end{cases}%
	\qcomma k \in \N
\end{equation}

A questo punto, abbiamo che per $ n $ pari

\begin{align}
	\begin{split}
		Z(GL_{n}(\R)) &\neq Z(SL_{n}(\R) \times \R \setminus \{0\})\\
		\{ I \} \times \R \setminus \{0\} &\neq Z(SL_{n}(\R)) \times Z(\R \setminus \{0\})\\
		&\neq Z(SL_{n}(\R)) \times Z(\R \setminus \{0\})\\
		&\neq \{ \pm I \} \times \R \setminus \{0\}
	\end{split}
\end{align}

mentre per $ n $ dispari

\begin{equation}
	Z(GL_{n}(\R)) = Z(SL_{n}(\R) \times \R \setminus \{0\}) = \{ I \} \times \R \setminus \{0\}
\end{equation}

Questo prova che non sia possibile costruire un isomorfismo per $ n $ pari in quanto, se i gruppi fossero isomorfi, in particolare dovrebbero esserlo anche i loro centri, fatto che non si verifica in questo caso.\\
A questo punto, esibiamo un isomorfismo esplicito, per $ n $ dispari, tra $ GL_{n}(\R) $ e $ SL_{n}(\R) \times \R \setminus \{0\} $:

\map{h}%
	{GL_{n}(\R)}{SL_{n}(\R) \times \R \setminus \{0\}}%
	{A}{\left( \det(A)^{\sfrac{-1}{n}} \, A, \det(A) \right)}

Notiamo che la prima entrata è ben definita in quanto esiste sempre la radice $ n $-esima di un numero reale per $ n $ dispari, che l'applicazione è liscia e la sua inversa è liscia

\map{h^{-1}}%
	{SL_{n}(\R) \times \R \setminus \{0\}}{GL_{n}(\R)}%
	{(S,r)}{r^{\sfrac{1}{n}} S}
	
Verifichiamo che sia l'inversa:

\begin{align}
	\begin{split}
		(h^{-1} \circ h)(A) &= h^{-1} \left( \det(A)^{\sfrac{-1}{n}} \, A, \det(A) \right)\\
		&= \det(A)^{\sfrac{-1}{n}} \det(A)^{\sfrac{-1}{n}} A\\
		&= A\\\\
		%
		(h \circ h^{-1})(S,r) &= h \left( r^{\sfrac{1}{n}} S \right)\\
		&= \left( \det(r^{\sfrac{1}{n}} S)^{\sfrac{-1}{n}} \, r^{\sfrac{1}{n}} S, \det(r^{\sfrac{1}{n}} S) \right)\\
		&= \left( \left( (r^{\sfrac{1}{n}})^{n} \det(S) \right)^{\sfrac{-1}{n}} \, r^{\sfrac{1}{n}} S, (r^{\sfrac{1}{n}})^{n} \det(S) \right)\\
		&= \left( r^{\sfrac{-1}{n}} r^{\sfrac{1}{n}} S, r \right)\\
		&= (S,r)
	\end{split}
\end{align}

dove $ \det(c \, A) = c^{n} \det(A) $ per $ \forall c \in \R $ e $ \forall A \in M_{n}(\R) $ e $ \det(S) = 1 $ poiché $ S \in SL_{n} (\R) $.\\
Verifichiamo ora che sia un omomorfismo:

\begin{align}
	\begin{split}
		h(A B) &= \left( \det(A B)^{\sfrac{-1}{n}} \, A B, \det(A B) \right)\\
		&= \left( \det(A)^{\sfrac{-1}{n}} \, A \, \det(B)^{\sfrac{-1}{n}} \, B, \det(A) \det(B) \right)\\
		&= \left( \det(A)^{\sfrac{-1}{n}} \, A, \det(A) \right) \, \left( \det(B)^{\sfrac{-1}{n}} \, B, \det(B) \right)\\
		&= h(A) \, h(B)
	\end{split}
\end{align}

Da cui otteniamo che $ h $ è un isomorfismo di gruppi di Lie tra $ GL_{n}(\R) $ e $ SL_{n}(\R) \times \R \setminus \{0\} $.

\section{Algebre di Lie}

Un'\textit{algebra di Lie su un campo} $ \K $ è uno spazio vettoriale\footnote{%
	Non necessariamente di dimensione finita.%
} $ V $ dotato di un'applicazione

\map{[,]}%
	{V \times V}{V}%
	{(x,y)}{[x,y]}

tale che:

\begin{itemize}
	\item L'applicazione $ [,] $ sia bilineare, i.e.
	
	\begin{equation}
		\begin{cases}
			[ax+by,z] = a[x,z] + b[y,z]\\
			[x,cz+dw] = c[x,z] + d[x,w]
		\end{cases}
	\end{equation}
	
	per $ \forall a,b,c,d \in \R $ e $ \forall x,y,z,w \in V $;
	
	\item L'applicazione sia antisimmetrica, i.e.
	
	\begin{equation}
		[x,y] = -[y,x], \qquad \forall x,y \in V
	\end{equation}
	
	\item Valga l'\textit{identità di Jacobi}
	
	\begin{equation}
		[x,[y,z]] + [y,[z,x]] + [z,[x,y]] = 0, \qquad \forall x,y,z \in V
	\end{equation}
\end{itemize}

\subsubsection{\textit{Esempi}}

\paragraph{1. Algebra di Lie abeliana}

Se $ V $ è uno spazio vettoriale su $ \K $, possiamo definire il commutatore nullo

\begin{equation}
	[x,y] = 0, \qquad \forall x,y \in V
\end{equation}

che definisce l'algebra di Lie \textit{abeliana} (gli elementi $ x $ e $ y $ commutano).

\paragraph{2. Insieme delle matrici quadrate}

Siano lo spazio vettoriale $ M_{n}(\K) $ delle matrici quadrate\footnote{%
	Questo spazio vettoriale ha dimensione $ n^{2} $.%
} su un campo $ \K $ e il commutatore

\begin{equation}
	[A,B] = AB - BA
\end{equation}

La verifica delle proprietà è immediata e dunque $ (V,[,]) $ è un'algebra di Lie.

\paragraph{3. Commutatore}

Una qualsiasi algebra $ A $ su un campo $ \K $ con commutatore

\begin{equation}
	[a,b] = ab - ba
\end{equation}

è un'algebra di Lie su campo $ \K $.

\subsection{Algebre e gruppi di Lie}

Siano un gruppo di Lie $ G $ e il suo elemento neutro $ e $, possiamo associare lo spazio vettoriale tangente a $ G $ in $ e $, i.e. $ T_{e}(G) $, il quale ha la stessa dimensione del gruppo di Lie $ \dim(T_{e}(G)) = \dim(G) $.\\
Vogliamo definire una struttura di algebra di Lie su $ T_{e}(G) $ che sia legata al gruppo di Lie $ G $: per fare questo, definiamo un commutatore

\map{[,]}%
	{T_{e}(G) \times T_{e}(G)}{T_{e}(G)}%
	{(v,w)}{[v,w]}

su $ T_{e}(G) $ che sia bilineare, antisimmetrico e che rispetti l'identità di Jacobi.\\
L'idea è usare il commutatore dei campi di vettori lisci sul gruppo di Lie $ G $, i.e. considerare l'algebra $ (\chi(G),[,]) $, per indurre un commutatore su $ T_{e}(G) $.

\subsubsection{Campi di vettori invarianti a sinistra}

Sia $ X $ un campo di vettori su $ G $ (non necessariamente liscio), diremo che $ X $ è \textit{invariante a sinistra} se

\begin{equation}
	L_{g_{*}} X = X \qcomma \forall g \in G
\end{equation}

cioè il pushforward\footnote{%
	Sia un'applicazione $ F : N \to M $ un diffeomorfismo (quindi sia iniettiva che suriettiva), il pushforward di $ X $ tramite $ F $ è definito come

	\begin{equation*}
		(F_{*} (X))_{q} = F_{*F^{-1}(q)} (X_{F^{-1}(q)})
	\end{equation*}%
} di $ X $ tramite la traslazione a sinistra è identico a sé stesso, dove la traslazione a sinistra è definita come

\map{L_{g}}%
	{G}{G}%
	{h}{g h}

la quale è un diffeomorfismo (quindi è possibile definire il pushforward in quanto $ L_{g} $ è sia iniettiva che suriettiva) con inversa $ L_{g}^{-1} = L_{g^{-1}} $. Equivalentemente, il campo di vettori $ X $ è invariante per traslazioni a sinistra se

\begin{equation}
	L_{g_{*h}} (X_{h}) = X_{gh} \qcomma \forall g,h \in G
\end{equation}

ciò significa che $ X $ è $ L_{g} $-related\footnote{%
	Sia un diffeomorfismo $ F : N \to M $, un campo di vettori $ X $ è $ F $-related a un altro campo di vettori $ Y $ se
	
	\begin{equation*}
		F_{*p}(X_{p}) = Y_{F(p)} \qcomma \forall p \in N
	\end{equation*}

	i.e. $ Y $ è il pushforward di $ X $ tramite $ F $.%
} a sé stesso.

\begin{remark}
	Un campo di vettori invariante a sinistra $ X $ è determinato dal suo valore nell'elemento neutro $ X_{e} $ in quanto
	
	\begin{equation}
		X_{g} = L_{g_{*e}} (X_{e})
	\end{equation}
\end{remark}

I campi di vettori invarianti a sinistra sono importanti perché esiste un isomorfismo (di spazi vettoriali) tra questi e lo spazio tangente ad un gruppo di Lie.

\begin{theorem}[Proprietà dei campi di vettori invarianti a sinistra]
	Siano $ G $ un gruppo di Lie ed $ e \in G $ il suo elemento neutro. Valgono le seguenti proprietà:
	
	\begin{enumerate}
		\item L'applicazione
		
		\map{\hat{}}%
			{T_{e}(G)}{L(G)}%
			{A}{\hat{A}}
			
		dove $ L(G) $ indica l'insieme dei campi di vettori invarianti a sinistra di $ G $ e
		
		\begin{equation}
			\hat{A}_{g} \doteq L_{g_{*e}}(A)
		\end{equation}
	
		con $ L_{g_{*e}} : T_{e}(G) \to T_{g}(G) $, è un isomorfismo di spazi vettoriali;
		
		\item L'insieme dei campi di vettori invarianti a sinistra di $ G $ è contenuto nell'insieme dei campi di vettori lisci su $ G $, i.e. $ L(G) \subset \chi(G) $ quindi un campo di vettori invariante a sinistra è sempre liscio;
		
		\item Il commutatore di due campi di vettori invarianti a sinistra è ancora un campo di vettori invariante a sinistra, i.e.
		
		\begin{equation}
			\comm{X}{Y} \in L(G) \qcomma \forall X,Y \in L(G)
		\end{equation}
	
		quindi i campi di vettori invarianti a sinistra sono chiusi rispetto al commutatore.
	\end{enumerate}
\end{theorem}

\begin{proof}
	\begin{enumerate}
		\item Verifichiamo innanzitutto che $ \hat{A} \in L(G) $:
		
		\begin{gather}
			\hat{A} \in L(G)\nonumber\\%
			\Updownarrow\nonumber\\%
			L_{g_{*}}(\hat{A}) = \hat{A} \qcomma \forall g \in G\\%
			\Updownarrow\nonumber\\%
			L_{g_{*h}}(\hat{A}_{h}) = \hat{A}_{gh} \qcomma \forall g,h \in G\nonumber
		\end{gather}
	
		Possiamo dimostrare l'ultima equazione tramite la definizione di $ \hat{A}_{g} $
		
		\begin{equation}
			\hat{A}_{g} \doteq L_{g_{*e}}(A)
		\end{equation}
	
		otteniamo dunque che
		
		\begin{align}
			\begin{split}
				L_{g_{*h}}(\hat{A}_{h}) &= L_{g_{*h}}(L_{h_{*e}}(A)) = \hat{A}_{gh}\\
				&= (L_{g} \circ L_{h})_{*e} (A)\\
				&= L_{gh_{*e}} (A)\\
				&= \hat{A}_{gh}
			\end{split}
		\end{align}
	
		quindi $ \hat{A} \in L(G) $.\\
		Per dimostrare che $ \hat{} $ sia un isomorfismo tra spazi vettoriali dobbiamo far vedere che sia un omomorfismo invertibile. Per dimostrare che sia un omomorfismo dobbiamo mostrare che $ \hat{} $ sia un'applicazione lineare, i.e.
		
		\begin{equation}
			\widehat{\lambda A + \mu B} = \lambda \hat{A} + \mu \hat{B}
		\end{equation}
	
		per fare ciò, applichiamo il primo membro ad un qualunque elemento $ g \in G $:
	
		\begin{align}
			\begin{split}
				(\widehat{\lambda A + \mu B})_{g} &\doteq L_{g_{*e}} (\lambda A + \mu B)\\
				&= \lambda L_{g_{*e}} (A) + \mu L_{g_{*e}} (B)\\
				&\doteq \lambda \hat{A}_{g} + \mu \hat{B}_{g}\\
				&= (\lambda \hat{A} + \mu \hat{B})_{g}
			\end{split}
		\end{align}
	
		per $ \forall \lambda,\mu \in \R $ e $ \forall A,B \in T_{e}(G) $. L'applicazione inversa (lineare) è la "valutazione in $ e $"
		
		\map{\mid_{e}}%
			{L(G)}{T_{e}(G)}%
			{X}{X_{e}}
			
		in quanto
		
		\begin{equation}
			(\mid_{e} \circ \, \hat{} \,)(A) = \hat{A}_{e} = L_{e_{*e}} (A) = A
		\end{equation}
	
		poiché
		
		\begin{equation}
			L_{e} = \operatorname{id}_{G} \implies L_{e_{*e}} = \operatorname{id}_{T_{e}(G)}
		\end{equation}
	
		e viceversa verifichiamo che
		
		\begin{equation}
			(\, \hat{} \, \circ \mid_{e})(A) = \hat{X_{e}} = X
		\end{equation}
	
		applicando il primo membro ad un qualunque elemento $ g \in G $:
		
		\begin{equation}
			\hat{X_{e}}_{| g} \doteq L_{g_{*e}}(X_{e}) = X_{g} \qcomma \forall g \in G \implies \hat{X_{e}} = X
		\end{equation}
		
		\item Un campo di vettori è liscio se la derivata di una funzione liscia rispetto al campo di vettori è ancora una funzione liscia, i.e.
		
		\begin{equation}
			X \in \chi(G) \iff X f \in C^{\infty}(G) \qcomma \forall f \in C^{\infty}(G)
		\end{equation}
	
		cioè la derivata direzionale $ (X f)(g) = X_{g} f $ varia in modo liscio al variare di $ g \in G $: presa una curva liscia
		
		\begin{equation}
			\begin{cases}
				\gamma : (-\varepsilon,\varepsilon) \to G\\
				\gamma(0) = g\\
				\gamma'(0) = X_{g}
			\end{cases}
		\end{equation}
	
		abbiamo che
		
		\begin{equation}
			X_{g} f = \eval{ \dv{t} f(\gamma(t)) }_{t=0}
		\end{equation}
	
		Consideriamo dunque la curva $ \gamma = g \cdot c $ dove $ \cdot $ è il prodotto in $ G $ e
		
		\begin{equation}
			\begin{cases}
				c : (-\varepsilon,\varepsilon) \to G\\
				c(0) = e\\
				c'(0) = X_{e}
			\end{cases}
		\end{equation}
	
		che rispetta le condizioni poste su $ \gamma $:
		
		\begin{align}
			\begin{split}
				\gamma(0) &= g \, c(0)= g \, e = g\\\\
				%
				\gamma'(0) &= \eval{ \dv{t} g \, c(t) }_{t=0}\\
				&= \eval{ \dv{t} L_{g}(c(t)) }_{t=0}\\
				&= (L_{g} \circ c)'(0)\\
				&= L_{g_{*c(0)}} (c'(0))\\
				&= L_{g_{*e}} (X_{e})\\
				&= X_{g}
			\end{split}
		\end{align}
	
		dove nel quarto passaggio abbiamo utilizzato
		
		\begin{equation}
			F_{*p}(X_{p}) = (F \circ c)'(0)
		\end{equation}
		
		e nell'ultimo abbiamo utilizzato il fatto che $ X $ è invariante a sinistra.\\
		A questo punto, per mostrare che $ X \in \chi(G) $ basta mostrare che
		
		\begin{equation}
			X_{g} f = \eval{ \dv{t} f(\gamma(t)) }_{t=0} = \eval{ \dv{t} f(g \cdot c(t)) }_{t=0}
		\end{equation}
	
		sia liscia in $ G $: questo è vero perché $ f \in C^{\infty}(G) $ per ipotesi, $ c(t) $ è una curva liscia, il prodotto $ \cdot $ è liscio in quanto $ G $ è un gruppo di Lie e $ f(g \cdot c(t)) $ rimane liscia se derivata rispetto a $ t $.
		
		\item Siano $ X,Y \in L(G) \subset \chi(G) $, perché il loro commutatore appartenga a $ L(G) $ è necessario che
		
		\begin{equation}
			L_{g_{*}}(\comm{X}{Y}) = \comm{X}{Y} \qcomma \forall g \in G \iff \comm{X}{Y} \in L(G)
		\end{equation}
	
		Sappiamo che $ X $ e $ Y $ sono lisci e inoltre sono $ L_{g} $-related rispettivamente a $ L_{g_{*}}(X) $ e $ L_{g_{*}}(Y) $ dunque, per il Corollario \ref{frel-brack}, abbiamo che
		
		\begin{equation}
			L_{g_{*}}(\comm{X}{Y}) = \comm{L_{g_{*}}(X)}{L_{g_{*}}(Y)}
		\end{equation}
	
		Essendo $ X $ e $ Y $ invarianti a sinistra, i.e.
		
		\begin{equation}
			\begin{cases}
				L_{g_{*}}(X) = X\\
				L_{g_{*}}(Y) = Y
			\end{cases}
		\end{equation}
		
		abbiamo dunque che
		
		\begin{equation}
			L_{g_{*}}(\comm{X}{Y}) = \comm{L_{g_{*}}(X)}{L_{g_{*}}(Y)} = \comm{X}{Y}
		\end{equation}		
	\end{enumerate}
\end{proof}

\begin{corollary}
	La coppia $ (L(G),\comm{}{}) $ è una sottoalgebra\footnote{%
		Presi due spazi vettoriali $ V $ e $ W \subset V $ con $ (V,\comm{}{}) $ algebra di Lie, $ (W,\comm{}{}_{| W}) $ è una sottoalgebra di Lie se $ W $ è un sottospazio vettoriale e
		
		\begin{equation}
			\comm{w_{1}}{w_{2}} \in W \qcomma \forall w_{1},w_{2} \in W
		\end{equation}%
	} di Lie di $ (\chi(G),\comm{}{}) $ e dunque essa stessa un'algebra di Lie finito-dimensionale, in quanto ha la stessa dimensione dello spazio tangente $ T_{e}(G) $ poiché $ L(G) \stackrel{iso.}{\simeq} T_{e}(G) $.
\end{corollary}

\subsection{Algebra di Lie su $ T_{e}(G) $}

Siano $ G $ un gruppo di Lie, $ e \in G $ il suo elemento neutro e $ T_{e}(G) $ lo spazio tangente a $ G $ nell'elemento $ e $.\\
Dati $ A,B \in T_{e}(G) $ definiamo il commutatore tra gli elementi dello spazio tangente mediante il commutatore tra i campi di vettori\footnote{%
	I due commutatori sono diversi in quanto hanno come entrate oggetti diversi (nel primo elementi dello spazio tangente a $ G $ mentre nel secondo campi di vettori invarianti a sinistra) e appartengono a spazi diversi, i.e. $ \comm{A}{B} \in T_{e}(G) $ mentre $ \comm{\hat{A}}{\hat{B}} \in L(G) $.%
}

\begin{equation}
	\comm{A}{B} \doteq \comm{\hat{A}}{\hat{B}}_{e}
\end{equation}

cioè tramite l'isomorfismo

\map{\hat{}}%
	{T_{e}(G)}{L(G)}%
	{A}{\hat{A}}
	
dove

\begin{equation}
	\hat{A}_{g} = L_{g_{*e}}(A)
\end{equation}

portiamo gli elementi dello spazio tangente $ A $ e $ B $ nei rispettivi campi di vettori invarianti a sinistra $ \hat{A} $ e $ \hat{B} $, ne facciamo il commutatore (come campi di vettori) e poi lo valutiamo nell'elemento $ e $, i.e. utilizzando l'inversa dell'isomorfismo

\map{\mid_{e}}%
	{L(G)}{T_{e}(G)}%
	{X}{X_{e}}

ottenendo dunque il commutatore $ \comm{A}{B} $ come un elemento dello spazio tangente $ T_{e}(G) $.\\
In questo modo, l'applicazione $ \hat{} $ è un isomorfismo di algebre di Lie tra $ (T_{e}(G),[,]) $ e $ (L(G),[,]) $ con i rispettivi commutatori; le proprietà di algebra di Lie sono rispettate da $ (T_{e}(G),[,]) $ in quanto isomorfo a $ (L(G),[,]) $, che le rispetta.\\
L'algebra $ (T_{e}(G),[,]) $ è chiamata \textit{algebra di Lie del gruppo} $ G $ e viene indicata con

\begin{equation}
	\mathfrak{g} = (T_{e}(G),[,])
\end{equation}

\begin{remark}
	Siano $ A,B \in T_{e}(G) $, il campo di vettori associato al loro commutatore tramite l'isomorfismo tra $ T_{e}(G) $ e $ L(G) $ è identico al commutatore tra i campi di vettori corrispondenti ad $ A $ e $ B $, i.e.
	
	\begin{equation}
		\widehat{[A,B]} = \widehat{\comm{\hat{A}}{\hat{B}}_{e}} = \comm{\hat{A}}{\hat{B}}
	\end{equation}

	dove nel primo passaggio abbiamo usato la definizione di commutatore nello spazio tangente e nel secondo abbiamo usato il fatto che l'isomorfismo $ \hat{} $ e la valutazione nell'elemento neutro $ \mid_{e} $ siano i rispettivi inversi.
\end{remark}

\subsubsection{Esempi di algebre di Lie su spazi tangenti a gruppi di Lie}

\paragraph{1. $ (\R^{n},+) $}

La coppia $ (\R^{n},+) $ è un gruppo di Lie (abeliano) con elemento neutro $ 0 \in \R^{n} $. Lo spazio tangente all'elemento neutro $ T_{0}(\R^{n}) $ può essere identificato con $ \R^{n} $ stesso:

\begin{equation}
	T_{0}(\R^{n}) = \left\{ \sum_{j=1}^{n} a^{j} \eval{\pdv{x^{j}}}_{0} \right\} = (a_{1},\dots,a^{n}) \in \R^{n}
\end{equation}

dove

\begin{equation}
	T_{0}(\R^{n}) = \expval{ \eval{ \pdv{x^{1}} }_{0}, \dots, \eval{ \pdv{x^{n}} }_{0} }
\end{equation}

Prendiamo ora l'isomorfismo $ \hat{} $, in modo da analizzare i campi di vettori invarianti a sinistra $ L(\R^{n}) $

\map{\hat{}}%
	{T_{0}(\R^{n})}{L(\R^{n})}%
	{\eval{ \pdv{x^{j}} }_{0}}{\widehat{ \pdv{x^{1}} }}

Applichiamolo alla base di $ T_{0}(\R^{n}) $ per ottenere i generatori dello spazio $ L(\R^{n}) $

\begin{equation}
	L(\R^{n}) = \expval{ \widehat{ \eval{ \pdv{x^{1}} }_{0} }, \dots, \widehat{ \eval{ \pdv{x^{n}} }_{0} } }
\end{equation}

Per esplicitare la forma dei campi di vettori invarianti a sinistra, valutiamo un rappresentante di $ L(\R^{n}) $ in un generico elemento $ g \in \R^{n} $

\begin{align}
	\begin{split}
		\left( \widehat{ \eval{ \pdv{x^{i}} }_{0} } \right)_{g} \in T_{g}(\R^{n}) %
		\implies%
		\left( \widehat{ \eval{ \pdv{x^{i}} }_{0} } \right)_{g} = \sum_{i=1}^{n} b^{ij} \eval{ \pdv{x^{j}} }_{g} \qcomma b^{ij} \in \R
	\end{split}
\end{align}

Al fine di trovare i coefficienti $ b^{ij} $, applichiamo entrambi i membri dell'equazione alla funzione

\map{x^{k}}%
	{\R^{n}}{\R}%
	{(x^{1},\dots,x^{n})}{x^{k}}

Per il secondo membro

\begin{align}
	\begin{split}
		\left( \sum_{i=1}^{n} b^{ij} \eval{ \pdv{x^{j}} }_{g} \right) (x^{k}) &= \sum_{i=1}^{n} b^{ij} \eval{ \pdv{x^{k}}{x^{j}} }_{g}\\
		&= \sum_{i=1}^{n} b^{ij} \delta^{jk}\\
		&= b^{ik}
	\end{split}
\end{align}

mentre per il primo membro, considerando $ g = (g^{1},\dots,g^{n}) $ e la traslazione a sinistra

\map{L_{g}}%
	{\R^{n}}{\R^{n}}%
	{x}{g+x}

abbiamo che

\begin{align}
	\begin{split}
		\left( \widehat{ \eval{ \pdv{x^{i}} }_{0} } \right)_{g} (x^{k}) &= L_{g_{*0}} \left( \eval{ \pdv{x^{i}} }_{0} \right) (x^{k})\\
		&= \eval{ \pdv{x^{i}} }_{0} (x^{k} \circ L_{g})\\
		&= \eval{ \pdv{x^{i}} }_{0} (g^{k} + x^{k})\\
		&= \cancel{ \eval{ \pdv{g^{k}}{x^{i}} }_{0} } + \eval{ \pdv{x^{k}}{x^{i}} }_{0}\\
		&= \delta^{ik}
	\end{split}
\end{align}

perciò $ b^{ik} = \delta^{ik} $ e quindi

\begin{equation}
	\left( \widehat{ \eval{ \pdv{x^{i}} }_{0} } \right)_{g} = \sum_{i=1}^{n} \delta^{ij} \eval{ \pdv{x^{j}} }_{g} = \eval{ \pdv{x^{i}} }_{g} \qcomma \forall g \in \R
\end{equation}

il che implica

\begin{equation}
	\widehat{ \eval{ \pdv{x^{i}} }_{0} } = \pdv{x^{i}}
\end{equation}

Un campo di vettori invariante a sinistra può dunque essere scritto come

\begin{equation}
	X \in L(\R^{n}) \iff X = \sum_{i=1}^{n} a^{i} \pdv{x^{j}} \qcomma a^{i} \in \R
\end{equation}

mentre un campo di vettori qualsiasi può avere al posto dei coefficienti reali $ a^{i} $ delle funzioni.\\
Il commutatore dell'algebra di Lie $ (T_{0}(\R^{n}),[,]) $, presi $ A,B \in T_{0}(\R^{n}) $ è definito come

\begin{align}
	\begin{split}
		\comm{A}{B} &\doteq \comm{\hat{A}}{\hat{B}}_{0}\\
		&= \comm{ \sum_{i=1}^{n} a^{i} \pdv{x^{i}} }{ \sum_{j=1}^{n} b^{j} \pdv{x^{j}} }_{0}\\
		&= \sum_{i,j=1}^{n} a^{i} b^{j} \cancel{ \comm{ \pdv{x^{i}} }{ \pdv{x^{j}} } }\\
		&= 0
	\end{split}
\end{align}

Essendo il commutatore nullo, l'algebra di Lie $ (T_{0}(\R^{n}),[,]) $ è abeliana.

\paragraph{2. $ (\S^{1},\cdot) $}

La coppia $ (\S^{1},\cdot) $, dove $ \S^{1} $ è una sottovarietà di $ \R^{2} = \C $, è un gruppo di Lie (abeliano) con elemento neutro $ 1 \in \S^{1} $. Lo spazio tangente all'elemento neutro $ T_{1}(\S^{1}) $ è generato dall'unità immaginaria, i.e.

\begin{equation}
	T_{1}(\S^{1}) = \expval{i}_{\R} = \{ a i \mid a \in \R \}
\end{equation}

Prendendo la traslazione a sinistra (anche $ (\S^{1},\cdot) $ è un gruppo abeliano)

\map{L_{g}}%
	{\S^{1}}{\S^{1}}%
	{h}{g h}
	
e la curva liscia

\map{c}%
	{(-\varepsilon,\varepsilon)}{\S^{1}}%
	{t}{e^{i t}}
	
con $ c(0) = 1 $ e $ c'(0) = i $, l'isomorfismo verso i campi di vettori invarianti a sinistra è definito come

\begin{align}
	\begin{split}
		\hat{i}_{g} &= L_{g_{*1}} (i)\\
		&= \eval{ \dv{t} L_{g}(c(t)) }_{t=0}\\
		&= \eval{ \dv{t} (g \, e^{i t}) }_{t=0}\\
		&= i g
	\end{split}
\end{align}

dove $ L(\S^{1}) = \expval{\hat{i}}_{\R} $ e $ \hat{i}_{g} $ è il vettore perpendicolare alla normale al punto $ g \in \S^{1} $ (la rotazione di $ \sfrac{\pi}{2} $ in senso antiorario è data da $ i $)\\
Anche in questo caso, il commutatore definito nello spazio tangente è nullo in quanto

\begin{align}
	\begin{split}
		\comm{a i}{b i} &\doteq \comm{\widehat{a i}}{\widehat{b i}}_{1}\\
		&= a b \cancel{ \comm{i}{i} }\\
		&= 0
	\end{split}
\end{align}

dunque anche l'algebra di Lie $ (T_{1}(\S^{1}),[,]) $ è abeliana.

\paragraph{3. $ (\T^{n},\cdot) $}

Il toro è definito come

\begin{equation}
	\T^{n} = \prod^{n} \S^{1} = \S^{1} \times \cdots \times \S^{1}
\end{equation}

La coppia $ (\T^{n},\cdot) $ è un gruppo di Lie (abeliano) con elemento neutro $ e = (1,\dots,1) \in \T^{n} $. Lo spazio tangente all'elemento neutro è

\begin{equation}
	T_{e}(\T^{n}) = \prod^{n} T_{1}(\S^{1}) = T_{1}(\S^{1}) \times \cdots \times T_{1}(\S^{1})
\end{equation}

e il commutatore dell'algebra di Lie $ (\T^{n},\cdot) $ è nullo, dunque l'algebra è abeliana.

\begin{definition}
	Se un gruppo di Lie $ G $ di dimensione $ \dim(G) = n $ è abeliano, allora
	
	\begin{equation}
		G \stackrel{iso.}{\simeq} \R^{k} \times \T^{n-k} \qcomma k \in [0,n]
	\end{equation}

	dove $ \times $ indica il prodotto diretto di gruppi di Lie.
\end{definition}

\paragraph{4. $ (GL_{n}(\R),\cdot) $}

L'insieme delle matrici invertibili

\begin{equation}
	GL_{n}(\R) = \{ A \in M_{n}(\R) \mid \det(A) \neq 0 \}
\end{equation}

insieme al prodotto matriciale $ (GL_{n}(\R),\cdot) $ forma un gruppo di Lie con elemento neutro $ I \in GL_{n}(\R) $. Lo spazio tangente all'elemento neutro è

\begin{equation}
	T_{I}(GL_{n}(\R)) = \{ X \in  \}
\end{equation}

Un elemento dello spazio tangente può essere scritto come

\begin{equation}
	A \in T_{I}(GL_{n}(\R)) %
	\iff%
	A = \sum_{i,j=1}^{n} a_{ij} \eval{ \pdv{x_{ij}} }_{I}
\end{equation}

e identificato con la matrice dei coefficienti

\begin{equation}
	A = [a_{ij}] \in M_{n}(\R)
\end{equation}

Consideriamo l'isomorfismo $ \hat{} $ tra lo spazio tangente $ T_{I}(GL_{n}(\R)) $ e l'insieme dei campi di vettori invarianti a sinistra $ L(GL_{n}(\R)) $: presa una matrice $ g = [g_{ij}] \in GL_{n}(\R) $ e la traslazione a sinistra

\map{L_{g}}%
	{GL_{n}(\R)}{GL_{n}(\R)}%
	{h}{g h}

un elemento di $ L(GL_{n}(\R)) $ è definito come

\begin{align}
	\begin{split}
		\hat{A}_{g} &= L_{g_{*I}} (A)\\
		&= \sum_{i,j=1}^{n} (g A)_{ij} \eval{ \pdv{x_{ij}} }_{g}
	\end{split}
\end{align}

Il commutatore nello spazio tangente è definito tramite l'isomorfismo ed avrà la forma

\begin{equation}
	\comm{A}{B} \doteq \comm{\hat{A}}{\hat{B}}_{I} = \sum_{i,j=1}^{n} c_{ij} \eval{ \pdv{x_{ij}} }_{I}
\end{equation}

può dunque essere identificato dalla matrice

\begin{equation}
	\comm{A}{B} = C = [c_{ij}] \in M_{n}(\R)
\end{equation}

Definendo la funzione

\map{x_{ij}}%
	{M_{n}(\R)}{\R}%
	{X = [x_{ij}]}{x_{ij}}

e applicando entrambi i membri del commutatore a questa funzione, abbiamo che

\begin{align}
	\begin{split}
		\left( \sum_{p,q=1}^{n} c_{pq} \eval{ \pdv{x_{pq}} }_{I} \right)(x_{ij}) &= \comm{\hat{A}}{\hat{B}}_{I} (x_{ij})\\
		c_{ij} &= \hat{A}_{I} (\hat{B} \, x_{ij}) - \hat{B}_{I} (\hat{A} \, x_{ij})\\
		&= A (\hat{B} \, x_{ij}) - B (\hat{A} \, x_{ij})
	\end{split}
\end{align}

per $ \forall i,j = 1,\dots,n $, dove $ \hat{X}_{I} = X $ per definizione dell'isomorfismo e della sua inversa.\\
Per calcolare il secondo membro, lo applichiamo ad una matrice $ g \in GL_{n}(\R) $ e utilizziamo il differenziale della traslazione a sinistra definito sopra, ottenendo (per la parte tra parentesi del secondo termine)

\begin{align}
	\begin{split}
		(\hat{A} \, x_{ij})(g) &= \hat{A}_{g} (x_{ij})\\
		&= \left( \sum_{p,q=1}^{n} (g A)_{pq} \eval{ \pdv{x_{pq}} }_{g} \right)(x_{ij})\\
		&= \left( \sum_{p,q=1}^{n} (g A)_{pq} \, \delta_{ip} \, \delta_{jq} \right)\\
		&= (g A)_{ij}\\
		&= \sum_{k=1}^{n} g_{ik} \, a_{kj}\\
		&= \sum_{k=1}^{n} a_{kj} \, x_{ik}(g)
	\end{split}
\end{align}

da cui, per analogia

\begin{equation}
	\begin{cases}
		\hat{A} \, x_{ij} = \displaystyle\sum_{k=1}^{n} a_{kj} \, x_{ik}\\\\
		\hat{B} \, x_{ij} = \displaystyle\sum_{k=1}^{n} b_{kj} \, x_{ik}
	\end{cases}
\end{equation}

Sostituendo ora in $ c_{ij} $

\begin{align}
	\begin{split}
		c_{ij} &= A (\hat{B} \, x_{ij}) - B (\hat{A} \, x_{ij})\\
		&= \left( \sum_{p,q=1}^{n} a_{pq} \eval{ \pdv{x_{pq}} }_{I} \right) \left( \displaystyle\sum_{k=1}^{n} b_{kj} \, x_{ik} \right) - \left( \sum_{p,q=1}^{n} b_{pq} \eval{ \pdv{x_{pq}} }_{I} \right) \left( \displaystyle\sum_{k=1}^{n} a_{kj} \, x_{ik} \right)\\
		&= \sum_{p,q,k=1}^{n} ( a_{pq} b_{kj} - b_{pq} a_{kj} ) \delta_{ip} \, \delta_{kq}\\
		&= \sum_{k=1}^{n} ( a_{ik} b_{kj} - b_{ik} a_{kj} )
	\end{split}
\end{align}

Tramite questo risultato e l'identificazione $ C = [c_{ij}] = [A,B] $, otteniamo dunque

\begin{equation}
	[A,B] = A B - B A
\end{equation}

cioè l'usuale commutatore tra matrici.













