\section{Gruppi di Lie}

Un gruppo $ G $ è un \textit{gruppo di Lie} se l'insieme del gruppo è una varietà differenziabile, è un gruppo algebrico e se le sue operazioni

\begin{align}
	\begin{split}
		\mu : G \times G &\to G\\
		(a,b) &\mapsto a b\\\\
		%
		i : G &\to G\\
		a &\mapsto a^{-1}
	\end{split}
\end{align}

sono lisce.\\\\
%
Le traslazioni possono essere a sinistra e a destra:

\begin{itemize}
	\item Dato $ a \in G $, la traslazione a sinistra
	
	\map{L_{a}}{G}{G}{b}{a b}
	
	è un'applicazione liscia, in quanto restrizione di un'applicazione liscia (i.e. $ L_{a} = \mu(a,\cdot) $), ed è inoltre un diffeomorfismo, in quanto $ L_{a}^{-1} = L_{a^{-1}} $ è ancora liscia;
	
	\item Dato $ a \in G $, la traslazione a destra
	
	\map{R_{a}}{G}{G}{b}{b a}
	
	è un'applicazione liscia, in quanto restrizione di un'applicazione liscia (i.e. $ R_{a} = \mu(\cdot,a) $), ed è inoltre un diffeomorfismo, in quanto $ R_{a}^{-1} = R_{a^{-1}} $ è ancora liscia.
\end{itemize}

\subsubsection{\textit{Esempi}}

\paragraph{1. Gruppo lineare $ GL_{n}(\R) $}

Il gruppo delle matrici invertibili

\begin{equation}
	GL_{n}(\R) = \{ A \in M_{n}(\R) \mid \det(A) \neq 0 \} \subset M_{n}(\R) = \R^{n^{2}}
\end{equation}

con la struttura differenziale ereditata da $ M_{n}(\mathbb{R}) = \mathbb{R}^{n^{2}} $ in quanto $ GL_{n}(\mathbb{R}) $ è aperto in questo spazio, è un gruppo di Lie rispetto alla moltiplicazione e l'inversione:

\begin{align}
	\begin{split}
		\mu : GL_{n}(\R) \times GL_{n}(\R) &\to GL_{n}(\R)\\
		(A,B) &\mapsto A B\\\\
		%
		i : GL_{n}(\R) &\to GL_{n}(\R)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

le quali sono lisce.

\paragraph{2. Gruppo lineare speciale $ SL_{n}(\R) $}

Il gruppo delle matrici con determinante unitario

\begin{equation}
	SL_{n}(\R) = \{ A \in M_{n}(\R) \mid \det(A) = 1 \} \subset M_{n}(\R) = \R^{n^{2}}
\end{equation}

è un gruppo di Lie rispetto alla moltiplicazione e l'inversione:

\begin{align}
	\begin{split}
		\mu : SL_{n}(\R) \times SL_{n}(\R) &\to SL_{n}(\R)\\
		(A,B) &\mapsto A B\\\\
		%
		i : SL_{n}(\R) &\to SL_{n}(\R)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

in quanto sottovarietà di $ GL_{n}(\R) $ (e quindi una varietà) di dimensione $ n^{2}-1 $ e gruppo algebrico rispetto alle sue operazioni, le quali sono lisce.\\
Dimostriamo ora che la moltiplicazione è liscia\footnote{%
	Questa dimostrazione è già stata fatta nell'Esempio \ref{ex-slnr}.%
}: prendiamo la moltiplicazione (liscia) in $ GL_{n}(\R) $

\map{F}{GL_{n}(\R) \times GL_{n}(\R)}{GL_{n}(\R)}{(A,B)}{A B}

e l'inclusione liscia

\begin{equation}
	i \times i : SL_{n}(\R) \times SL_{n}(\R) \to GL_{n}(\R) \times GL_{n}(\R)
\end{equation}

L'applicazione $ G = F \circ (i \times i) $ è dunque liscia perché composizione di applicazione lisce; siccome il prodotto di due matrici con determinante unitario (i.e. in $ SL_{n}(\R) $) è ancora una matrice con determinante unitario, l'immagine di $ G $

\begin{equation}
	G(SL_{n}(\R) \times SL_{n}(\R)) \subset SL_{n}(\R)
\end{equation}

A questo punto, dato che $ SL_{n}(\R) $ è una sottovarietà di $ GL_{n}(\R) $ (ipotesi del teorema che permette di mantenere l'applicazione liscia), possiamo considerare la restrizione del codominio della funzione $ G $

\map{\tilde{G}}{SL_{n}(\R) \times SL_{n}(\R)}{SL_{n}(\R)}{(A,B)}{A B}

la quale è identica a $ \mu $, dunque $ \mu \in C^{\infty}(SL_{n}(\R) \times SL_{n}(\R)) $.\\
Per dimostrare che l'inversione sia liscia, consideriamo l'inversione in $ GL_{n}(\R) $

\map{F}{GL_{n}(\R)}{GL_{n}(\R)}{A}{A^{-1}}

la quale è liscia e, analogamente per la moltiplicazione, definiamo $ G = F \circ i $

\map{G}{SL_{n}(\R)}{GL_{n}(\R)}{A}{A^{-1}}

e dunque la restrizione del suo codominio a $ SL_{n}(\R) $ (sottovarietà di $ GL_{n}(\R) $)

\map{\tilde{G}}{SL_{n}(\R)}{SL_{n}(\R)}{A}{A^{-1}}

funzione che coincide con l'inversione in $ SL_{n}(\R) $, rendendo dunque l'inversione  $ i $ liscia.

\paragraph{3. Gruppo ortogonale $ O(n) $}

Il gruppo delle matrici ortogonali

\begin{equation}
	O(n) = \{ A \in GL_{n}(\R) \mid A^{T} A = I \} \subset GL_{n}(\R)
\end{equation}

è una sottovarietà di $ GL_{n}(\R) $ (dal teorema della preimmagine di un'applicazione di rango costante).\\
Il ragionamento per cui $ O(n) $ sia un gruppo di Lie rispetto alla moltiplicazione e l'inversione

\begin{align}
	\begin{split}
		\mu : O(n) \times O(n) &\to O(n)\\
		(A,B) &\mapsto A B\\\\
		%
		i : O(n) &\to O(n)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

è analogo a quello fatto per $ SL_{n}(\R) $.\\
Calcoliamo ora la dimensione di $ O(n) $ come varietà e il suo spazio tangente nell'identità $ T_{I}(O(n)) $. Consideriamo l'insieme delle matrici simmetriche di ordine $ n $

\begin{equation}
	S(n) = \{ B \in M_{n}(\R) \mid B^{T} = B \}
\end{equation}

e l'applicazione liscia

\map{f}{GL_{n}(\R)}{S(n)}{A}{A^{T} A}

dove $ (A^{T} A)^{T} = A^{T} A $ dunque $ A^{T} A \in S(n) $. Dimostrando che $ I \in \mathcal{VR}_{f} $, per il teorema della preimmagine di un valore regolare, avremmo che $ O(n) $ è una sottovarietà di $ GL_{n}(\R) $ con dimensione

\begin{equation}
	\dim(O(n)) = \dim(GL_{n}(\R)) - \dim(S(n))
\end{equation}

dove $ S(n) $ è uno spazio vettoriale su $ \R $ e anche un sottospazio vettoriale di $ M_{n}(\R) $, la cui dimensione è data dalla quantità di condizioni necessarie al fine di determinare una matrice simmetrica, i.e.

\begin{equation}
	\dim(S(n)) = \dfrac{n (n+1)}{2}
\end{equation}

Siccome $ \dim(GL_{n}(\R)) = n^{2} $, otteniamo che

\begin{equation}
	\dim(O(n)) = n^{2} - \dfrac{n (n+1)}{2} = \dfrac{n (n-1)}{2}
\end{equation}

Verifichiamo ora che $ I \in \mathcal{VR}_{f} $: per fare ciò, dobbiamo calcolare il differenziale di $ f $ e controllare che tutti i punti che stanno nell'immagine di $ I $ attraverso $ f_{*} $ siano punti regolari

\begin{equation}
	f_{*A} : T_{A}(GL_{n}(\R)) \to T_{f(A)}(S(n))
\end{equation}

possiamo identificare $ T_{A}(GL_{n}(\R)) = M_{n}(\R) $ e $ T_{f(A)}(S(n)) = S(n) $ (in quanto $ S(n) $ è uno spazio vettoriale), perciò $ f_{*} $ porta matrici in matrici simmetriche. Calcoliamo dunque il differenziale $ f_{*A}(X) $ prendendo una curva che passi per $ A $ in 0 e il cui vettore tangente sia $ B $

\begin{equation}
	\begin{cases}
		c : (-\varepsilon,\varepsilon) \to GL_{n}(\R)\\
		c(0) = A\\
		c'(0) = \dot{c}(0) = X
	\end{cases}
\end{equation}

dove

\begin{equation}
	T_{A}(GL_{n}(\R)) = M_{n}(\R) \implies c'(0) = \dot{c}(0)
\end{equation}

perciò

\begin{align}
	\begin{split}
		f_{*A}(X) &= \left. \dfrac{\operatorname{d}}{\operatorname{dt}} f(c(t)) \right|_{t=0}\\
		&= \left. \dfrac{\operatorname{d}}{\operatorname{dt}} [c(t)^{T} c(t)] \right|_{t=0}\\
		&= \left. [ \dot{c}(t)^{T} c(t) + c(t)^{T} \dot{c}(t) ] \right|_{t=0}\\
		&= X^{T} A + X A^{T}
	\end{split}
\end{align}

i.e.

\map{f_{*A}}{M_{n}(\R)}{S(n)}{X}{X^{T} A + X A^{T}}

Vale la condizione

\begin{equation}
	I \in \mathcal{VR}_{f} %
	\iff%
	f_{*A} : M_{n}(\R) \to S(n) \text{ suriettiva} \qquad \forall A \in f^{-1}(I) = O_{n}
\end{equation}

Perché $ f_{*A} $ sia suriettiva

\begin{equation}
	\forall A \in O(n), \, \forall B \in S(n), \, \exists X \in M_{n}(\R) \mid X^{T} A + X A^{T} = B
\end{equation}

è sufficiente dunque prendere $ X = \sfrac{1}{2} AB $, i.e.

\begin{align}
	\begin{split}
		X^{T} A + X A^{T} &= \left( \dfrac{1}{2} A B \right)^{T} A + \dfrac{1}{2} A^{T} A B\\
		&= \dfrac{1}{2} B^{T} A^{T} A + \dfrac{1}{2} B\\
		&= \dfrac{1}{2} B + \dfrac{1}{2} B\\
		&= B
	\end{split}
\end{align}

dunque $ I \in \mathcal{VR}_{f} $ e $ \dim(O(n)) = n(n-1)/2 $. Da questo ragionamento, otteniamo anche lo spazio tangente a $ O(n) $ in $ I $ poiché, per il teorema della preimmagine di un valore regolare, vale la seguente uguaglianza

\begin{equation}
	T_{A}(f^{-1}(I)) = T_{A}(O(n)) = \ker(f_{*A}), \qquad A \in f^{-1}(I)
\end{equation}

perciò

\begin{equation}
	T_{I}(O(n)) = \ker(f_{*I})
\end{equation}

ma abbiamo che

\begin{equation}
	f_{*I}(X) = X^{T} + X
\end{equation}

perciò

\begin{equation}
	T_{I}(O(n)) = \ker(f_{*I}) = \{ X \in M_{n}(\R) \mid X^{T} = -X \}
\end{equation}

ovvero lo spazio tangente di $ O(n) $ è formato dalle matrici antisimmetriche (spazio vettoriale), il quale ha dimensione esattamente $ \dim(T_{I}(O(n))) = n(n-1)/2 $.

\subsection{Omomorfismi e isomorfismi}

Un \textit{omomorfismo} di gruppi\footnote{%
	Omomorfismo e omeomorfismo sono due concetti differenti legati a due parti differenti della matematica.%
} è un'applicazione (non necessariamente liscia) che preserva le moltiplicazioni di un gruppo nell'altro.

\begin{remark}
Siano $ F $ un omomorfismo, $ e_{H} \in H $ e $ e_{G} \in G $ gli elementi neutri dei rispettivi gruppi, allora $ F(e_{H}) = e_{G} $.
\end{remark}

Un omomorfismo di due gruppi di Lie $ H $ e $ G $ è un'applicazione liscia $ F : H \to G $ tale che sia un omomorfismo di gruppi, i.e.

\begin{equation}
	F \circ L_{h} = L_{F(h)} \circ F, \qquad \forall h \in H
\end{equation}

in quanto, se $ F $ è un omomorfismo di gruppi

\begin{align}
	\begin{split}
		F(h k) &= F(h) \, F(k)\\
		(F \circ L_{h})(k) &= (L_{F(h)} \circ F)(k)\\
		F \circ L_{h} &= L_{F(h)} \circ F
	\end{split}
\end{align}

Un \textit{isomorfismo di Lie} è un omomorfismo di gruppi che sia anche un diffeomorfismo.

\subsection{Sottogruppi di Lie}

Siano $ G $ un gruppo di Lie e $ H \neq \{\} $ un suo sottoinsieme, diremo che $ H $ è un \textit{sottogruppo di Lie} (immerso) di $ G $ se:

\begin{enumerate}
	\item $ H $ è un sottogruppo algebrico di $ G $, in notazione $ H < G $;
	
	\item $ H $ è una sottovarietà immersa di $ G $, i.e. è l'immagine di $ G $ tramite un'immersione iniettiva (la topologia di $ H $ non è necessariamente la topologia indotta da $ G $);
	
	\item Le operazioni di moltiplicazione e inversione, indotte da $ G $, sono lisce.
\end{enumerate}

\begin{remark}
	Se $ H \subset G $ sottogruppo algebrico ed $ H $ è una sottovarietà di $ G $, allora la terza condizione è superflua. Questo perché: consideriamo la moltiplicazione $ f : G \times G \to G $ in $ G $ e l'inclusione $ i : H \to G $, la loro composizione $ g = f \circ (i \times i) : H \times H \to G $ ha come immagine $ g(H \times H) \subset H $ perciò possiamo restringere il codominio di questa al solo insieme $ H $, ottenendo dunque la moltiplicazione $ \tilde{g} = \mu : H \times H \to H $ per $ H $, la quale è liscia per i teoremi sulle sottovarietà; il ragionamento è analogo per l'inversione.
\end{remark}

Se $ H \subset G $ sottogruppo algebrico ed $ H $ è una sottovarietà di $ G $, diremo che $ H $ un \textit{sottogruppo di Lie embedded}.\\
Ad esempio, $ SL_{n}(\R) $ e $ O(n) $ sono sottogruppi di Lie embedded di $ GL_{n}(\R) $, perché sottovarietà di quest'ultimo.

\begin{theorem}
	Siano $ G $ e $ H $ varietà differenziabili, se $ H < G $ sottogruppo algebrico e $ H $ è chiuso in $ G $ (come sottospazio topologico) allora $ H $ è un sottogruppo embedded di $ G $.
\end{theorem}

Da questo teorema, possiamo ancora derivare che $ SL_{n}(\R) $ e $ O(n) $ siano sottogruppi di Lie embedded di $ GL_{n}(\R) $ (e quindi sottovarietà) in quanto varietà e chiusi:

\begin{itemize}
	\item $ SL_{n}(\R) $ è un chiuso in $ GL_{n}(\R) $ in quanto controimmagine di 1 (chiuso in $ \R $) tramite l'applicazione continua (porta chiusi in chiusi)
	
	\map{f}{GL_{n}(\R)}{\R}{A}{\det(A)}
	
	\item $ O(n) $ è un chiuso in $ GL_{n}(\R) $ in quanto controimmagine di $ I $ (chiuso in $ S(n) $) tramite l'applicazione
	
	\map{f}{GL_{n}(\R)}{S(n)}{A}{A^{T} A}
\end{itemize}

\subsubsection{\textit{Esempio}}

Questo esempio è riferito ad un sottogruppo di Lie immerso importante, il resto dei sottogruppi considerati, saranno sottogruppi di Lie embedded.\\
Consideriamo come gruppo di Lie il toro $ \T^{2} = \S^{1} \times \S^{1} $ ($ \S^{1} $ è un gruppo di Lie, quindi il prodotto diretto di due $ \S^{1} $ è ancora un gruppo di Lie) con le operazioni naturali e l'applicazione

\map{F}{\R}{\T^{2}}{t}{(e^{2 \pi i t},e^{2 \pi i \alpha t})}

dove $ \alpha \in \R \setminus \Q $, i.e. $ \alpha $ è irrazionale; questa applicazione è un'immersione iniettiva e un omomorfismo di gruppi, in quanto

\begin{align}
	\begin{split}
		F(t+s) &= (e^{2 \pi i (t+s)},e^{2 \pi i \alpha (t+s)})\\
		&= (e^{2 \pi i t} e^{2 \pi i s},e^{2 \pi i \alpha t} e^{2 \pi i \alpha s})\\
		&= (e^{2 \pi i t},e^{2 \pi i \alpha t}) (e^{2 \pi i s},e^{2 \pi i \alpha s})\\
		&= F(t) \, F(s)
	\end{split}
\end{align}

L'immagine di $ \R $ tramite $ F $ è un sottogruppo di $ \T^{2} $, i.e. $ F(\R) < \T^{2} $, è una sottovarietà immersa ($ F $ non è un embedding) di $ \T^{2} $ e le operazioni su $ F(\R) $ sono lisce, dunque $ H \doteq F(\R) $ è un sottogruppo di Lie (immerso) di $ \T^{2} $.

\section{Esponenziale di una matrice}

Sia una matrice quadrata $ X \in M_{n}(\R) $, definiamo l'\textit{esponenziale di matrice} come\footnote{%
In particolare $ X^{0} = I $.}

\begin{equation}
	e^{X} = \sum_{i=0}^{+\infty} \dfrac{X^{i}}{i!}, \qquad X \in M_{n}(\R)
\end{equation}

sulla falsa riga delle serie di potenze

\begin{equation}
	e^{x} = \sum_{i=0}^{+\infty} \dfrac{x^{i}}{i!}, \qquad x \in \R
\end{equation}

Non è però chiaro se la serie di matrici considerata sia convergente e dunque che $ e^{X} \in M_{n}(\R) $: questo è facilmente dimostrabile se la matrice all'esponente è

\begin{itemize}
	\item la matrice nulla $ X = 0_{n} $ da cui $ e^{0_{n}} = I_{n} = I $;
	
	\item la matrice è un multiplo della matrice identità $ X = x I_{n} $, i.e. $ X^{i} = x^{i} I_{n} $, da cui $ e^{X} = x I_{n} $.
\end{itemize}

Per dimostrarlo in generale, facciamo una digressione su concetti di analisi utili alla dimostrazione.

\subsection{Spazi vettoriali e algebre normati}

Uno spazio vettoriale $ V $ sui reali $ \R $ è detto \textit{normato} se esiste un'applicazione

\map{\norm{\cdot}}{V}{\R}{v}{\norm{v}}

chiamata \textit{norma} tale che siano soddisfatte le condizioni:

\begin{equation}
	\begin{cases}
		\begin{cases}
			\norm{v} \geqslant 0\\
			\norm{v} = 0 \iff v = 0 \in V
		\end{cases}%
		 & \text{definita positiva}\\
		\norm{\lambda v} = \abs{\lambda} \norm{v} & \text{omogeneità}\\
		\norm{v + w} \leqslant \norm{v} + \norm{w} & \text{subaddittività}
	\end{cases}
\end{equation}

per $ \forall \lambda \in \R $ e $ \forall v,w \in V $.\\
Consideriamo in particolare lo spazio vettoriale delle matrici quadrate sui reali $ M_{n}(\R) $ di dimensione $ n^{2} $ e come norma

\map{\norm{}}%
	{M_{n}(\R)}%
	{\R}%
	{X = [x_{ij}]_{i,j=1,\dots,n}}%
	{\left( \sum_{i,j=1}^{n} x_{ij}^{2} \right)^{\sfrac{1}{2}}}
	
la quale corrisponde alla norma usuale in $ \R^{n^{2}} $: da ciò deriva che questa norma soddisfa le condizioni poste sopra\footnote{%
	In particolare, la subadditività deriva dalla diseguaglianza di Cauchy-Schwarz (\textit{C-S}):
	
	\begin{equation}
		\norm{v \cdot w} \leqslant \norm{v} \norm{w}, \qquad v,w \in \R^{n},
	\end{equation}%
}, rendendo quindi $ M_{n}(\R) $ uno spazio vettoriale normato.\\\\
%
Una tripletta $ (V,\cdot,\norm{}) $ è un'\textit{algebra normata} se $ V $ è uno spazio vettoriale su $ \R $, $ (V,\cdot) $ è un'algebra su $ \R $ e $ (V,\norm{\cdot}) $ è uno spazio vettoriale normato tali che valga la proprietà di submoltiplicatività

\begin{equation}
	\norm{v \cdot w} \leqslant \norm{v} \norm{w}, \qquad v,w \in V,
\end{equation}

che lega l'operazione dell'algebra $ \cdot $ e la norma $ \norm{} $.\\
La tripletta $ (M_{n}(\R),\cdot,\norm{}) $ con $ \cdot $ la moltiplicazione tra matrici e $ \norm{} $ la norma sopra definita per $ M_{n}(\R) $ è un'algebra normata: per verificarlo dobbiamo dimostrare che il prodotto tra matrici e la norma soddisfino la proprietà di submoltiplicatività (in quanto le altre condizioni sono già soddisfatte).\\
Siano le matrici quadrate $ X = [x_{ij}] $ e $ y = [y_{ij}] $, allora utilizzando la diseguaglianza di Cauchy-Schwarz

\begin{equation}
	(X Y)_{ij}^{2} = \left( \sum_{k=1}^{n} x_{ik} y_{kj} \right)^{2} \leqslant \left( \sum_{k=1}^{n} x_{ik}^{2} \right) \left( \sum_{k=1}^{n} y_{kj}^{2} \right), \qquad i,j=1,\dots,n
\end{equation}

se consideriamo dunque la somma al quadrato

\begin{align}
	\begin{split}
		\norm{X Y}^{2} &= \sum_{i,j=1}^{n} (X Y)_{ij}^{2}\\
		&= \sum_{i,j=1}^{n} \left( \sum_{k=1}^{n} x_{ik} y_{kj} \right)^{2}\\
		&\leqslant \sum_{i,j=1}^{n} \left( \sum_{k=1}^{n} x_{ik}^{2} \right) \left( \sum_{k=1}^{n} y_{kj}^{2} \right)\\
		&= \left( \sum_{i,k=1}^{n} x_{ik}^{2} \right) \left( \sum_{j,k=1}^{n} y_{kj}^{2} \right)\\
		&= \norm{X}^{2} \norm{Y}^{2}
	\end{split}
\end{align}

da cui $ \norm{X Y} \leqslant \norm{X} \norm{Y} $.

\begin{definition}
	Sia $ (V,\cdot,\norm{}) $ un'algebra normata, allora
	
	\begin{itemize}
		\item Se $ s_{n} \in V $ è una successione convergente, i.e $ s_{n} \to s \in V $, allora
		
		\begin{equation}
			s_{n} \to s \implies a s_{n} \to a s, \qquad \forall a \in V
		\end{equation}
	
		\item Se consideriamo la serie
		
		\begin{equation}
			\sum_{n=0}^{+\infty} s_{n} = s %
			\implies%
			\begin{cases}
				\displaystyle \sum_{n=0}^{+\infty} a s_{n} = a s\\\\
				\displaystyle \sum_{n=0}^{+\infty} s_{n} a = s a
			\end{cases}
			\qquad \forall a \in V
		\end{equation}
	
		dove $ a s $ e $ s a $ implicano la moltiplicazione dell'algebra.
	\end{itemize}
\end{definition}

Per definizione, la notazione $ s_{n} \to s $ implica l'equazione

\begin{equation}
	\lim_{n \to \infty} \norm{s_{n} - s} = 0
\end{equation}

\begin{proof}
	Per la prima proprietà
	
	\begin{equation}
		\begin{cases}
			\norm{a s_{n} - a s} = \norm{a (s_{n} - s)}\\
			\lim\limits_{n \to \infty} \norm{s_{n} - s} = 0
		\end{cases}
		\implies%
		\begin{cases}
			\norm{a s_{n} - a s} = \abs{a} \norm{s_{n} - s}\\
			s_{n} \to s
		\end{cases}
		\implies%
		a s_{n} \to a s
	\end{equation}

	Per la seconda: per definizione, una serie converge se la successione delle somme parziali converge allo stesso valore, i.e.
	
	\begin{equation}
		\sum_{n=0}^{+\infty} s_{n} = s %
		\iff%
		\begin{cases}
			\displaystyle \tilde{s}_{k} = \sum_{n=0}^{k} s_{n}\\
			\tilde{s}_{k} \to s
		\end{cases}
	\end{equation}

	dunque per la proprietà
	
	\begin{equation}
		\sum_{n=0}^{+\infty} a s_{n} = a s %
		\iff%
		\begin{cases}
			\displaystyle a \tilde{s}_{k} = \sum_{n=0}^{k} a s_{n}\\
			a \tilde{s}_{k} \to a s
		\end{cases}
	\end{equation}

	dove per la prima proprietà
	
	\begin{equation}
		\sum_{n=0}^{+\infty} s_{n} = s %
		\iff%
		\tilde{s}_{k} \to s%
		\implies%
		a \tilde{s}_{n} \to a \tilde{s} %
		\iff%
		\sum_{n=0}^{+\infty} a s_{n} = a s
	\end{equation}

	Il ragionamento è analogo per la moltiplicazione per $ a $ a destra.
\end{proof}

Una serie è \textit{assolutamente convergente} se è convergente la stessa serie considerando le norme degli addendi, i.e.

\begin{equation}
	\sum_{n=0}^{+\infty} s_{n} \text{ assolutamente convergente} \iff \sum_{n=0}^{+\infty} \norm{s_{n}} \text{ convergente}
\end{equation}

Una successione $ s_{n} $ è una \textit{successione di Cauchy} se

\begin{equation}
	s_{n} \text{ di Cauchy} \iff \forall \varepsilon > 0, \, \exists p,q,N_{\varepsilon} \in \N \mid \norm{s_{p} - s_{q}} \leqslant \epsilon, \quad \forall p,q \geqslant N_{\varepsilon}
\end{equation}

\begin{remark}
	Una successione convergente è di Cauchy\footnote{%
		Per dimostrarlo è sufficiente prendere una differenza tra gli addendi arbitrariamente piccola e sfruttare la subadditività.%
	}, ma non è necessariamente vero che una successione di Cauchy sia convergente.
\end{remark}

\subsection{Spazi vettoriali e algebre completi}

Uno spazio normato $ (V,\norm{}) $ è detto \textit{completo} se ogni sua successione di Cauchy è convergente. Uno spazio normato e completo si chiama \textit{spazio di Banach}.\\
Analogamente, un'algebra normata $ (V,\cdot,\norm{}) $ è \textit{completa} se $ (V,\norm{}) $ è completo. Un'algebra normata e completa si chiama \textit{algebra di Banach}.\\\\
%
Possiamo prendere come esempio di algebra di Banach $ (M_{n}(\R),\cdot,\norm{}) $, in quanto $ (M_{n}(\R),\norm{}) $ è uno spazio completo perché lo è $ \R^{n^{2}} $ e questo si identifica con lo spazio delle matrici quadrate $ M_{n}(\R) = \R^{n^{2}} $.

\begin{definition}
	Sia $ (V,\norm{}) $ uno spazio completo, se una serie è assolutamente convergente, allora è anche convergente (strettamente), i.e.
	
	\begin{equation}
		\sum_{n=0}^{+\infty} \norm{s_{n}} \text{ convergente} \implies \sum_{n=0}^{+\infty} s_{n} \text{ convergente}
	\end{equation}
\end{definition}

Come controesempio dell'implicazione inversa, la serie in $ \R $

\begin{equation}
	\sum_{n=0}^{+\infty} \dfrac{(-1)^{n}}{n}
\end{equation}

è convergente ma non assolutamente convergente, se si prende la norma in $ \R $, i.e. il valore assoluto.

\begin{proof}
	Consideriamo la successione di serie parziali $ \tilde{s}_{k} \in V $ e $ p,q \in \N $ con $ p > q $, per ipotesi la serie è assolutamente convergente perciò
	
	\begin{equation}
		\norm{\tilde{s}_{p} - \tilde{s}_{q}} = \norm{ \sum_{n=q+1}^{p} \tilde{s}_{n} } \leqslant \sum_{n=q+1}^{p} \norm{\tilde{s}_{n}} < \varepsilon, \qquad \forall p,q > N_{\varepsilon} \in \N
	\end{equation}

	dove nel secondo passaggio abbiamo utilizzato la subadditività, dunque $ \tilde{s}_{k} $ è di Cauchy. Essendo $ V $ completo, la successione $ \tilde{s}_{k} $ è convergente strettamente dunque anche la serie è convergente.
\end{proof}

\subsection{Definizione di esponenziale di matrice}

Mostriamo ora che la serie

\begin{equation}
	e^{X} = \sum_{i=0}^{+\infty} \dfrac{X^{i}}{i!}, \qquad X \in M_{n}(\R)
\end{equation}

sia convergente in $ M_{n}(\R) $.\\
Essendo lo spazio $ M_{n}(\R) $ completo, è sufficiente verificare la serie sia assolutamente convergente. Utilizzando la submoltiplicatività

\begin{equation}
	\sum_{i=0}^{+\infty} \norm{ \dfrac{X^{i}}{i!} } = %
	\sum_{i=0}^{+\infty} \dfrac{1}{i!} \norm{X^{i}} \leqslant %
	\sum_{i=0}^{+\infty} \dfrac{1}{i!} \norm{X}^{i} = %
	e^{\norm{X}}
\end{equation}

dove $ \norm{X} \in \R $, dunque la serie converge.

\subsection{Proprietà dell'esponenziale di matrice}

Valgono le seguenti proprietà:

\begin{itemize}
	\item Se $ A B = B A $ i.e. $ [A,B] = 0 $, allora $ e^{A + B} = e^{A} e^{B} = e^{B} e^{A} $ per $ \forall A,B \in M_{n}(\R) $;
	
	\item L'esponenziale di matrice è una matrice invertibile, i.e. $ e^{A} \in GL_{n}(\R) $ per $ \forall A \in M_{n}(\R) $;
	
	\item %
	\begin{equation}
		\dfrac{\operatorname{d}}{\operatorname{dt}} e^{t X} = X e^{t X} = e^{t X} X, \qquad \forall X\in M_{n}(\R), \, \forall t \in \R
	\end{equation}
\end{itemize}

\begin{proof}
	\begin{itemize}
		\item Considerando che $ [A,B] = 0 $ possiamo utilizzare la formula binomiale, dunque
		
		\begin{align}
			\begin{split}
				e^{A + B} &= \sum_{k=0}^{+\infty} \dfrac{1}{k!} (A+B)^{k}\\
				&= \sum_{k=0}^{+\infty} \dfrac{1}{k!} \left( \sum_{j=0}^{k} \binom{k}{j} A^{k-j} B^{j} \right)\\
				&= \sum_{k=0}^{+\infty} \dfrac{1}{k!} \left( \sum_{j=0}^{k} \dfrac{k!}{j! (k-j)!} A^{k-j} B^{j} \right)\\
				&= \sum_{k=0}^{+\infty} \sum_{j=0}^{k} \left( \dfrac{A^{k-j}}{(k-j)!} \right) \left( \dfrac{B^{j}}{j!} \right)\\
				&= \left( \sum_{k=0}^{+\infty} \dfrac{A^{k}}{k!} \right) \left( \sum_{j=0}^{+\infty} \dfrac{B^{j}}{j!} \right)\\
				&= e^{A} e^{B}
			\end{split}
		\end{align}
		
		\item La matrice $ e^{A} $ è invertibile con inversa, dalla prima proprietà, $ e^{-A} $ in quanto
		
		\begin{equation}
			e^{A} e^{-A} = e^{-A} e^{A} = e^{A-A} = e^{0_{n}} = I
		\end{equation}
	
		\item %
		\begin{align}
			\begin{split}
				\dfrac{\operatorname{d}}{\operatorname{dt}} e^{t X} &= \dfrac{\operatorname{d}}{\operatorname{dt}} \left( \sum_{j=0}^{+\infty} \dfrac{(t X)^{j}}{j!} \right)\\
				&= \sum_{j=0}^{+\infty} \dfrac{\operatorname{d}}{\operatorname{dt}} \left( \dfrac{(t X)^{j}}{j!} \right)\\
				&= \sum_{j=0}^{+\infty} \left( \dfrac{j X (t X)^{j-1}}{j!} \right)\\
				&= X \sum_{j=0}^{+\infty} \left( \dfrac{j (t X)^{j-1}}{j (j-1)!} \right)\\
				&= X \sum_{j=0}^{+\infty} \dfrac{(t X)^{j-1}}{(j-1)!}\\
				&= X \sum_{k=0}^{+\infty} \dfrac{(t X)^{k}}{k!}\\
				&= X e^{t X} = e^{t X} X
			\end{split}
		\end{align}
	
		dove nel quarto passaggio abbiamo usato il fatto che
		
		\begin{equation}
			s_{n} \to s \implies a s_{n} \to a s
		\end{equation}
	\end{itemize}
\end{proof}

\begin{remark}
	Se consideriamo il campo dei numeri complessi $ \C $ al posto di $ \R $ per gli spazi vettoriali e dunque una matrice con entrate complesse ha come norma
	
	\map{\norm{}}%
		{M_{n}(\C)}%
		{\R}%
		{X = [x_{ij}]_{i,j=1,\dots,n}}%
		{\left( \sum_{i,j=1}^{n} \abs{x_{ij}}^{2} \right)^{\sfrac{1}{2}}}
		
	tutti i ragionamenti fatti in questa sezione sono validi, e.g. $ e^{X} $ è ancora una matrice invertibile anche se $ X \in M_{n}(\C) $, i.e. $ e^{X} \in GL_{n}(\C) $.
\end{remark}

\section{Richiami di algebra lineare}

\subsection{Prodotti scalari ed hermitiani}

Nell'algebra $ (\R,\cdot) $, presi due vettori $ v,w \in \R^{n} $ il loro prodotto scalare $ v \cdot w \in \R $ è definito come

\begin{equation}
	v \cdot w = (v^{1},\dots,v^{n}) \cdot (w^{1},\dots,w^{n}) \doteq \sum_{j=1}^{n} v^{j} w^{j}
\end{equation}

Considerando tutti i vettori come matrici con $ n $ righe e una colonna, possiamo pensare al prodotto scalare come il prodotto di un vettore riga per un vettore colonna

\begin{equation}
	v \cdot w = v^{T} w = %
	\begin{pmatrix}
		v^{1} & \cdots & v^{n}
	\end{pmatrix}%
	\begin{pmatrix}
		w^{1} \\ \vdots \\ w^{n}
	\end{pmatrix}
\end{equation}

Per vettori in campo complesso, i.e. nell'algebra $ (\C,\cdot) $, definiamo il \textit{prodotto hermitiano} $ v \cdot w \in \C $ come

\begin{equation}
	v \cdot w \doteq \sum_{j=1}^{n} v^{j} \bar{w}^{j} = v^{T} \bar{w}
\end{equation}

con $ v,w \in \C^{n} $ e $ \bar{w} $ indica il coniugato di $ w $ (sia per la componente che per tutte le componenti dell'intero vettore colonna). Il prodotto hermitiano è definito positivo in quanto

\begin{equation}
	v \cdot v = \sum_{j=1}^{n} v^{j} \bar{v}^{j} = \sum_{j=1}^{n} \abs{v}^{j}
\end{equation}

Essendo $ v \cdot v \in \R $ ha senso dire che $ v \cdot v \geqslant 0 $ e che

\begin{equation}
	v \cdot v = 0 \iff v = 0 \in \C^{n}
\end{equation}

cioè il prodotto hermitiano è definito positivo.\\
Il prodotto hermitiano non è bilineare come il prodotto scalare, ma lineare per la prima entrata e sesquilineare per la seconda, i.e.

\begin{equation}
	\begin{cases}
		(\lambda v_{1} + \mu v_{2}) \cdot w = \lambda (v_{1} \cdot w) + \mu (v_{2} \cdot w)\\
		v \cdot (\lambda w_{1} + \mu w_{2}) = \bar{\lambda} (v \cdot w_{1}) + \bar{\mu} (v \cdot w_{2})
	\end{cases}%
	\qquad \forall \lambda,\mu \in \C, \, \forall v,w \in \C^{n}
\end{equation}

dunque non è nemmeno simmetrico ma vale l'uguaglianza

\begin{equation}
	v \cdot w = \overline{w \cdot v}
\end{equation}

\subsection{Matrici ortogonali e unitarie}

Utilizzando come entrate i numeri reali, le matrici ortogonali $ O(n) $ sono definite come

\begin{equation}
	O(n) = \{ A \in M_{n}(\R) \mid A^{T} A = I \}
\end{equation}

In campo complesso, si parla di \textit{matrici unitarie}, definite come

\begin{equation}
	U(n) = \{ A \in M_{n}(\C) \mid  = I \}
\end{equation}

dove $ A ^{-1} = A^{*} = \bar{A}^{T} $.\\
Analogamente come le matrici ortogonali hanno colonne ortogonali tra loro e di norma unitaria (entrambi rispetto al prodotto scalare), anche le matrici unitarie hanno colonne ortogonali tra loro e di norma unitaria (entrambi rispetto al prodotto hermitiano): svolgendo il prodotto matriciale nella definizione di $ U(n) $

\begin{equation}
	A^{*} A = %
	\begin{pmatrix}
		\bar{a}_{11} & \bar{a}_{21} & \cdots & \bar{a}_{n1} \\ %
		\bar{a}_{12} & \bar{a}_{22} & & \bar{a}_{n2}\\ %
		\vdots & & \ddots & \vdots \\ %
		\bar{a}_{1n} & \bar{a}_{2n} & \cdots & \bar{a}_{nn}
	\end{pmatrix}%
	\begin{pmatrix}
		a_{11} & a_{12} & \cdots & a_{1n} \\ %
		a_{21} & a_{22} & & a_{2n}\\ %
		\vdots & & \ddots & \vdots \\ %
		a_{n1} & a_{n2} & \cdots & a_{nn}
	\end{pmatrix}
\end{equation}

se consideriamo solo la prima entrata della matrice prodotto, otteniamo

\begin{equation}
	\abs{a_{11}}^{2} + \abs{a_{21}}^{2} + \cdots + \abs{a_{n1}}^{2} = 1
\end{equation}

e così per il resto degli elementi nella diagonale, mentre tutti gli altri prodotti hermitiani tra il resto delle colonne è nullo, i.e. le colonne sono tra loro ortogonali.

\begin{definition}
	Le colonne di una matrice unitaria sono una base ortonormale per l'algebra $ (\C^{n},\cdot) $, i.e. prese due colonne $ v_{i} $ e $ v_{j} $ di una matrice unitaria, vale la seguente uguaglianza $ v_{i} \cdot v_{j} = \delta_{ij} $.
\end{definition}


\begin{definition}
	La dimensione di un sottospazio $ W \subset \C^{n} $ in $ \C^{n} $ è doppia rispetto alla dimensione che questo avrebbe sui numeri reali $ \R $, in quanto esiste l'identificazione $ \C^{n} = \R^{2n} $, i.e.

	\begin{equation}
		\dim(W)_{\C^{n}}(W) = 2 \dim(W)_{\R^{n}}(W)
	\end{equation}
\end{definition}

\subsection{Matrici simili}

Siano due matrici $ A,B \in M_{n}(\C) $, diremo che $ A $ è \textit{simile} a $ B $ se esiste una matrice invertibile $ S \in GL_{n}(\C) $ tale che valga la relazione

\begin{equation}
	B = S^{-1} A S
\end{equation}

Le stesse due matrici sono \textit{unitariamente simili} se la matrice $ S $ della relazione è unitaria.

\subsection{Altre proprietà}

\begin{definition}
	Se $ A $ è simile a $ B $ se e solo se esiste un'applicazione lineare $ T : \C^{n} \to \C^{n} $ tali che $ A $ e $ B $ rappresentano $ T $ rispetto a due basi di $ \C^{n} $.
\end{definition}

\begin{definition}
	$ A $ è unitariamente simile a $ B $ se e solo se esiste un'applicazione lineare $ T : \C^{n} \to \C^{n} $ tali che $ A $ e $ B $ rappresentano $ T $ rispetto a due basi ortonormali di $ \C^{n} $.
\end{definition}

Questi due risultati derivano dal fatto che, prese due basi $ \mathcal{A} $ e $ \mathcal{B}' $ per $ \C^{n} $ con la matrice $ S $ che rappresenta il cambio di base, i.e.

\begin{equation}
	\begin{cases}
		x' = S x\\
		[T(x)]_{\mathcal{B}} = S [T(x)]_{\mathcal{A}}
	\end{cases}
\end{equation}

abbiamo che l'applicazione lineare $ T $ applicata ad un vettore $ x $ ha due immagini a seconda della base in cui sono scritte

\begin{align}
	\begin{split}
		T : \C^{n} &\to \C^{n}\\
		x &\stackrel{\mathcal{A}}{\mapsto} [T(x)]_{\mathcal{A}} = A x\\
		x' &\stackrel{\mathcal{B}}{\mapsto} [T(x)]_{\mathcal{B}} = B x'
	\end{split}
\end{align}

le quali sono legate da

\begin{align}
	\begin{split}
		[T(x)]_{\mathcal{B}} &= B x'\\
		S [T(x)]_{\mathcal{A}} &= B S x\\
		[T(x)]_{\mathcal{A}} &= S^{-1} B S x\\
		[T(x)]_{\mathcal{A}} &= A x
	\end{split}
\end{align}

da cui

\begin{equation}
	A = S^{-1} B S
\end{equation}

il quale rende le due matrici simili.

\begin{definition}
	"$ A $ è (unitariamente) simile a $ B $" è una relazione di equivalenza in $ M_{n}(\C) $, i.e. $ A \sim B $.
\end{definition}

\begin{definition}
	Se $ A \sim B $ allora $ A $ e $ B $ hanno stessi traccia, rango, determinante e polinomio caratteristico.
\end{definition}

\begin{proof}
	\begin{itemize}
		\item Per quanto riguarda la traccia:

		\begin{equation}
			\tr(A) = \tr(S^{-1} B S) = \tr(B S^{-1} S) = \tr(B)
		\end{equation}

		\item Il rango di una matrice non cambia se la si moltiplica per una matrice invertibile

		\item Per quanto riguarda il determinante, per Binet:

		\begin{equation}
			\det(A) = \det(S^{-1} B S) = \det(S^{-1}) \det(B) \det(S) = \det(B) \det(S^{-1} S) = \det(B)
		\end{equation}

		\item Per quanto riguarda il polinomio caratteristico, per Binet:
		
		\begin{align}
			\begin{split}
				P_{\lambda}(A) &= \det(A - \lambda I)\\
				&= \det(S^{-1} B S - \lambda I)\\
				&= \det(S^{-1} B S - S^{-1} \lambda I S)\\
				&= \det(S^{-1}) \det(B - \lambda I) \det(S)\\
				&= \det(B - \lambda I)\\
				&= P_{\lambda}(B)
			\end{split}
		\end{align}
		
	\end{itemize}
\end{proof}

\subsection{Teorema di Schur e teorema spettrale}

\begin{theorem}(Teorema di Schur)
	Data una matrice $ A \in M_{n}(\C) $ esiste una matrice $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} A U = T = %
		\smqty(\dmat{\lambda_{1}, & & K \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}

	con $ T $ matrice triangolare (superiore), i.e. una matrice che ha tutte entrate nulle sotto la diagonale principale.\\
	In altre parole, $ A $ è unitariamente simile ad una matrice triangolare (superiore), il che rende $ \lambda_{1},\dots,\lambda_{n} $ gli autovalori di $ A $.
\end{theorem}

\begin{proof}
	La dimostrazione di questo teorema è fatta per induzione su $ n $.\\
	Per $ n=1 $, le matrici diventano numeri e il teorema è dimostrato in quanto sono già "diagonali".\\
	Supponiamo dunque che sia vero per $ n-1 $ e dimostriamo per $ n $: dimostrare che $ A $ è simile a una matrice triangolare superiore è equivalente a trovare una base ortonormale $ \{v_{1},\dots,v_{n}\} $ (rispetto al prodotto hermitiano, i.e. $ v_{i}^{T} \bar{v}_{j} = \delta_{ij} $ con $ i,j=1,\dots,n $) di $ \C^{n} $ tale che $ A v_{k} $ sia combinazione lineare dei vettori di base.\\
	Siano $ \lambda_{1} \in \C $ un autovalore per $ A $ e $ v_{1} \neq 0 $ il corrispondente autovettore, i.e. $ A v_{1} = \lambda_{1} v_{1} $. Sia lo spazio dei vettori generati da $ v_{1} $
	
	\begin{equation}
		W = \expval{v_{1}}_{\C} = \{ \lambda v_{1} \mid \lambda \in \C \}
	\end{equation}
	
	il quale avrà $ \dim_{\C}(W) = 1 $, consideriamo ora il suo \textit{complemento ortogonale}
	
	\begin{equation}
		W^{\perp} = \{ v\in \C^{n} \mid v \cdot w = 0, \, \forall w \in W \}
	\end{equation}

	con $ \dim_{\C}(W^{\perp}) = n-1 $ e la proiezione $ \pi_{W^{\perp}} : \C^{n} \to W^{\perp} $. Consideriamo l'applicazione lineare
	
	\map{\pi_{W^{\perp}} \circ A}{W^{\perp}}{W^{\perp}}{w}{\pi_{W^{\perp}}(A w)}
	
	Per ipotesi induttiva, esiste una base ortonormale $ \{v_{2},\dots,v_{n}\} $ di $ W^{\perp} $ tale che $ (\pi_{W^{\perp}} \circ A)(v_{k}) $ è combinazione lineare di $ v_{2},\dots,v_{k} $ per $ \forall k=2,\dots,n $.\\
	A questo punto $ \{v_{1},\dots,v_{n}\} $ è una base ortonormale di $ \C^{n} $ tale che $ A v_{k} $ sia combinazione lineare dei vettori di base.\\
	Essendo la base ortonormale, la matrice $ A $ è dunque unitariamente simile a una matrice triangolare (superiore).
\end{proof}

La base $ \{v_{1},\dots,v_{n}\} $ che appare nel teorema di Schur viene chiamata \textit{base di Schur}.\\\\
%
Una matrice $ A \in M_{n}(\C) $ è detta \textit{normale} se $ A A^{*} = A^{*} A $, i.e. commuta con la sua trasposta coniugata: esempi di matrici normali sono

\begin{itemize}
	\item Le matrici unitarie, in quanto $ A^{*} A = I $;
	
	\item Le matrici ortogonali (le quali hanno entrate reali), perciò $ A^{*} A = A^{T} A = I $;
	
	\item Le matrici hermitiane, che coincidono con la loro trasposta coniugata, i.e. $ A = A^{*} $;
	
	\item Le matrici simmetriche $ A \in S(n) $ (con entrate reali), in quanto $ A = A^{T} $.
\end{itemize}

\begin{theorem}(Teorema spettrale)
	Ogni matrice $ A \in M_{n}(\C) $ normale è unitariamente simile a una matrice diagonale, i.e.
	
	\begin{equation}
		A \in M_{n}(\C) \qq{normale} \implies %
		\exists U \in U(n) \mid U^{*} A U = %
		\smqty(\dmat{\lambda_{1}, & & 0 \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}

	con $ \lambda_{j} $ gli autovalori di $ A $.\\
	In altre parole, una matrice normale è sempre diagonalizzabile e i suoi autovettori sono una base ortonormale di $ \C^{n} $.\\
	Ancora, una matrice $ A \in M_{n}(\C) $ è normale se e solo se esiste una base ortonormale di $ \C^{n} $ costituita dagli autovettori di $ A $.
\end{theorem}

\begin{proof}
	Se $ A \in M_{n}(\C) $, per il teorema di Schur esiste una matrice unitaria $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} A U = T = %
		\smqty(\dmat{\lambda_{1}, & & K \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}

	cioè $ A $ sia simile ad una matrice triangolare superiore.\\
	Essendo $ A $ normale, questo implica che anche $ U^{*} A U $ lo sia perché
	
	\begin{align}
		\begin{split}
			(U^{*} A U) (U^{*} A U)^{*} &= U^{*} A U U^{*} A^{*} U\\
			&= U^{*} A A^{*} U\\
			&= U^{*} A^{*} A U\\
			&= U^{*} A^{*} U U^{*} A U\\
			&= (U^{*} A U)^{*} (U^{*} A U)
		\end{split}
	\end{align}

	A questo punto, $ T $ è normale ma se una matrice è triangolare superiore e normale allora è diagonale: dimostriamo questo per induzione su $ n $.\\
	Per $ n=1 $, la matrice è diagonale in quanto numero.\\
	Supponiamo che sia vero per matrici di ordine $ n-1 $ e mostriamolo per matricidi ordine $ n $: riscriviamo la matrice normale triangolare superiore come
	
	\begin {equation}
		T = \mqty(\lambda_{1} & B \\ 0 & C)
	\end{equation}

	dove $ \lambda_{1} \in \C $, $ B \in M_{1,n-1}(\C) $, $ 0 \in M_{n-1,1} $ e $ C \in M_{n-1}(\C) $. Imponendo la condizione di matrice normale
	
	\begin{align}
		\begin{split}
			T T^{*} &= T^{*} T\\
			\mqty(\lambda_{1} & B \\ 0 & C) \mqty(\bar{\lambda}_{1} & 0 \\ B^{*} & C^{*}) &= \mqty(\bar{\lambda}_{1} & 0 \\ B^{*} & C^{*}) \mqty(\lambda_{1} & B \\ 0 & C)\\
			\mqty(\abs{\lambda}_{1} + B B^{*} & B C^{*} \\ C B^{*} & C C^{*}) &= \mqty(\abs{\lambda}_{1} & \bar{\lambda}_{1} B \\ \lambda_{1} B^{*} & B^{*} B + C^{*} C)
		\end{split}
	\end{align}

	otteniamo che
	
	\begin{equation}
		\begin{cases}
			B B^{*} = 0 \implies B = 0\\
			C^{*} C = C C^{*}
		\end{cases}
	\end{equation}

	rendendo $ C $ normale oltre che triangolare superiore: per ipotesi induttiva, $ C $ è diagonale quindi lo è anche $ T $.
\end{proof}

\begin{corollary}[1. Teorema spettrale per matrici simmetriche]
	Sia $ A \in S(n) $ con entrate reali ($ A \in M_{n}(\R) $), allora esiste una matrice $ P \in O(n) $ tale che
	
	\begin{equation}
		P^{T} A P = D = %
		\smqty(\dmat{\lambda_{1}, & & 0 \\ & \ddots & \\ 0 & & ,\lambda_{n}})
	\end{equation}
\end{corollary}

\begin{proof}
	Per dimostrare questo corollario, è sufficiente dimostrare che gli autovalori di $ A $ siano reali e dunque anche gli autovettori sono reali: considerando l'uguaglianza
	
	\begin{equation}
		U(n) \cup M_{n}(\R) = O(n)
	\end{equation}

	il resto della dimostrazione deriva dal teorema spettrale.\\
	Per dimostrare che gli autovalori di una matrice $ A \in S(n) \subset M_{n}(\C) $  ad entrate reali siano reali, consideriamo la relazione
	
	\begin{equation}
		A v = \lambda v \qcomma \lambda \in \C, \, v \in \C^{n}
	\end{equation}

	da cui

	\begin{align}
		\begin{split}
			\lambda v &= A v\\
			\bar{v}^{T} \lambda v &= \bar{v}^{T} A v\\
			\lambda \abs{v}^{2} &= \bar{v}^{T} \bar{A}^{T} v\\
			&= \overline{A v}^{T} v\\
			&= \overline{\lambda v}^{T} v\\
			&= \bar{\lambda} v^{T} v\\
			&= \bar{\lambda} \abs{v}^{2}
		\end{split}
	\end{align}

	perciò $ \bar{\lambda} = \lambda \implies \lambda \in \R $.
\end{proof}

\begin{corollary}[2]
	Siano una matrice normale $ A \in M_{n}(\C) $ e un suo autovalore $ \lambda $, allora la molteplicità algebrica di $ \lambda $ coincide con quella geometrica, dove la prima indica il grado della soluzione $ \lambda $ all'interno del polinomio caratteristico mentre la seconda indica la dimensione del autospazio associato a $ \lambda $.
\end{corollary}

\begin{proof}
	Per dimostrare il corollario, ricordiamo che una matrice normale è sempre diagonalizzabile e una matrice è diagonalizzabile se e solo se la molteplicità algebrica dei suoi autovalori coincide con quella geometrica.
\end{proof}

\begin{corollary}[3]
	Sia $ A \in U(n) $, i.e. $ A^{*} A = I $, allora esiste una matrice unitaria $ U \in U(n) $ tale che
	
	\begin{equation}
		U^{*} A U = %
		\smqty(\dmat{ e^{i \theta_{n}} & & 0 \\ & \ddots & \\ 0 & & e^{i \theta_{n}} }) \qcomma \theta_{j} \in \R, \, j=1,\dots,n
	\end{equation}

	In altre parole, una matrice unitaria è simile a una matrice diagonale con entrate di norma unitaria.
\end{corollary}

\begin{proof}
	Se $ A \in U(n) $ allora è normale, dunque esiste $ U \in U(n) $ tale che $ U^{*} A U $ sia diagonale, i.e.
	
	\begin{equation}
		U^{T} A U = %
		\smqty(\dmat{\lambda_{1} & & 0 \\ & \ddots & \\ 0 & & \lambda_{n}})
	\end{equation}

	quindi è sufficiente dimostrare che $ \abs{\lambda_{j}} = 1 $ per $ \forall j=1,\dots,n $:
	
	\begin{align}
		\begin{split}
			\lambda v &= A v\\
			\overline{(A v)}^{T} \lambda v &= \overline{(A v)}^{T} A v\\
			\overline{(\lambda v)}^{T} \lambda v &= \bar{v}^{T} A^{*} A v\\
			\bar{\lambda} \lambda \bar{v}^{T} v &= \bar{v}^{T} v\\
			\abs{\lambda}^{2} \abs{v}^{2} &= \abs{v}^{2}\\
			\abs{\lambda} &= 1
		\end{split}
	\end{align}
\end{proof}

Consideriamo l'insieme delle \textit{matrici unitarie speciali}:

\begin{equation}
	SU(n) = \{ X \in U(n) \mid \det(X) = 1 \} = \{ X \in M_{n}(\C) \mid X^{*} X = I \wedge \det(X) = 1 \}
\end{equation}

\begin{corollary}[4]
	Sia $ A \in SU(n) $, allora esiste una matrice unitaria $ U $ tale che
	
	\begin{equation}
		U^{*} A U = %
		\smqty(\dmat{ e^{i \theta_{n}} & & 0 \\ & \ddots & \\ 0 & & e^{i \theta_{n}} }) \qcomma \theta_{j} \in \R, \, j=1,\dots,n
	\end{equation}

	con
	
	\begin{equation}
		\sum_{j=1}^{n} \theta_{j} = 2 k \pi \qcomma k \in \Z
	\end{equation}
\end{corollary}

\begin{proof}
	\begin{align}
		\begin{split}
			1 &= \det(A)\\
			&= \prod_{j=1}^{n} e^{i \theta_{j}}\\
			&= e^{i \sum_{j=1}^{n} \theta_{j}}
		\end{split}
	\end{align}

	\begin{equation}
		1 = \det(A) = \det(U^{*} A U) = \prod_{j=1}^{n} e^{i \theta_{j}} = e^{i \sum_{j=1}^{n} \theta_{j}} %
		\implies%
		\sum_{j=1}^{n} \theta_{j} = 2 k \pi \qcomma k \in \Z
	\end{equation}
\end{proof}

\subsection{Forma canonica}

\begin{theorem}[Forma canonica ortogonale]
	Sia una matrice $ A \in O(n) $, i.e. $ A^{T} A = I $, allora esistono una matrice ortogonale $ P $, $ p,q \in \R $ e $ \theta_{j} \in (0,\pi) $ con $ j=1,\dots,k $ dove $ k = (n-p-q)/2 $ tali che
	
	\begin{equation}
		P^{T} A P = %
		\smqty(\dmat{ I_{p} & & & & & \\ & - I_{q} & & & 0 & \\ & & R_{1} & & & \\ & 0 & & \ddots & \\ & & & & R_{n-p-q} })
	\end{equation}

	sia una matrice a blocchi dove
	
	\begin{equation}
		R_{j} = \mqty(\dmat{ \cos(\theta_{j}) & \sin(\theta_{j}) \\\\ - \sin(\theta_{j}) & \cos(\theta_{j}) })
	\end{equation}

	in cui $ \det(R_{j}) = 1 $ e
	
	\begin{equation}
		\det(P^{T} A P) = %
		\begin{cases}
			1 & q = 2 k\\
			- 1 & q = 2 k + 1
		\end{cases}
	\end{equation}

	con $ k \in \N $.\\
	Se $ A \in SO(n) $ allora $ q $ è pari.
\end{theorem}

\textbf{l 30}

\section{Traccia, determinante ed esponenziale di una matrice}

qui

\section{Algebra di Lie}

qui