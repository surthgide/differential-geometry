%% map definition
%
%\newcommand{\R}{\mathbb{R}}
%\renewcommand{\S}{\mathbb{S}}
%
%\newcommand{\map}[5]{
%	\begin{align}
%		\begin{split}
%			#1 : #2 &\to #3\\
%			#4 &\mapsto #5
%		\end{split}
%	\end{align}
%}
%
%%
%
%% image definition
%
%\newcommand{\img}[2]{
%	\begin{figure}[H]
%		\centering
%		\includegraphics[width=#1\textwidth,keepaspectratio]{#2}
%	\end{figure}
%}
%
%%

\section{Gruppi di Lie}

Un gruppo $ G $ è un \textit{gruppo di Lie} se l'insieme del gruppo è una varietà differenziabile, è un gruppo algebrico e se le sue operazioni

\begin{align}
	\begin{split}
		\mu : G \times G &\to G\\
		(a,b) &\mapsto a b\\\\
		%
		i : G &\to G\\
		a &\mapsto a^{-1}
	\end{split}
\end{align}

sono lisce.\\\\
%
Le traslazioni possono essere a sinistra e a destra:

\begin{itemize}
	\item Dato $ a \in G $, la traslazione a sinistra
	
	\map{L_{a}}{G}{G}{b}{a b}
	
	è un'applicazione liscia, in quanto restrizione di un'applicazione liscia (i.e. $ L_{a} = \mu(a,\cdot) $), ed è inoltre un diffeomorfismo, in quanto $ L_{a}^{-1} = L_{a^{-1}} $ è ancora liscia;
	
	\item Dato $ a \in G $, la traslazione a destra
	
	\map{R_{a}}{G}{G}{b}{b a}
	
	è un'applicazione liscia, in quanto restrizione di un'applicazione liscia (i.e. $ R_{a} = \mu(\cdot,a) $), ed è inoltre un diffeomorfismo, in quanto $ R_{a}^{-1} = R_{a^{-1}} $ è ancora liscia.
\end{itemize}

\subsubsection{\textit{Esempi}}

\paragraph{1. Gruppo lineare $ GL_{n}(\R) $}

Il gruppo delle matrici invertibili

\begin{equation}
	GL_{n}(\R) = \{ A \in M_{n}(\R) \mid \det(A) \neq 0 \} \subset M_{n}(\R) = \R^{n^{2}}
\end{equation}

con la struttura differenziale ereditata da $ M_{n}(\mathbb{R}) = \mathbb{R}^{n^{2}} $ in quanto $ GL_{n}(\mathbb{R}) $ è aperto in questo spazio, è un gruppo di Lie rispetto alla moltiplicazione e l'inversione:

\begin{align}
	\begin{split}
		\mu : GL_{n}(\R) \times GL_{n}(\R) &\to GL_{n}(\R)\\
		(A,B) &\mapsto A B\\\\
		%
		i : GL_{n}(\R) &\to GL_{n}(\R)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

le quali sono lisce.

\paragraph{2. Gruppo lineare speciale $ SL_{n}(\R) $}

Il gruppo delle matrici con determinante unitario

\begin{equation}
	SL_{n}(\R) = \{ A \in M_{n}(\R) \mid \det(A) = 1 \} \subset M_{n}(\R) = \R^{n^{2}}
\end{equation}

è un gruppo di Lie rispetto alla moltiplicazione e l'inversione:

\begin{align}
	\begin{split}
		\mu : SL_{n}(\R) \times SL_{n}(\R) &\to SL_{n}(\R)\\
		(A,B) &\mapsto A B\\\\
		%
		i : SL_{n}(\R) &\to SL_{n}(\R)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

in quanto sottovarietà di $ GL_{n}(\R) $ (e quindi una varietà) di dimensione $ n^{2}-1 $ e gruppo algebrico rispetto alle sue operazioni, le quali sono lisce.\\
Dimostriamo ora che la moltiplicazione è liscia\footnote{%
	Questa dimostrazione è già stata fatta nell'Esempio \ref{ex-slnr}.%
}: prendiamo la moltiplicazione (liscia) in $ GL_{n}(\R) $

\map{F}{GL_{n}(\R) \times GL_{n}(\R)}{GL_{n}(\R)}{(A,B)}{A B}

e l'inclusione liscia

\begin{equation}
	i \times i : SL_{n}(\R) \times SL_{n}(\R) \to GL_{n}(\R) \times GL_{n}(\R)
\end{equation}

L'applicazione $ G = F \circ (i \times i) $ è dunque liscia perché composizione di applicazione lisce; siccome il prodotto di due matrici con determinante unitario (i.e. in $ SL_{n}(\R) $) è ancora una matrice con determinante unitario, l'immagine di $ G $

\begin{equation}
	G(SL_{n}(\R) \times SL_{n}(\R)) \subset SL_{n}(\R)
\end{equation}

A questo punto, dato che $ SL_{n}(\R) $ è una sottovarietà di $ GL_{n}(\R) $ (ipotesi del teorema che permette di mantenere l'applicazione liscia), possiamo considerare la restrizione del codominio della funzione $ G $

\map{\tilde{G}}{SL_{n}(\R) \times SL_{n}(\R)}{SL_{n}(\R)}{(A,B)}{A B}

la quale è identica a $ \mu $, dunque $ \mu \in C^{\infty}(SL_{n}(\R) \times SL_{n}(\R)) $.\\
Per dimostrare che l'inversione sia liscia, consideriamo l'inversione in $ GL_{n}(\R) $

\map{F}{GL_{n}(\R)}{GL_{n}(\R)}{A}{A^{-1}}

la quale è liscia e, analogamente per la moltiplicazione, definiamo $ G = F \circ i $

\map{G}{SL_{n}(\R)}{GL_{n}(\R)}{A}{A^{-1}}

e dunque la restrizione del suo codominio a $ SL_{n}(\R) $ (sottovarietà di $ GL_{n}(\R) $)

\map{\tilde{G}}{SL_{n}(\R)}{SL_{n}(\R)}{A}{A^{-1}}

funzione che coincide con l'inversione in $ SL_{n}(\R) $, rendendo dunque l'inversione  $ i $ liscia.

\paragraph{3. Gruppo ortogonale $ O(n) $}

Il gruppo delle matrici ortogonali

\begin{equation}
	O(n) = \{ A \in GL_{n}(\R) \mid A^{T} A = I \} \subset GL_{n}(\R)
\end{equation}

è una sottovarietà di $ GL_{n}(\R) $ (dal teorema della preimmagine di un'applicazione di rango costante).\\
Il ragionamento per cui $ O(n) $ sia un gruppo di Lie rispetto alla moltiplicazione e l'inversione

\begin{align}
	\begin{split}
		\mu : O(n) \times O(n) &\to O(n)\\
		(A,B) &\mapsto A B\\\\
		%
		i : O(n) &\to O(n)\\
		A &\mapsto A^{-1}
	\end{split}
\end{align}

è analogo a quello fatto per $ SL_{n}(\R) $.\\
Calcoliamo ora la dimensione di $ O(n) $ come varietà e il suo spazio tangente nell'identità $ T_{I}(O(n)) $. Consideriamo l'insieme delle matrici simmetriche di ordine $ n $

\begin{equation}
	S(n) = \{ B \in M_{n}(\R) \mid B^{T} = B \}
\end{equation}

e l'applicazione liscia

\map{f}{GL_{n}(\R)}{S(n)}{A}{A^{T} A}

dove $ (A^{T} A)^{T} = A^{T} A $ dunque $ A^{T} A \in S(n) $. Dimostrando che $ I \in \mathcal{VR}_{f} $, per il teorema della preimmagine di un valore regolare, avremmo che $ O(n) $ è una sottovarietà di $ GL_{n}(\R) $ con dimensione

\begin{equation}
	\dim(O(n)) = \dim(GL_{n}(\R)) - \dim(S(n))
\end{equation}

dove $ S(n) $ è uno spazio vettoriale su $ \R $ e anche un sottospazio vettoriale di $ M_{n}(\R) $, la cui dimensione è data dalla quantità di condizioni necessarie al fine di determinare una matrice simmetrica, i.e.

\begin{equation}
	\dim(S(n)) = \dfrac{n (n+1)}{2}
\end{equation}

Siccome $ \dim(GL_{n}(\R)) = n^{2} $, otteniamo che

\begin{equation}
	\dim(O(n)) = n^{2} - \dfrac{n (n+1)}{2} = \dfrac{n (n-1)}{2}
\end{equation}

Verifichiamo ora che $ I \in \mathcal{VR}_{f} $: per fare ciò, dobbiamo calcolare il differenziale di $ f $ e controllare che tutti i punti che stanno nell'immagine di $ I $ attraverso $ f_{*} $ siano punti regolari

\begin{equation}
	f_{*A} : T_{A}(GL_{n}(\R)) \to T_{f(A)}(S(n))
\end{equation}

possiamo identificare $ T_{A}(GL_{n}(\R)) = M_{n}(\R) $ e $ T_{f(A)}(S(n)) = S(n) $ (in quanto $ S(n) $ è uno spazio vettoriale), perciò $ f_{*} $ porta matrici in matrici simmetriche. Calcoliamo dunque il differenziale $ f_{*A}(X) $ prendendo una curva che passi per $ A $ in 0 e il cui vettore tangente sia $ B $

\begin{equation}
	\begin{cases}
		c : (-\varepsilon,\varepsilon) \to GL_{n}(\R)\\
		c(0) = A\\
		c'(0) = \dot{c}(0) = X
	\end{cases}
\end{equation}

dove

\begin{equation}
	T_{A}(GL_{n}(\R)) = M_{n}(\R) \implies c'(0) = \dot{c}(0)
\end{equation}

perciò

\begin{align}
	\begin{split}
		f_{*A}(X) &= \left. \dfrac{\operatorname{d}}{\operatorname{dt}} f(c(t)) \right|_{t=0}\\
		&= \left. \dfrac{\operatorname{d}}{\operatorname{dt}} [c(t)^{T} c(t)] \right|_{t=0}\\
		&= \left. [ \dot{c}(t)^{T} c(t) + c(t)^{T} \dot{c}(t) ] \right|_{t=0}\\
		&= X^{T} A + X A^{T}
	\end{split}
\end{align}

i.e.

\map{f_{*A}}{M_{n}(\R)}{S(n)}{X}{X^{T} A + X A^{T}}

Vale la condizione

\begin{equation}
	I \in \mathcal{VR}_{f} %
	\iff%
	f_{*A} : M_{n}(\R) \to S(n) \text{ suriettiva} \qquad \forall A \in f^{-1}(I) = O_{n}
\end{equation}

Perché $ f_{*A} $ sia suriettiva

\begin{equation}
	\forall A \in O(n), \, \forall B \in S(n), \, \exists X \in M_{n}(\R) \mid X^{T} A + X A^{T} = B
\end{equation}

è sufficiente dunque prendere $ X = \sfrac{1}{2} AB $, i.e.

\begin{align}
	\begin{split}
		X^{T} A + X A^{T} &= \left( \dfrac{1}{2} A B \right)^{T} A + \dfrac{1}{2} A^{T} A B\\
		&= \dfrac{1}{2} B^{T} A^{T} A + \dfrac{1}{2} B\\
		&= \dfrac{1}{2} B + \dfrac{1}{2} B\\
		&= B
	\end{split}
\end{align}

dunque $ I \in \mathcal{VR}_{f} $ e $ \dim(O(n)) = n(n-1)/2 $. Da questo ragionamento, otteniamo anche lo spazio tangente a $ O(n) $ in $ I $ poiché, per il teorema della preimmagine di un valore regolare, vale la seguente uguaglianza

\begin{equation}
	T_{A}(f^{-1}(I)) = T_{A}(O(n)) = \ker(f_{*A}), \qquad A \in f^{-1}(I)
\end{equation}

perciò

\begin{equation}
	T_{I}(O(n)) = \ker(f_{*I})
\end{equation}

ma abbiamo che

\begin{equation}
	f_{*I}(X) = X^{T} + X
\end{equation}

perciò

\begin{equation}
	T_{I}(O(n)) = \ker(f_{*I}) = \{ X \in M_{n}(\R) \mid X^{T} = -X \}
\end{equation}

ovvero lo spazio tangente di $ O(n) $ è formato dalle matrici antisimmetriche (spazio vettoriale), il quale ha dimensione esattamente $ \dim(T_{I}(O(n))) = n(n-1)/2 $.

\subsection{Omomorfismi e isomorfismi}

Un \textit{omomorfismo} di gruppi\footnote{%
	Omomorfismo e omeomorfismo sono due concetti differenti legati a due parti differenti della matematica.%
} è un'applicazione (non necessariamente liscia) che preserva le moltiplicazioni di un gruppo nell'altro.

\begin{remark}
Siano $ F $ un omomorfismo, $ e_{H} \in H $ e $ e_{G} \in G $ gli elementi neutri dei rispettivi gruppi, allora $ F(e_{H}) = e_{G} $.
\end{remark}

Un omomorfismo di due gruppi di Lie $ H $ e $ G $ è un'applicazione liscia $ F : H \to G $ tale che sia un omomorfismo di gruppi, i.e.

\begin{equation}
	F \circ L_{h} = L_{F(h)} \circ F, \qquad \forall h \in H
\end{equation}

in quanto, se $ F $ è un omomorfismo di gruppi

\begin{align}
	\begin{split}
		F(h k) &= F(h) \, F(k)\\
		(F \circ L_{h})(k) &= (L_{F(h)} \circ F)(k)\\
		F \circ L_{h} &= L_{F(h)} \circ F
	\end{split}
\end{align}

Un \textit{isomorfismo di Lie} è un omomorfismo di gruppi che sia anche un diffeomorfismo.

\subsection{Sottogruppi di Lie}

Siano $ G $ un gruppo di Lie e $ H \neq \{\} $ un suo sottoinsieme, diremo che $ H $ è un \textit{sottogruppo di Lie} (immerso) di $ G $ se:

\begin{enumerate}
	\item $ H $ è un sottogruppo algebrico di $ G $, in notazione $ H < G $;
	
	\item $ H $ è una sottovarietà immersa di $ G $, i.e. è l'immagine di $ G $ tramite un'immersione iniettiva (la topologia di $ H $ non è necessariamente la topologia indotta da $ G $);
	
	\item Le operazioni di moltiplicazione e inversione, indotte da $ G $, sono lisce.
\end{enumerate}

\begin{remark}
	Se $ H \subset G $ sottogruppo algebrico ed $ H $ è una sottovarietà di $ G $, allora la terza condizione è superflua. Questo perché: consideriamo la moltiplicazione $ f : G \times G \to G $ in $ G $ e l'inclusione $ i : H \to G $, la loro composizione $ g = f \circ (i \times i) : H \times H \to G $ ha come immagine $ g(H \times H) \subset H $ perciò possiamo restringere il codominio di questa al solo insieme $ H $, ottenendo dunque la moltiplicazione $ \tilde{g} = \mu : H \times H \to H $ per $ H $, la quale è liscia per i teoremi sulle sottovarietà; il ragionamento è analogo per l'inversione.
\end{remark}

Se $ H \subset G $ sottogruppo algebrico ed $ H $ è una sottovarietà di $ G $, diremo che $ H $ un \textit{sottogruppo di Lie embedded}.\\
Ad esempio, $ SL_{n}(\R) $ e $ O(n) $ sono sottogruppi di Lie embedded di $ GL_{n}(\R) $, perché sottovarietà di quest'ultimo.

\begin{theorem}
	Siano $ G $ e $ H $ varietà differenziabili, se $ H < G $ sottogruppo algebrico e $ H $ è chiuso in $ G $ (come sottospazio topologico) allora $ H $ è un sottogruppo embedded di $ G $.
\end{theorem}

Da questo teorema, possiamo ancora derivare che $ SL_{n}(\R) $ e $ O(n) $ siano sottogruppi di Lie embedded di $ GL_{n}(\R) $ (e quindi sottovarietà) in quanto varietà e chiusi:

\begin{itemize}
	\item $ SL_{n}(\R) $ è un chiuso in $ GL_{n}(\R) $ in quanto controimmagine di 1 (chiuso in $ \R $) tramite l'applicazione continua (porta chiusi in chiusi)
	
	\map{f}{GL_{n}(\R)}{\R}{A}{\det(A)}
	
	\item $ O(n) $ è un chiuso in $ GL_{n}(\R) $ in quanto controimmagine di $ I $ (chiuso in $ S(n) $) tramite l'applicazione
	
	\map{f}{GL_{n}(\R)}{S(n)}{A}{A^{T} A}
\end{itemize}

\subsubsection{\textit{Esempio}}

Questo esempio è riferito ad un sottogruppo di Lie immerso importante, il resto dei sottogruppi considerati, saranno sottogruppi di Lie embedded.\\
Consideriamo come gruppo di Lie il toro $ \T^{2} = \S^{1} \times \S^{1} $ ($ \S^{1} $ è un gruppo di Lie, quindi il prodotto diretto di due $ \S^{1} $ è ancora un gruppo di Lie) con le operazioni naturali e l'applicazione

\map{F}{\R}{\T^{2}}{t}{(e^{2 \pi i t},e^{2 \pi i \alpha t})}

dove $ \alpha \in \R \setminus \Q $, i.e. $ \alpha $ è irrazionale; questa applicazione è un'immersione iniettiva e un omomorfismo di gruppi, in quanto

\begin{align}
	\begin{split}
		F(t+s) &= (e^{2 \pi i (t+s)},e^{2 \pi i \alpha (t+s)})\\
		&= (e^{2 \pi i t} e^{2 \pi i s},e^{2 \pi i \alpha t} e^{2 \pi i \alpha s})\\
		&= (e^{2 \pi i t},e^{2 \pi i \alpha t}) (e^{2 \pi i s},e^{2 \pi i \alpha s})\\
		&= F(t) \, F(s)
	\end{split}
\end{align}

L'immagine di $ \R $ tramite $ F $ è un sottogruppo di $ \T^{2} $, i.e. $ F(\R) < \T^{2} $, è una sottovarietà immersa ($ F $ non è un embedding) di $ \T^{2} $ e le operazioni su $ F(\R) $ sono lisce, dunque $ H \doteq F(\R) $ è un sottogruppo di Lie (immerso) di $ \T^{2} $.

\section{Esponenziale di una matrice}

Sia una matrice quadrata $ X \in M_{n}(\R) $, definiamo l'\textit{esponenziale di matrice} come

\begin{equation}
	e^{X} = \sum_{i=0}^{+\infty} \dfrac{X^{i}}{i!}, \qquad X \in M_{n}(\R)
\end{equation}

dove in particolare $ X^{0} = I $, sulla falsa riga delle serie di potenze

\begin{equation}
	e^{x} = \sum_{i=0}^{+\infty} \dfrac{x^{i}}{i!}, \qquad x \in \R
\end{equation}

Non è però chiaro se la serie di matrici considerata sia convergente e dunque che $ e^{X} \in M_{n}(\R) $: questo è facilmente dimostrabile se la matrice all'esponente è

\begin{itemize}
	\item la matrice nulla $ X = 0_{n} $ da cui $ e^{0_{n}} = I_{n} = I $;
	
	\item la matrice è un multiplo della matrice identità $ X = x I_{n} $, i.e. $ X^{i} = x^{i} I_{n} $, da cui $ e^{X} = x I_{n} $.
\end{itemize}

Per dimostrare questo, facciamo una digressione su concetti di analisi utili alla dimostrazione.

\subsection{Spazi vettoriali e algebre normati}

Uno spazio vettoriale $ V $ sui reali $ \R $ è detto \textit{normato} se esiste un'applicazione

\map{\norm{\cdot}}{V}{\R}{v}{\norm{v}}

chiamata \textit{norma} tale che siano soddisfatte le condizioni:

\begin{equation}
	\begin{cases}
		\begin{cases}
			\norm{v} \geqslant 0\\
			\norm{v} = 0 \iff v = 0 \in V
		\end{cases}%
		 & \text{definita positiva}\\
		\norm{\lambda v} = \abs{\lambda} \norm{v} & \text{omogeneità}\\
		\norm{v + w} \leqslant \norm{v} + \norm{w} & \text{subaddittività}
	\end{cases}
\end{equation}

per $ \forall \lambda \in \R $ e $ \forall v,w \in V $.\\
Consideriamo in particolare lo spazio vettoriale delle matrici quadrate sui reali $ M_{n}(\R) $ di dimensione $ n^{2} $ e come norma

\map{\norm{}}%
	{M_{n}(\R)}%
	{\R}%
	{X = [x_{ij}]_{i,j=1,\dots,n}}%
	{\left( \sum_{i,j=1}^{n} x_{ij}^{2} \right)^{\sfrac{1}{2}}}
	
la quale corrisponde alla norma usuale in $ \R^{n^{2}} $: da ciò deriva che questa norma soddisfa le condizioni poste sopra\footnote{%
	In particolare, la subadditività deriva dalla diseguaglianza di Cauchy-Schwarz (\textit{C-S}):
	
	\begin{equation}
		\norm{v \cdot w} \leqslant \norm{v} \norm{w}, \qquad v,w \in \R^{n},
	\end{equation}%
}, rendendo quindi $ M_{n}(\R) $ uno spazio vettoriale normato.\\\\
%
Una tripletta $ (V,\cdot,\norm{}) $ è un'\textit{algebra normata} se $ V $ è uno spazio vettoriale su $ \R $, $ (V,\cdot) $ è un'algebra su $ \R $ e $ (V,\norm{\cdot}) $ è uno spazio vettoriale normato tali che valga la proprietà di submoltiplicatività

\begin{equation}
	\norm{v \cdot w} \leqslant \norm{v} \norm{w}, \qquad v,w \in V,
\end{equation}

che lega l'operazione dell'algebra $ \cdot $ e la norma $ \norm{} $.\\
La tripletta $ (M_{n}(\R),\cdot,\norm{}) $ con $ \cdot $ la moltiplicazione tra matrici e $ \norm{} $ la norma sopra definita per $ M_{n}(\R) $ è un'algebra normata: per verificarlo dobbiamo dimostrare che il prodotto tra matrici e la norma soddisfino la proprietà di submoltiplicatività (in quanto le altre condizioni sono già soddisfatte).\\
Siano le matrici quadrate $ X = [x_{ij}] $ e $ y = [y_{ij}] $, allora utilizzando la diseguaglianza di Cauchy-Schwarz

\begin{equation}
	(X Y)_{ij}^{2} = \left( \sum_{k=1}^{n} x_{ik} y_{kj} \right)^{2} \leqslant \left( \sum_{k=1}^{n} x_{ik}^{2} \right) \left( \sum_{k=1}^{n} y_{kj}^{2} \right), \qquad i,j=1,\dots,n
\end{equation}

se consideriamo dunque la somma al quadrato

\begin{align}
	\begin{split}
		\norm{X Y}^{2} &= \sum_{i,j=1}^{n} (X Y)_{ij}^{2}\\
		&= \sum_{i,j=1}^{n} \left( \sum_{k=1}^{n} x_{ik} y_{kj} \right)^{2}\\
		&\leqslant \sum_{i,j=1}^{n} \left( \sum_{k=1}^{n} x_{ik}^{2} \right) \left( \sum_{k=1}^{n} y_{kj}^{2} \right)\\
		&= \left( \sum_{i,k=1}^{n} x_{ik}^{2} \right) \left( \sum_{j,k=1}^{n} y_{kj}^{2} \right)\\
		&= \norm{X}^{2} \norm{Y}^{2}
	\end{split}
\end{align}

da cui $ \norm{X Y} \leqslant \norm{X} \norm{Y} $.

\begin{definition}
	Sia $ (V,\cdot,\norm{}) $ un'algebra normata, allora
	
	\begin{itemize}
		\item Se $ s_{n} \in V $ è una successione convergente, i.e $ s_{n} \to s \in V $, allora
		
		\begin{equation}
			s_{n} \to s \implies a s_{n} \to a s, \qquad \forall a \in V
		\end{equation}
	
		\item Se consideriamo la serie
		
		\begin{equation}
			\sum_{n=0}^{+\infty} s_{n} = s %
			\implies%
			\begin{cases}
				\displaystyle \sum_{n=0}^{+\infty} a s_{n} = a s\\\\
				\displaystyle \sum_{n=0}^{+\infty} s_{n} a = s a
			\end{cases}
			\qquad \forall a \in V
		\end{equation}
	
		dove $ a s $ e $ s a $ implicano la moltiplicazione dell'algebra.
	\end{itemize}
\end{definition}

Per definizione, la notazione $ s_{n} \to s $ implica l'equazione

\begin{equation}
	\lim_{n \to \infty} \norm{s_{n} - s} = 0
\end{equation}

\begin{proof}
	Per la prima proprietà
	
	\begin{equation}
		\begin{cases}
			\norm{a s_{n} - a s} = \norm{a (s_{n} - s)}\\
			\lim\limits_{n \to \infty} \norm{s_{n} - s} = 0
		\end{cases}
		\implies%
		\begin{cases}
			\norm{a s_{n} - a s} = \abs{a} \norm{s_{n} - s}\\
			s_{n} \to s
		\end{cases}
		\implies%
		a s_{n} \to a s
	\end{equation}

	Per la seconda: per definizione, una serie converge se la successione delle somme parziali converge allo stesso valore, i.e.
	
	\begin{equation}
		\sum_{n=0}^{+\infty} s_{n} = s %
		\iff%
		\begin{cases}
			\displaystyle \tilde{s}_{k} = \sum_{n=0}^{k} s_{n}\\
			\tilde{s}_{k} \to s
		\end{cases}
	\end{equation}

	dunque per la proprietà
	
	\begin{equation}
		\sum_{n=0}^{+\infty} a s_{n} = a s %
		\iff%
		\begin{cases}
			\displaystyle a \tilde{s}_{k} = \sum_{n=0}^{k} a s_{n}\\
			a \tilde{s}_{k} \to a s
		\end{cases}
	\end{equation}

	dove per la prima proprietà
	
	\begin{equation}
		\sum_{n=0}^{+\infty} s_{n} = s %
		\iff%
		\tilde{s}_{k} \to s%
		\implies%
		a \tilde{s}_{n} \to a \tilde{s} %
		\iff%
		\sum_{n=0}^{+\infty} a s_{n} = a s
	\end{equation}

	Il ragionamento è analogo per la moltiplicazione per $ a $ a destra.
\end{proof}

Una serie è \textit{assolutamente convergente} se è convergente la stessa serie considerando le norme degli addendi, i.e.

\begin{equation}
	\sum_{n=0}^{+\infty} s_{n} \text{ assolutamente convergente} \iff \sum_{n=0}^{+\infty} \norm{s_{n}} \text{ convergente}
\end{equation}

Una successione $ s_{n} $ è una \textit{successione di Cauchy} se

\begin{equation}
	s_{n} \text{ di Cauchy} \iff \forall \varepsilon > 0, \, \exists N_{\varepsilon} \mid \norm{s_{p} - s_{q}} \leqslant \epsilon, \quad \forall p,q \geqslant N_{\varepsilon}
\end{equation}

\begin{remark}
	Una successione convergente è di Cauchy\footnote{%
		Per dimostrarlo è sufficiente prendere una differenza tra gli addendi arbitrariamente piccola e sfruttare la subadditività.%
	}, ma non è necessariamente vero che una successione di Cauchy sia convergente.
\end{remark}

\textbf{l 28 m 33}

\section{Richiami di algebra lineare}

qui

\section{Traccia, determinante ed esponenziale di una matrice}

qui

\section{Algebra di Lie}

