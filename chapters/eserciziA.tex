\exer{Funzione $ C^{k}(\R) $ ma non $ C^{k+1}(\R) $}
{exer1-1}
{%
Per ogni numero naturale $ k \in \N $ costruire una funzione $ C^{k}(\R) $ ma non $ C^{k+1}(\R) $. %
}
{
Per la funzione

\map{f_{k}}
	{\R}{\R}
	{x}{\alpha x^{k(k+2)/(k+1)} + \beta}

per qualsiasi $ \alpha,\beta \in \R $ e con $ k \in \N $, vale $ f_{k} \in C^{k}(\R) $ ma $ f_{k} \notin C^{k+1}(\R) $.
}

%=======================================================================================

\exer{Funzione liscia ma non reale analitica}
{exer1-2}
{
Dimostrare che la funzione

\map{f}
	{\R}{\R}
	{x}{%
		\begin{cases}
			e^{\sfrac{-1}{x^{2}}} & x \neq 0 \\
			0 & x = 0
		\end{cases}
	}

risulta essere liscia ma non reale analitica.
}
{
La funzione $ f $ è liscia in quanto, perché lo sia, è necessario che

\begin{equation}
	\pdv[k]{f}{x} \, (0) = \pdv[k]{x} e^{\sfrac{-1}{x^{2}}} \, (0) = 0 \qcomma \forall k \in \N
\end{equation}

e questo è vero poiché

\begin{equation}
	\lim_{x \to 0} \left( \dfrac{e^{\sfrac{-1}{x^{2}}}}{x^{p}} \right) = 0 \qcomma \forall p \in \N %
	\implies%
	\lim_{x \to 0} \left( \pdv[k]{x} \left( e^{\sfrac{-1}{x^{2}}} \right) \right) = 0 \qcomma \forall k \in \N
\end{equation}

La funzione non è però reale analitica perché, in un intervallo aperto qualunque di 0 non coincide con il suo sviluppo di Taylor: lo sviluppo di Taylor per la parte dei reali positivi è diversa da 0 per qualunque valore di $ x $ non nullo mentre la parte per i reali negativi è identicamente nulla, i.e. preso $ U $ un qualunque intorno di 0

\begin{equation}
	\sum_{k=0}^{+\infty} \left( \pdv[k]{x} \left( e^{\sfrac{-1}{x^{2}}} \right) \right) \dfrac{x^{k}}{k!} \neq 0 \qcomma \forall x \in U \setminus \{0\}
\end{equation}
}

%=======================================================================================

\exer{Intervalli diffeomorfi a $ \R $}
{exer1-3}
{
Siano $ a,b,c,d \in \R $ tale che $ a<b $. Dimostrare che i seguenti intervalli sono tutti diffeomorfi tra loro e diffeomorfi a $ \R $:

\begin{equation}
	\begin{cases}
		(a,b) \\
		(c,+\infty) \\
		(-\infty,d)
	\end{cases}
\end{equation}
}
{
Consideriamo le applicazioni:

\sbs{0.5}{%
			\map{f}
				{(a,b)}{(0,1)}
				{x}{\dfrac{x-a}{b-a}}
			
			\map{g}
				{(0,1)}{(c,+\infty)}
				{x}{\dfrac{c}{x}}
			}
	{0.5}{%
			\map{h}
				{(0,1)}{(-\infty,d)}
				{x}{\ln(x)-d}
	
			\map{i}
				{(c,+\infty)}{\R}
				{x}{\ln(x-c)}
			}

Queste sono diffeomorfismi in quanto bigezioni lisce con inversa liscia, dunque le loro composizioni sono ancora diffeomorfismi. Le seguenti composizioni delle applicazioni sopraccitate inducono i seguenti diffeomorfismi:

\begin{equation}
	\begin{cases}
		g \circ f \implies (a,b) \simeq (c,+\infty) \\
		h \circ f \implies (a,b) \simeq (-\infty,d) \\
		i \circ g \circ f \implies (a,b) \simeq \R \\
		h \circ g^{-1} \implies (c,+\infty) \simeq (-\infty,d) \\
		i \implies (c,+\infty) \simeq \R \\
		i \circ g \circ h^{-1} \implies (-\infty,d) \simeq \R
	\end{cases}
\end{equation}
}

%=======================================================================================

\exer{Diffeomorfismo tra $ B_{r}(c) $ e $ \R^{n} $}
{exer1-4}
{
Dimostrare che l'applicazione

\map{h}
	{B_{1}(0)}{\R^{n}}
	{x}{\left( \dfrac{x^{1}}{\sqrt{1 - \norm{x}^{2}}}, \cdots, \dfrac{x^{n}}{\sqrt{1 - \norm{x}^{2}}} \right)}

definisce un diffeomorfismo tra la palla aperta unitaria centrata nell'origine di $ \R^{n} $ e $ \R^{n} $. Dedurre che la palla aperta di centro $ c \in \R^{n} $ e raggio $ r > 0 $ in $ \R^{n} $ è diffeomorfa a $ \R^{n} $.
}
{
L'applicazione $ h $ è una bigezione liscia in quanto ogni sua componente è liscia poiché

\begin{equation}
	\pdv[k]{(x^{i})} \left( \dfrac{x^{i}}{\sqrt{ 1-\norm{x}^{2} }} \right) < \infty %
	\qcomma \forall k \in \N, \, \forall x \in B_{1}(0), \, \forall i=1,\dots,n
\end{equation}

La sua inversa

\map{h^{-1}}
	{\R^{n}}{B_{1}(0)}
	{x}{\left( \dfrac{x^{1}}{\sqrt{1 + \norm{x}^{2}}}, \cdots, \dfrac{x^{n}}{\sqrt{1 + \norm{x}^{2}}} \right)}

è ancora liscia per lo stesso motivo, dunque $ h $ induce il diffeomorfismo $ B_{1}(0) \simeq \R^{n} $. \\
Se consideriamo l'applicazione lineare (dunque liscia con inversa liscia e perciò diffeomorfismo)

\map{g}
	{B_{r}(c)}{B_{1}(0)}
	{x}{\dfrac{x-c}{r}}

con $ c = (c^{1},\dots,c^{n}) $, e la componiamo con $ h $, otteniamo

\map{f = h \circ g}
	{B_{r}(c)}{\R^{n}}
	{x}{\left( \dfrac{\dfrac{x^{1} - c^{1}}{r}}{\sqrt{1 + \norm{\dfrac{x-c}{r}}^{2}}}, \cdots, \dfrac{\dfrac{x^{n} - c^{n}}{r}}{\sqrt{1 + \norm{\dfrac{x-c}{r}}^{2}}} \right)}

L'applicazione $ f $ è un diffeomorfismo in quanto composizione di diffeomorfismi, dunque $ f $ induce il diffeomorfismo $ B_{r}(c) \simeq \R^{n} $.
}

%=======================================================================================

\exer{Teorema di Taylor con resto per funzione a due variabili}
{exer1-5}
{
Sia $ f \in C^{\infty}(\R^{2}) $. Usando il teorema di Taylor con resto, dimostrare che esistono $ g_{11},g_{12},g_{22} \in C^{\infty}(\R^{2}) $ tali che

\begin{equation}
	f(x,y) = f(0,0) + x \, \dfrac{\partial f}{\partial x} (0,0) + y \, \dfrac{\partial f}{\partial y} (0,0) + x^{2} \, g_{11}(x,y) + x y \, g_{12}(x,y) + y^{2} \, g_{22}(x,y)
\end{equation}
}
{
Dal teorema di Taylor con resto, se $ f \in C^{\infty} (\R^{2}) $ ($ \R^{2} $ è stellato rispetto all'origine), abbiamo che

\begin{equation}
	\E g_{i_{1} \cdots i_{k}} \in C^{\infty} (\R^{2})
\end{equation}

definite come

\begin{equation}
	g_{i_{1} \cdots i_{k}} (0,0) \doteq \dfrac{1}{k!} \dfrac{\partial^{k} f}{\partial x^{i_{1}} \cdots \partial x^{i_{k}}} (0,0)
\end{equation}

tali che

\begin{equation}
	f(x,y) = f(0,0) + \sum_{m=1}^{k} \sum_{\substack{i_{1},\dots,i_{k}=1 \\ {i_{k}} > \cdots > i_{1}}}^{m} g_{i_{1} \cdots i_{k}} (x,y) \prod_{j=1}^{k} x^{i_{j}} %
	\qcomma \forall k \in \N
\end{equation}

Espandendo quest'ultima forma per $ k=1 $ otteniamo

\begin{align}
	\begin{split}
		f(x,y) &= f(0,0) + x \, g_{1} (x,y) + y \, g_{2} (x,y) \\
		&= f(0,0) + x \, g_{1} (0,0) + y \, g_{2} (0,0) + x^{2} \, g_{11}(x,y) + x y \, g_{12}(x,y) + y^{2} \, g_{22}(x,y) \\
		&= f(0,0) + x \, \dfrac{\partial f}{\partial x} (0,0) + y \, \dfrac{\partial f}{\partial y} (0,0) + x^{2} \, g_{11}(x,y) + x y \, g_{12}(x,y) + y^{2} \, g_{22}(x,y)
	\end{split}
\end{align}

dove gli ultimi tre termini indicano il resto.
}

%=======================================================================================

\exer{Funzione liscia tramite incollamento}
{exer1-6}
{
Sia $ f \in C^{\infty} (\R^{2}) $ tale che

\begin{equation}
	f(0,0) = \pdv{f}{x} \, (0,0) = \pdv{f}{y} \, (0,0) = 0
\end{equation}

Sia l'applicazione

\map{g}
	{\R^{2}}{\R}
	{(t,u)}{%
			\begin{cases}
				\dfrac{f(t,tu)}{t} & t \neq 0 \\ \\
				0 & t = 0
			\end{cases}
			}

Dimostrare che $ g \in C^{\infty}(\R^{2}) $.
}
{
Per il Teorema \ref{thm:taylor}, esistono due applicazioni $ h_{1},h_{2} \in C^{\infty}(\R^{2}) $ tali che

\begin{equation}
	\begin{cases}
		f(x,y) = f(0,0) + h_{1}(x,y) + h_{2}(x,y) \\\\
		h_{1}(0,0) = \dpdv{f}{x} \, (0,0) \\\\
		h_{2}(0,0) = \dpdv{f}{y} \, (0,0)
	\end{cases}
\end{equation}

Dalle ipotesi, possiamo scrivere

\begin{gather}
	f(x,y) = h_{1}(x,y) + h_{2}(x,y) \\
	 h_{1}(0,0) = h_{2}(0,0) = 0
\end{gather}

Considerando l'applicazione $ g $, possiamo dividere la trattazione in due casi:

\begin{itemize}
	\item $ t \neq 0 $:
	%
	\begin{align}
		\begin{split}
			g(t,u) &= \dfrac{1}{t} f(t,tu) \\
			&= \dfrac{1}{t} ( t h_{1}(t,tu) + t u \, h_{2}(t,tu) ) \\
			&= h_{1}(t,tu) + u \, h_{2}(t,tu)
		\end{split}
	\end{align}

	\item $ t = 0 $:
	%
	\begin{equation}
		g(0,u) = \cancelto{0}{h_{1}(0,0)} + u \, \cancelto{0}{h_{2}(0,0)} = 0
	\end{equation}
\end{itemize}

dunque

\begin{equation}
	g(t,u) = h_{1}(t,tu) + u \, h_{2}(t,tu) \qcomma \forall (t,u) \in \R^{2}
\end{equation}

Questo dimostra che $ g \in C^{0}(\R^{2}) $. Per dimostrare che sia liscia, consideriamo la derivata di $ g(t,u) $ rispetto a $ t $:

\begin{align}
	\begin{split}
		\dv{g(t,u)}{t} &= \pdv{g(t,u)}{x} \pdv{x}{t} + \pdv{g(t,u)}{y} \pdv{y}{t} \\
		&= \pdv{h_{1}(t,tu)}{x} \pdv{t}{t} + \pdv{h_{1}(t,tu)}{y} \pdv{(tu)}{t} + u \left( \pdv{h_{2}(t,tu)}{x} \pdv{t}{t} + \pdv{h_{2}(t,tu)}{y} \pdv{(tu)}{t} \right) \\
		&= \pdv{h_{1}(t,tu)}{x} + u \, \pdv{h_{1}(t,tu)}{y} + u \left( \pdv{h_{2}(t,tu)}{x} + u \, \pdv{h_{2}(t,tu)}{y} \right)\\
		&= \pdv{h_{1}(t,tu)}{x} + u \, \pdv{h_{1}(t,tu)}{y} + u \, \pdv{h_{2}(t,tu)}{x} + u^{2} \, \pdv{h_{2}(t,tu)}{y}
	\end{split}
\end{align}

Questa applicazione è liscia in quanto composizione liscia di applicazioni lisce (questo ragionamento si estende alle derivate di grado maggiore), dunque $ g \in C^{\infty}(\R^{2}) $.
}

%=======================================================================================

\exer{$ C_{p}^{\infty}(\R^{n}) $ come algebra commutativa e unitaria}
{exer1-7}
{
Dimostrare che l'insieme $ C_{p}^{\infty}(\R^{n}) $ dei germi delle funzioni lisce intorno a $ p \in \R^{n} $ con le operazioni di somma e di prodotto definite negli appunti è un'algebra commutativa e unitaria.
}
{
L'algebra $ A = (C_{p}^{\infty}(\R^{n}),+,\cdot) $ ha le operazioni definite come segue:

\map{+}
	{A \times A}{A}
	{([(f,U)],[(g,V)])}{[(f+g,U \cap V)]}

\map{\cdot}
	{A \times A}{A}
	{([(f,U)],[(g,V)])}{[(f g,U \cap V)]}

Perché sia effettivamente un'algebra, verifichiamo che sia distributiva e omogenea. \\
Per la distributività sinistra:

\begin{align}
	\begin{split}
		([(f,U)] + [(g,V)]) \cdot [(h,W)] &= [(f+g,U \cap V)] \cdot [(h,W)] \\
		&= [((f+g) h,U \cap V \cap W)] \\
		&= [(fh + gh,U \cap V \cap W)] \\
		&= [(fh,U \cap W)] + [(gh,V \cap W)] \\
		&= [(f,U)] \cdot [(h,W)] + [(g,V)] \cdot [(h,W)]
	\end{split}
\end{align}

per qualsiasi $ [(f,U)], [(g,V)], [(h,W)] \in C_{p}^{\infty}(\R^{n}) $. \\
La distributività destra deriva immediatamente dalla distributività sinistra e dalla commutatività (condizione non necessaria per un'algebra): quest'ultima può essere verificata tramite i seguenti passaggi

\sbs{0.5}{%
			\begin{align}
				\begin{split}
					[(f,U)] + [(g,V)] &= [(f+g,U \cap V)] \\
					&= [(g+f,V \cap U)] \\
					&= [(g,V)] + [(f,U)]
				\end{split}
			\end{align}
			}
	{0.5}{%
			\begin{align}
				\begin{split}
					[(f,U)] \cdot [(g,V)] &= [(fg,U \cap V)] \\
					&= [(gf,V \cap U)] \\
					&= [(g,V)] \cdot [(f,U)]
				\end{split}
			\end{align}
			}

per qualsiasi $ [(f,U)], [(g,V)] \in C_{p}^{\infty}(\R^{n}) $. \\
Per l'omogeneità:

\begin{align}
	\begin{split}
		\lambda ([(f,U)] \cdot [(g,V)]) &= \lambda [(fg,U \cap V)] \\
		&= [(\lambda fg,V \cap U)] \\
		&= [(\lambda f,U)] \cdot [(g,V)] \\
		&= [(f,U)] \cdot [(\lambda g,V)]
	\end{split}
\end{align}

per qualsiasi $ [(f,U)], [(g,V)] \in C_{p}^{\infty}(\R^{n}) $ e qualsiasi $ \lambda \in \R $. \\
Infine l'unitarietà, i.e.

\begin{equation}
	\E e = [(1,U)] \in C_{p}^{\infty}(\R^{n}) \mid [(f,U)] \cdot e = e \cdot [(f,U)] = [(f,U)] %
	\qcomma \forall [(f,U)] \in C_{p}^{\infty}(\R^{n})
\end{equation}

può essere verificata tramite i seguenti passaggi:

\begin{align}
	\begin{split}
		[(f,U)] \cdot [(1,U)] &= [(f \cdot 1,U \cap U)] \\
		&= [(1 \cdot f,U \cap U)] \\
		&= [(1,U)] \cdot [(f,U)] \\
		&= [(f,U)]
	\end{split}
\end{align}
}

%=======================================================================================

\exer{$ \der_{p}(C_{p}^{\infty}(\R^{n})) $ come spazio vettoriale su $ \R $}
{exer1-8}
{
Dimostrare che l'insieme $ \der_{p}(C_{p}^{\infty}(\R^{n})) $ delle derivazioni puntuali con le operazioni definite negli appunti è uno spazio vettoriale su $ \R $.
}
{
Per dimostrare che $ \der_{p}(C_{p}^{\infty}(\R^{n})) $ sia uno spazio vettoriale su $ \R $ è necessario che le operazioni di somma tra derivazioni e moltiplicazione per scalari rispettino i seguenti 8 assiomi:

\begin{equation}
	\begin{cases}
		D_{v} + (D_{w} + D_{x}) = (D_{v} + D_{w}) + D_{x} & \text{ 1. associatività (somma) } \\
		%
		D_{v} + D_{w} = D_{w} + D_{v} & \text{ 2. commutatività (somma) } \\
		%
		\E 0 \in \der_{p}(C_{p}^{\infty}(\R^{n})) \, \mid \, D_{v} + 0 = D_{v} & \text{ 3. elemento neutro (somma) } \\
		%
		\E - D_{v} \in \der_{p}(C_{p}^{\infty}(\R^{n})) \, \mid \, D_{v} + (- D_{v}) = 0 & \text{ 4. inverso (somma) } \\
		%
		\alpha (\beta D_{v}) = (\alpha \beta) D_{v} & \text{ 5. compatibilità (moltiplicazione) } \\
		%
		\E 1 \in \R \, \mid \, 1 D_{v} = D_{v} & \text{ 6. elemento neutro (moltiplicazione) } \\
		%
		(\alpha + \beta) D_{v} = \alpha D_{v} + \beta D_{v} & \text{ 7. distributività (somma vettoriale) } \\
		%
		\alpha (D_{v} + D_{w}) = \alpha D_{v} + \alpha D_{w} & \text{ 8. distributività (somma scalare) }
	\end{cases}
\end{equation}

per qualsiasi $ D_{v}, D_{w}, D_{x} \in \der_{p}(C_{p}^{\infty}(\R^{n})) $ e qualsiasi $ \alpha, \beta \in \R $. \\
Per dimostrare queste proprietà consideriamo un qualsiasi $ [(f,U)] \in C_{p}^{\infty}(\R^{n}) $ e applichiamo a questo le derivazioni:

\begin{enumerate}
	\item Associatività (somma)
	
	\begin{align}
		\begin{split}
			( D_{v} + (D_{w} + D_{x}) ) ([(f,U)]) &= D_{v} ([(f,U)]) + (D_{w} + D_{x}) ([(f,U)]) \\
			&= D_{v} ([(f,U)]) + D_{w} ([(f,U)]) + D_{x} ([(f,U)]) \\
			&= (D_{v} + D_{w}) ([(f,U)]) + D_{x} ([(f,U)]) \\
			&= ( (D_{v} + D_{w}) + D_{x} ) ([(f,U)])
		\end{split}
	\end{align}
	
	\item Commutatività (somma)
	
	\begin{align}
		\begin{split}
			(D_{v} + D_{w}) ([(f,U)]) &= D_{v} ([(f,U)]) + D_{w} ([(f,U)]) \\
			&= D_{w} ([(f,U)]) + D_{v} ([(f,U)]) \\
			&= (D_{w} + D_{v}) ([(f,U)])
		\end{split}
	\end{align}
	
	dove nel secondo passaggio abbiamo usato la commutatività della somma in $ \R $
	
	\item Elemento neutro (somma)
	
	\map{0}
		{C_{p}^{\infty}(\R^{n})}{\R}
		{[(f,U)]}{0}
	
	per qualsiasi $ [(f,U)] \in C_{p}^{\infty}(\R^{n}) $, dunque
	
	\begin{align}
		\begin{split}
			(D_{v} + 0) ([(f,U)]) &= D_{v} ([(f,U)]) + 0 ([(f,U)]) \\
			&= D_{v} ([(f,U)]) + 0 \\
			&= D_{v} ([(f,U)])
		\end{split}
	\end{align}
	
	\item Inverso (somma)
	
	\map{- D_{v}}
		{C_{p}^{\infty}(\R^{n})}{\R}
		{[(f,U)]}{- \sum_{i=1}^{n} \pdv{f}{x^{i}} \, (p) \, v^{i}}
	
	dunque
	
	\begin{align}
		\begin{split}
			(D_{v} + (- D_{v})) ([(f,U)]) &= D_{v} ([(f,U)]) + (- D_{v}) ([(f,U)]) \\
			&= \sum_{i=1}^{n} \pdv{f}{x^{i}} \, (p) \, v^{i} + \left( - \sum_{i=1}^{n} \pdv{f}{x^{i}} \, (p) \, v^{i} \right) \\
			&= 0
		\end{split}
	\end{align}
	
	\item Compatibilità (moltiplicazione)
	
	\begin{align}
		\begin{split}
			(\alpha (\beta D_{v})) ([(f,U)]) &= \alpha (\beta D_{v}) ([(f,U)]) \\
			&= \alpha D_{v} ([(\beta f,U)]) \\
			&= \alpha \beta D_{v} ([(f,U)]) \\
			&= (\alpha \beta) D_{v} ([(f,U)])
		\end{split}
	\end{align}
	
	\item Elemento neutro (moltiplicazione)
	
	\begin{align}
		\begin{split}
			(1 D_{v}) ([(f,U)]) &= D_{v} ([(1 f,U)]) \\
			&= D_{v} ([(f,U)])
		\end{split}
	\end{align}
	
	\item Distributività (somma vettoriale)
	
	\begin{align}
		\begin{split}
			(\alpha + \beta) D_{v} ([(f,U)]) &= D_{v} ([((\alpha + \beta) f,U)]) \\
			&= D_{v} ([(\alpha f + \beta f,U)]) \\
			&= \alpha D_{v} ([(f,U)]) + \beta D_{v} ([(f,U)]) \\
		\end{split}
	\end{align}
	
	\item Distributività (somma scalare)
	
	\begin{align}
		\begin{split}
			\alpha (D_{v} + D_{w}) ([(f,U)]) &= (D_{v} + D_{w}) ([(\alpha f,U)]) \\
			&= D_{v} ([(\alpha f,U)]) + D_{w} ([(\alpha f,U)]) \\
			&= \alpha D_{v} ([(f,U)]) + \alpha D_{w} ([(f,U)])
		\end{split}
	\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per qualsiasi $ D_{v}, D_{w}, D_{x} \in \der_{p}(C_{p}^{\infty}(\R^{n})) $ e qualsiasi $ \alpha, \beta \in \R $.
}

%=======================================================================================

\exer{$ \chi(U) $ come spazio vettoriale su $ \R $ e $ C^{\infty} $-modulo}
{exer1-9}
{
Dimostrare che l'insieme dei campi di vettori lisci $ \chi(U) $ su un aperto $ U \subset \R^{n} $ con le operazioni definite negli appunti è uno spazio vettoriale su $ \R $ e un $ C^{\infty} $-modulo.
}
{
\paragraph{Spazio vettoriale su $ \R $}

Per dimostrare che $ \chi(U) $ sia uno spazio vettoriale su $ \R $ è necessario che le operazioni di somma tra campi di vettori e moltiplicazione per scalari rispettino i seguenti 8 assiomi:

\begin{equation}
	\begin{cases}
		X + (Y + Z) = (X + Y) + Z & \text{ 1. associatività (somma) } \\
		%
		X + Y = Y + X & \text{ 2. commutatività (somma) } \\
		%
		\E 0 \in \chi(U) \, \mid \, X + 0 = X & \text{ 3. elemento neutro (somma) } \\
		%
		\E - X \in \chi(U) \, \mid \, X + (- X) = 0 & \text{ 4. inverso (somma) } \\
		%
		\alpha (\beta X) = (\alpha \beta) X & \text{ 5. compatibilità (moltiplicazione) } \\
		%
		\E 1 \in \R \, \mid \, 1 X = X & \text{ 6. elemento neutro (moltiplicazione) } \\
		%
		(\alpha + \beta) X = \alpha X + \beta X & \text{ 7. distributività (somma vettoriale) } \\
		%
		\alpha (X + Y) = \alpha X + \alpha Y & \text{ 8. distributività (somma scalare) }
	\end{cases}
\end{equation}

per qualsiasi $ X, Y, Z \in \chi(U) $ e qualsiasi $ \alpha, \beta \in \R $. \\
Ricordiamo che le operazioni sono definite come:

\begin{gather}
	(X + Y)_{p} \doteq X_{p} + Y_{p} \\
	(\alpha X)_{p} \doteq \alpha X_{p}
\end{gather}

per qualsiasi $ X, Y \in \chi(U) $, qualsiasi $ \alpha \in \R $ e qualsiasi $ p \in U \subset \R^{n} $, dove i campi di vettori saranno:

\begin{equation}
	X = \sum_{i=1}^{n} a^{i} \pdv{x^{i}} \qcomma Y = \sum_{i=1}^{n} b^{i} \pdv{x^{i}}
\end{equation}

con $ a_{i}, b_{i} \in C^{\infty}(U) $. \\
Per dimostrare dunque queste proprietà, valutiamo i campi di vettori in un qualsiasi $ p \in U \subset \R^{n} $:

\begin{enumerate}
	\item Associatività (somma)
	
	\begin{align}
		\begin{split}
			(X + (Y + Z))_{p} &= (X + Y)_{p} + Z_{p} \\
			&= X_{p} + Y_{p} + Z_{p} \\
			&= X_{p} + (Y + Z)_{p} \\
			&= (X + (Y + Z))_{p}
		\end{split}
	\end{align}
	
	\item Commutatività (somma)
	
	\begin{align}
		\begin{split}
			(X + Y)_{p} &= X_{p} + Y_{p} \\
			&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= Y_{p} + X_{p} \\
			&= (Y + X)_{p}
		\end{split}
	\end{align}
	
	\item Elemento neutro (somma)
	
	\map{0}
		{U}{\bigsqcup_{p \in U} T_{p} (\R^{n})}
		{p}{%
			0_{p} = \sum_{i=1}^{n} 0 \eval{ \pdv{x^{i}} }_{p} \\
			&\stackrel{\R^{n}}{\mapsto} (0, \dots, 0)%
			}
	
	dunque
	
	\begin{align}
		\begin{split}
			(X + 0)_{p} &= X_{p} + 0_{p} \\
			&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} 0 \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} (a^{i} (p) + 0) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= X_{p}
		\end{split}
	\end{align}
	
	\item Inverso (somma)
	
	\map{-X}
		{U}{\bigsqcup_{p \in U} T_{p} (\R^{n})}
		{p}{\sum_{i=1}^{n} (- a^{i} (p)) \eval{ \pdv{x^{i}} }_{p}}
	
	dunque
	
	\begin{align}
		\begin{split}
			(X + (- X))_{p} &= X_{p} + (- X)_{p} \\
			&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} (- a^{i} (p)) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} (a^{i} (p) - a^{i} (p)) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} 0 \eval{ \pdv{x^{i}} }_{p} \\
			&= 0_{p}
		\end{split}
	\end{align}
	
	\item Compatibilità (moltiplicazione)
	
	\begin{align}
		\begin{split}
			(\alpha (\beta X))_{p} &= \alpha (\beta X)_{p} \\
			&= \alpha \beta X_{p} \\
			&= (\alpha \beta) X_{p} \\
			&= ((\alpha \beta) X)_{p}
		\end{split}
	\end{align}
	
	\item Elemento neutro (moltiplicazione)
	
	\begin{align}
		\begin{split}
			(1 X_{p}) &= 1 X_{p} \\
			&= 1 \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} (1 a^{i} (p)) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= X_{p}
		\end{split}
	\end{align}
	
	\item Distributività (somma vettoriale)
	
	\begin{align}
		\begin{split}
			(\alpha (X + Y))_{p} &= \alpha (X + Y)_{p} \\
			&= \alpha (X_{p} + Y_{p}) \\
			&= \alpha \left( \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p} \right) \\
			&= \alpha \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \alpha \sum_{i=1}^{n} b^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= \alpha X_{p} + \alpha Y_{p} \\
			&= (\alpha X)_{p} + (\alpha Y)_{p} \\
			&= (\alpha X + \alpha Y)_{p}
		\end{split}
	\end{align}
	
	\item Distributività (somma scalare)
	
	\begin{align}
		\begin{split}
			((\alpha + \beta) X)_{p} &= (\alpha + \beta) X_{p} \\
			&= (\alpha + \beta) \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= \alpha \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} + \beta \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= \alpha X_{p} + \beta X_{p} \\
			&= (\alpha X)_{p} + (\beta X)_{p} \\
			&= (\alpha X + \beta X)_{p} 
		\end{split}
	\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per qualsiasi $ X, Y, Z \in \chi(U) $ e qualsiasi $ \alpha, \beta \in \R $.

\paragraph{$ C^{\infty} $-modulo}

Sia l'applicazione

\map{\cdot}
	{C^{\infty}(U) \times \chi(U)}{\chi(U)}
	{(f,X)}{f X}

Per dimostrare che $ (\chi(U),+) $ sia un $ C^{\infty} $-modulo è necessario che siano verificate queste proprietà sia a sinistra che a destra:

\begin{equation}
	\begin{cases}
		1_{C^{\infty}(U)} X = X & \text{ 1. elemento neutro (somma) } \\
		f (g X) = (f g) X & \text{ 2. compatibilità (moltiplicazione) } \\
		f (X+Y) = f X + f Y & \text{ 3. distributività (somma vettoriale) } \\
		(f+g) X = f X + g X & \text{ 4. distributività (somma scalare) }
	\end{cases}
\end{equation}

per qualsiasi $ f,g \in C^{\infty}(U) $ e qualsiasi $ X,Y \in \chi(U) $. Siccome la moltiplicazione per funzione è commutativa è sufficiente dimostrare che $ (\chi(U),+) $ sia un $ C^{\infty}(U) $-modulo sinistro (o destro) per dimostrare che sia $ C^{\infty}(U) $-modulo. \\
Dimostriamo dunque le proprietà riportate sopra, ancora una volta valutando i campi di vettori in un qualsiasi $ p \in U \subset \R^{n} $:

\begin{enumerate}
	\item Elemento neutro (somma)
	
	\map{1_{C^{\infty}(U)}}
		{U}{\R}
		{p}{1}
	
	dunque
	
	\begin{align}
		\begin{split}
			(1_{C^{\infty}(U)} X)_{p} &= 1_{C^{\infty}(U)} (p) X_{p} \\
			&= 1 \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} (1 a^{i} (p)) \eval{ \pdv{x^{i}} }_{p} \\
			&= \sum_{i=1}^{n} a^{i} (p) \eval{ \pdv{x^{i}} }_{p} \\
			&= X_{p}
		\end{split}
	\end{align}
	
	\item Compatibilità (moltiplicazione)
	
	\begin{align}
		\begin{split}
			(f (g X))_{p} &= f (p) (g X)_{p} \\
			&= f (p) g (p) X_{p} \\
			&= (f g) (p) X_{p} \\
			&= ((f g) X)_{p}
		\end{split}
	\end{align}
	
	\item Distributività (somma vettoriale)
	
	\begin{align}
		\begin{split}
			(f (X + Y))_{p} &= f (p) (X + Y)_{p} \\
			&= f (p) (X_{p} + Y_{p}) \\
			&= f (p) X_{p} + f (p) Y_{p} \\
			&= (f X)_{p} + (f Y)_{p} \\
			&= (f X + f Y)_{p}
		\end{split}
	\end{align}
	
	\item Distributività (somma scalare)
	
	\begin{align}
		\begin{split}
			((f + g) X)_{p} &= (f + g) (p) X_{p} \\
			&= (f (p) + g (p)) X_{p} \\
			&= f (p) X_{p} + g (p) X_{p} \\
			&= (f X)_{p} + (g X)_{p} \\
			&= (f X + g X)_{p}
		\end{split}
	\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per qualsiasi $ f,g \in C^{\infty}(U) $ e qualsiasi $ X,Y \in \chi(U) $.
}

%=======================================================================================

\exer{$ \der(A) $ come spazio vettoriale su $ \K $}
{exer1-10}
{
Sia $ A $ un'algebra su un campo $ \K $. Dimostrare che le operazioni

\begin{equation}
	\begin{cases}
		(D_{1}+D_{2})(a) = D_{1}(a) + D_{2}(a) \\
		(\lambda D)(a) = \lambda D(a)
	\end{cases}
\end{equation}

per qualsiasi $ \lambda \in \K $ e qualsiasi $ D_{1},D_{2},D \in \der(A) $ dotano $ \der(A) $ della struttura di spazio vettoriale su $ \K $.
}
{
Per dimostrare che $ \der(A) $ sia uno spazio vettoriale su un campo $ \K $ è necessario che le operazioni di somma tra derivazioni e moltiplicazione per scalari rispettino i seguenti 8 assiomi:

\begin{equation}
	\begin{cases}
		D_{1} + (D_{2} + D_{3}) = (D_{1} + D_{2}) + D_{3} & \text{ 1. associatività (somma) } \\
		%
		D_{1} + D_{2} = D_{2} + D_{1} & \text{ 2. commutatività (somma) } \\
		%
		\E 0 \in \der(A)) \, \mid \, D + 0 = D & \text{ 3. elemento neutro (somma) } \\
		%
		\E - D \in \der(A) \, \mid \, D + (- D) = 0 & \text{ 4. inverso (somma) } \\
		%
		\alpha (\beta D) = (\alpha \beta) D & \text{ 5. compatibilità (moltiplicazione) } \\
		%
		\E \eta \in \K \, \mid \, \eta D = D & \text{ 6. elemento neutro (moltiplicazione) } \\
		%
		(\alpha + \beta) D = \alpha D + \beta D & \text{ 7. distributività (somma vettoriale) } \\
		%
		\alpha (D_{1} + D_{2}) = \alpha D_{1} + \alpha D_{2} & \text{ 8. distributività (somma scalare) }
	\end{cases}
\end{equation}

per qualsiasi $ D_{1}, D_{2}, D_{3}, D \in \der(A) $ e qualsiasi $ \alpha, \beta \in \K $. \\
Per dimostrare queste proprietà consideriamo un qualsiasi $ a \in A $ e applichiamo a questo le derivazioni:

\begin{enumerate}
	\item Associatività (somma)
	
	\begin{align}
		\begin{split}
			( D_{1} + (D_{2} + D_{3}) ) (a) &= D_{1} (a) + (D_{2} + D_{3}) (a) \\
			&= D_{1} (a) + D_{2} (a) + D_{3} (a) \\
			&= (D_{1} + D_{2}) (a) + D_{3} (a) \\
			&= ( (D_{1} + D_{2}) + D_{3} ) (a)
		\end{split}
	\end{align}
	
	\item Commutatività (somma)
	
	\begin{align}
		\begin{split}
			(D_{1} + D_{2}) (a) &= D_{1} (a) + D_{2} (a) \\
			&= D_{2} (a) + D_{1} (a) \\
			&= (D_{2} + D_{1}) (a)
		\end{split}
	\end{align}
	
	dove nel secondo passaggio abbiamo usato la commutatività della somma dell'algebra ereditata dallo spazio vettoriale che la compone
	
	\item Elemento neutro (somma)
	
	\map{0}
		{A}{A}
		{a}{0}
	
	dove
	
	\begin{equation}
		a + 0 = a \qcomma \forall a \in A
	\end{equation}
	
	dunque
	
	\begin{align}
		\begin{split}
			(D + 0) (a) &= D (a) + 0 (a) \\
			&= D (a) + 0 \\
			&= D (a)
		\end{split}
	\end{align}
	
	\item Inverso (somma)
	
	\map{- D}
		{A}{A}
		{a}{- D (a)}
	
	dunque
	
	\begin{align}
		\begin{split}
			(D + (- D)) (a) &= D (a) + (- D) (a) \\
			&= D (a) + - D (a) \\
			&= 0
		\end{split}
	\end{align}
	
	\item Compatibilità (moltiplicazione)
	
	\begin{align}
		\begin{split}
			(\alpha (\beta D)) (a) &= \alpha (\beta D) (a) \\
			&= \alpha \beta D (a) \\
			&= (\alpha \beta) D (a) \\
			&= ((\alpha \beta) D) (a)
		\end{split}
	\end{align}
	
	\item Elemento neutro (moltiplicazione)
	
	\begin{align}
		\begin{split}
			(\eta D) (a) &= \eta D (a) \\
			&= D (\eta a) \\
			&= D (a)
		\end{split}
	\end{align}
	
	dove abbiamo usato il fatto che lo spazio vettoriale che compone l'algebra è sullo stesso campo $ \K $ rispetto a quest'ultima
	
	\item Distributività (somma vettoriale)
	
	\begin{align}
		\begin{split}
			((\alpha + \beta) D) (a) &= (\alpha + \beta) D (a) \\
			&= \alpha D (a) + \beta D (a) \\
			&= (\alpha D) (a) + (\beta D) (a) \\
			&= (\alpha D + \beta D) (a)
		\end{split}
	\end{align}
	
	\item Distributività (somma scalare)
	
	\begin{align}
		\begin{split}
			(\alpha (D_{1} + D_{2})) (a) &= \alpha (D_{1} + D_{2}) (a) \\
			&= \alpha (D_{1} (a) + D_{2} (a)) \\
			&= \alpha D_{1} (a) + \alpha D_{2} (a) \\
			&= (\alpha D_{1}) (a) + (\alpha D_{2}) (a) \\
			&= (\alpha D_{1} + \alpha D_{2}) (a)
		\end{split}
	\end{align}
\end{enumerate}

Tutte queste proprietà sono valide per qualsiasi $ D_{1}, D_{2}, D_{3}, D \in \der(A) $ e qualsiasi $ \alpha, \beta \in \K $.
}

%=======================================================================================

\exer{Commutatore come derivazione}
{exer1-11}
{
Siano $ D_{1} $ e $ D_{2} $ due derivazioni di un'algebra $ A $ su un campo $ \K $, i.e. $ D_{1},D_{2} \in \der(A) $. Mostrare che $ D_{1} \circ D_{2} $ non è necessariamente una derivazione di $ A $ mentre

\begin{equation}
	D_{1} \circ D_{2} - D_{2} \circ D_{1} \in \der(A)
\end{equation}
}
{
Perché $ D_{1} \circ D_{2} $ sia una derivazione deve, in particolare, soddisfare la regola di Leibniz, ma questo non è verificato:

\begin{align}
	\begin{split}
		(D_{1} \circ D_{2})(a \cdot b) &= D_{1}(D_{2}(a \cdot b)) \\
		&= D_{1}( D_{2}(a) \cdot b + a \cdot D_{2}(b) ) \\
		&= D_{1}(D_{2}(a)) \cdot b + D_{2}(a) \cdot D_{1}(b) + D_{1}(a) \cdot D_{2}(b) + a \cdot D_{1}(D_{2}(b)) \\
		&= (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b) + D_{2}(a) \cdot D_{1}(b) + D_{1}(a) \cdot D_{2}(b) \\
		&\neq (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b)
	\end{split}
\end{align}

Mentre per la combinazione $ D_{1} \circ D_{2} - D_{2} \circ D_{1} $ questa prescrizione è verificata:

\begin{align}
	\begin{split}
		(D_{1} \circ D_{2} - D_{2} &\circ D_{1})(a \cdot b) = D_{1}(D_{2}(a \cdot b)) - D_{2}(D_{1}(a \cdot b)) \\
		&= (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b) + D_{2}(a) \cdot D_{1}(b) + D_{1}(a) \cdot D_{2}(b) + \\
		& \hal - ((D_{2} \circ D_{1})(a) \cdot b + a \cdot (D_{2} \circ D_{1})(b) + D_{1}(a) \cdot D_{2}(b) + D_{2}(a) \cdot D_{1}(b)) \\
		%
		&= (D_{1} \circ D_{2})(a) \cdot b + a \cdot (D_{1} \circ D_{2})(b) + \cancel{ D_{2}(a) \cdot D_{1}(b) } + \cancel{ D_{1}(a) \cdot D_{2}(b) } + \\
		& \hal - (D_{2} \circ D_{1})(a) \cdot b - a \cdot (D_{2} \circ D_{1})(b) - \cancel{ D_{1}(a) \cdot D_{2}(b) } - \cancel{ D_{2}(a) \cdot D_{1}(b) } \\
		%
		&= (D_{1} \circ D_{2} - D_{2} \circ D_{1})(a) \cdot b + a \cdot (D_{1} \circ D_{2} - D_{2} \circ D_{1})(b)
	\end{split}
\end{align}

dunque $ D_{1} \circ D_{2} - D_{2} \circ D_{1} \in \der(A) $.
}
